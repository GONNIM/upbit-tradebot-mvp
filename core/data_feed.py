from __future__ import annotations
import pyupbit
import pandas as pd
import time
import logging
import random
import gc
import psutil
import os
from datetime import datetime, timedelta

from zoneinfo import ZoneInfo


logger = logging.getLogger(__name__)


# --------- ì‹œê°„/ê²½ê³„ ìœ í‹¸ (KST naiveë¡œ ì¼ê´€) ---------
_IV_MIN = {
    "minute1": 1, "minute3": 3, "minute5": 5, "minute10": 10, "minute15": 15,
    "minute30": 30, "minute60": 60, "day": 1440,
}

def _iv_min(interval: str) -> int:
    return _IV_MIN.get(interval, 10)

# v1.2025.10.18.2031
def _now_kst_naive() -> datetime:
    """
    âœ… ì‹œìŠ¤í…œ ë¡œì»¬íƒ€ì„(UTC ë“±)ì— ì˜ì¡´í•˜ì§€ ì•Šê³  KST ì‹œê°ì„ tz-awareë¡œ ë§Œë“  ë’¤ tz ì œê±°.
    - ëª¨ë“  ë°” ê²½ê³„ ê³„ì‚°ì„ 'KST-naive'ë¡œ í†µì¼í•˜ê¸° ìœ„í•¨.
    """
    kst_now = datetime.now(tz=ZoneInfo("Asia/Seoul"))
    return kst_now.replace(second=0, microsecond=0).replace(tzinfo=None)

def _floor_boundary(dt: datetime, interval: str) -> datetime:
    if interval == "day":
        return dt.replace(hour=0, minute=0, second=0, microsecond=0)
    iv = _iv_min(interval)
    m = (dt.minute // iv) * iv
    return dt.replace(minute=m, second=0, microsecond=0)

def _next_boundary(dt: datetime, interval: str) -> datetime:
    if interval == "day":
        nxt = dt.replace(hour=0, minute=0, second=0, microsecond=0)
        if dt >= nxt:
            nxt += timedelta(days=1)
        return nxt
    iv = _iv_min(interval)
    m = (dt.minute // iv + 1) * iv
    add_h = m // 60
    m = m % 60
    h = (dt.hour + add_h) % 24
    nxt = dt.replace(hour=h, minute=m, second=0, microsecond=0)
    if dt.hour + add_h >= 24:
        nxt += timedelta(days=1)
    return nxt

def _fmt_to_param(dt: datetime) -> str:
    return dt.strftime("%Y-%m-%d %H:%M:%S")


# --------- ë©”ëª¨ë¦¬ ìœ í‹¸ ---------
def _optimize_dataframe_memory(old_df, new_data, max_length):
    try:
        if len(old_df) >= max_length:
            old_df = old_df.iloc[-(max_length - 10):].copy()
        combined = pd.concat([old_df, new_data], ignore_index=False)
        result = combined.drop_duplicates().sort_index().iloc[-max_length:]
        memory_usage_mb = result.memory_usage(deep=True).sum() / 1024 / 1024
        if memory_usage_mb > 10:
            logger.warning(f"âš ï¸ DataFrame ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³¼ë‹¤: {memory_usage_mb:.2f}MB")
        return result
    except Exception as e:
        logger.error(f"âŒ DataFrame ìµœì í™” ì‹¤íŒ¨: {e}")
        return pd.concat([old_df, new_data]).drop_duplicates().sort_index().iloc[-max_length:]

def _force_memory_cleanup():
    try:
        collected = gc.collect()
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024
        logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: ê°ì²´ {collected}ê°œ ìˆ˜ì§‘, í˜„ì¬ ë©”ëª¨ë¦¬: {memory_mb:.1f}MB")
        if memory_mb > 500:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_mb:.1f}MB - ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ í•„ìš”")
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")


# --------- ë©”ì¸ ìŠ¤íŠ¸ë¦¼ ---------
def stream_candles(
    ticker: str,
    interval: str,
    q=None,
    max_retry: int = 5,
    retry_wait: int = 3,
    stop_event=None,
    max_length: int = 500,
):
    def _log(level: str, msg: str):
        (logger.warning if level == "WARN" else logger.error if level == "ERROR" else logger.info)(msg)
        if q:
            # í•­ìƒ 3-íŠœí”Œ ìœ ì§€
            prefix = "âš ï¸" if level == "WARN" else "âŒ" if level == "ERROR" else "â„¹ï¸"
            q.put((time.time(), "LOG", f"{prefix} {msg}"))

    def standardize_ohlcv(df):
        if df is None or df.empty:
            raise ValueError(f"OHLCV ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {ticker}, {interval}")
        df = df.rename(columns={"open":"Open","high":"High","low":"Low","close":"Close","volume":"Volume"})
        if "value" in df.columns:
            df = df.drop(columns=["value"])

        # ì¸ë±ìŠ¤ tz ì •ê·œí™”: tz-awareë©´ Asia/Seoulë¡œ ë³€í™˜ í›„ tz ì œê±°(naive)
        idx = pd.to_datetime(df.index)
        try:
            if getattr(idx, "tz", None) is not None:
                # tz-naiveë¼ë©´ UTCë¡œ ê°„ì£¼ í›„ ë¡œì»¬ë¼ì´ì¦ˆ
                idx = idx.tz_localize("UTC")
            # KSTë¡œ ë³€í™˜ í›„ tz ì œê±°í•˜ì—¬ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ 'KST-naive'ë¡œ í†µì¼
            idx = idx.tz_convert("Asia/Seoul").tz_localize(None)
        except Exception:
            # ì˜ˆì™¸ ì‹œì—ë„ ìµœì†Œ ì •ë ¬/ì¤‘ë³µ ì œê±°ëŠ” ìˆ˜í–‰
            pass

        df.index = idx
        # ìµœì‹  ê°’ ìš°ì„ ìœ¼ë¡œ ì¤‘ë³µ ì œê±° + ì •ë ¬
        return df.dropna().sort_index().loc[~df.index.duplicated(keep="last")]

    # ---- ì´ˆê¸° ë¡œë“œ: ë§‰ ë‹«íŒ ê²½ê³„ê¹Œì§€ ----
    base_delay = retry_wait
    df = None
    now = _now_kst_naive()
    bar_close = _floor_boundary(now, interval)
    to_param = _fmt_to_param(bar_close)

    for attempt in range(1, max_retry + 1):
        if stop_event and stop_event.is_set():
            _log("WARN", "stream_candles ì¤‘ë‹¨ë¨: ì´ˆê¸° ìˆ˜ì§‘ ì¤‘ stop_event ê°ì§€")
            return
        try:
            df = pyupbit.get_ohlcv(ticker, interval=interval, count=max_length, to=to_param)
            if df is not None and not df.empty:
                break
        except Exception as e:
            _log("ERROR", f"[ì´ˆê¸°] API ì˜ˆì™¸ ë°œìƒ: {e}")

        delay = min(base_delay * (2 ** (attempt - 1)), 60) + random.uniform(0, 5)
        _log("WARN", f"[ì´ˆê¸°] API ì‹¤íŒ¨ ({attempt}/{max_retry}), {delay:.1f}ì´ˆ í›„ ì¬ì‹œë„")
        time.sleep(delay)

    if df is None or df.empty:
        _log("ERROR", "[ì´ˆê¸°] ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨, ë¹ˆ DataFrameìœ¼ë¡œ ì‹œì‘")
        df = pd.DataFrame(columns=["Open","High","Low","Close","Volume"])
        df.index = pd.to_datetime([])

    df = standardize_ohlcv(df).drop_duplicates()
    yield df

    last_open = df.index[-1]  # ìš°ë¦¬ê°€ ê°€ì§„ ë§ˆì§€ë§‰ bar_open (tz-naive)

    # ---- ì‹¤ì‹œê°„ ë£¨í”„: ê²½ê³„ ë™ê¸°í™” â†’ ë‹«íŒ ë´‰ ì¡°íšŒ â†’ ê°­ ë°±í•„ ----
    JITTER = 0.7
    while not (stop_event and stop_event.is_set()):
        now = _now_kst_naive()
        next_close = _next_boundary(now, interval)
        sleep_sec = max(0.0, (next_close - now).total_seconds() + JITTER)
        time.sleep(sleep_sec)

        # ë§‰ ë‹«íŒ ë´‰ì˜ open
        iv = _iv_min(interval)
        boundary_open = next_close - timedelta(minutes=iv)  # ë‘˜ ë‹¤ tz-naive

        # ì¤‘ê°„ ëˆ„ë½ë¶„ ê³„ì‚°(ë¶„ ë‹¨ìœ„)
        gap = int((boundary_open - last_open).total_seconds() // (iv * 60))
        need = max(1, min(gap, 200))

        # ì¬ì‹œë„ ë£¨í”„
        new = None
        for attempt in range(1, max_retry + 1):
            if stop_event and stop_event.is_set():
                _log("WARN", "stream_candles ì¤‘ë‹¨ë¨: ì‹¤ì‹œê°„ ë£¨í”„ ì¤‘ stop_event ê°ì§€")
                return
            try:
                new = pyupbit.get_ohlcv(ticker, interval=interval, count=need, to=_fmt_to_param(next_close))
                if new is not None and not new.empty:
                    break
            except Exception as e:
                _log("ERROR", f"[ì‹¤ì‹œê°„] API ì˜ˆì™¸: {e}")
            delay = min(base_delay * (2 ** (attempt - 1)), 30) + random.uniform(0, 2)
            _log("WARN", f"[ì‹¤ì‹œê°„] API ì‹¤íŒ¨ ({attempt}/{max_retry}), {delay:.1f}ì´ˆ í›„ ì¬ì‹œë„")
            time.sleep(delay)
        else:
            backoff = min(30 + random.uniform(0, 10), 300)
            _log("ERROR", f"[ì‹¤ì‹œê°„] API ì—°ê²° ì‹¤íŒ¨, {backoff:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
            time.sleep(backoff)
            continue

        new = standardize_ohlcv(new).drop_duplicates()
        # ìš°ë¦¬ê°€ ê°€ì§„ ë§ˆì§€ë§‰ ì´í›„ ê²ƒë§Œ
        new = new[new.index > last_open]
        if new.empty:
            continue

        old_df = df
        # ì¤‘ë³µ/ì •ë ¬ì€ _optimize_dataframe_memory ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë˜ì§€ë§Œ
        # í˜¹ì‹œ ë‚¨ì€ ì¤‘ë³µì— ëŒ€í•´ ìµœì‹  ê°’ ìš°ì„ ìœ¼ë¡œ í•œ ë²ˆ ë” ë³´ì •
        # df = _optimize_dataframe_memory(df, new, max_length).loc[~_optimize_dataframe_memory(df, new, max_length).index.duplicated(keep="last")].sort_index()
        # âœ… í•œ ë²ˆë§Œ ê³„ì‚°í•œ ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ í˜¸ì¶œ/ë ˆì´ìŠ¤ ìœ„í—˜ ì œê±°
        tmp = _optimize_dataframe_memory(df, new, max_length)
        df = tmp.loc[~tmp.index.duplicated(keep="last")].sort_index()
        del old_df

        last_open = df.index[-1]
        # ì‚¬ìš©ì í˜¼ë€ ë°©ì§€ìš© ë™ê¸°í™” ë¡œê·¸ (bar_open / bar_close ëª…ì‹œ)
        if q:
            last_close = last_open + timedelta(minutes=iv)
            # run_at = datetime.now()
            run_at = _now_kst_naive()  # âœ… KST-naiveë¡œ ê¸°ë¡ í†µì¼
            q.put((
                time.time(),
                "LOG",
                f"â± run_at={run_at:%Y-%m-%d %H:%M:%S} | bar_open={last_open} | bar_close={last_close} "
            ))

        # ì£¼ê¸°ì  GC
        if hasattr(_optimize_dataframe_memory, "last_gc_time"):
            if time.time() - _optimize_dataframe_memory.last_gc_time > 300:
                _force_memory_cleanup()
                _optimize_dataframe_memory.last_gc_time = time.time()
        else:
            _optimize_dataframe_memory.last_gc_time = time.time()

        yield df


_INTERVAL_MAP = {
    "minute1": "minute1",
    "minute3": "minute3",
    "minute5": "minute5",
    "minute10": "minute10",
    "minute15": "minute15",
    "minute30": "minute30",
    "minute60": "minute60",
    "minute240": "minute240",
    "day": "day",
    "week": "week",
}

# get_ohlcv_once() ì£¼ì„ ë° ì¸ë±ìŠ¤ ì •ê·œí™” ìˆ˜ì •
def get_ohlcv_once(ticker: str, interval_code: str, count: int = 500) -> pd.DataFrame:
    """
    ëŒ€ì‹œë³´ë“œìš© ì›ìƒ· OHLCV.
    âœ… ë°˜í™˜: columns = [Open, High, Low, Close, Volume], DatetimeIndex = 'KST-naive' (streamê³¼ ë™ì¼ ê¸°ì¤€)
    """
    interval = _INTERVAL_MAP.get(interval_code, "minute1")
    df = pyupbit.get_ohlcv(ticker=ticker, interval=interval, count=count)
    if df is None or df.empty:
        return pd.DataFrame(columns=["Open","High","Low","Close","Volume"])

    # pyupbit ì¸ë±ìŠ¤ê°€ tz-naive(=UTC)ì¼ ê°€ëŠ¥ì„± ë†’ìŒ â†’ KST-naiveë¡œ í†µì¼
    if isinstance(df.index, pd.DatetimeIndex):
        idx = pd.to_datetime(df.index)
        if getattr(idx, "tz", None) is None:
            idx = idx.tz_localize("UTC")
        idx = idx.tz_convert("Asia/Seoul").tz_localize(None)
        df.index = idx

    return df[["open","high","low","close","volume"]].rename(
        columns={"open":"Open","high":"High","low":"Low","close":"Close","volume":"Volume"}
    )
