from __future__ import annotations
import pyupbit
import pandas as pd
import time
import logging
import random
import gc
import psutil
import os
import math
from datetime import datetime, timedelta

from zoneinfo import ZoneInfo


logger = logging.getLogger(__name__)


# --------- ì‹œê°„/ê²½ê³„ ìœ í‹¸ (KST naiveë¡œ ì¼ê´€) ---------
_IV_MIN = {
    "minute1": 1,
    "minute3": 3,
    "minute5": 5,
    "minute10": 10,
    "minute15": 15,
    "minute30": 30,
    "minute60": 60,
    "day": 1440,
}

# --------- JITTER ê°’ (intervalë³„ ì°¨ë“± ì ìš©) ---------
# ë´‰ ì¢…ê°€ í™•ì • í›„ ì¶”ê°€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
# âš ï¸ ì¤‘ìš”: Upbit APIëŠ” ë´‰ ì¢…ê°€ í™•ì • í›„ ë°ì´í„° ì¤€ë¹„ê¹Œì§€ ì‹œê°„ì´ ê±¸ë¦¼
# - ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼: ì›¹ì‚¬ì´íŠ¸ì—ëŠ” ë°ì´í„°ê°€ ìˆì§€ë§Œ APIëŠ” 4~5ì´ˆ ì§€ì—°
# - ë„ˆë¬´ ì§§ìœ¼ë©´: ë°ì´í„° ëˆ„ë½ â†’ ë°±í•„ ì‹¤íŒ¨ â†’ ì˜êµ¬ ëˆ„ë½ (ì¹˜ëª…ì !)
# - ê¶Œì¥: 1ë¶„ë´‰ 3ì´ˆ, 3ë¶„ë´‰ 6ì´ˆ, ì¥ê¸°ë´‰ 8~15ì´ˆ
# - ì‹¤ì‹œê°„ì„±ë³´ë‹¤ ì•ˆì •ì„± ìš°ì„  (ëˆ„ë½ ë°©ì§€ê°€ ìµœìš°ì„ )
# - ë°±í•„ ë¡œì§(5íšŒ ì¬ì‹œë„)ì´ ì¶”ê°€ ì•ˆì „ì¥ì¹˜ ì—­í• 
JITTER_BY_INTERVAL = {
    "minute1": 3.0,   # 1ë¶„ë´‰: Upbit API ë°ì´í„° ì¤€ë¹„ ì‹œê°„ í™•ë³´ (ê¸°ì¡´ 1.5 â†’ 3.0)
    "minute3": 6.0,   # 3ë¶„ë´‰: ëˆ„ë½ ë°©ì§€ ìµœìš°ì„  (ê¸°ì¡´ 2.0 â†’ 6.0)
    "minute5": 6.0,   # 5ë¶„ë´‰: ì•ˆì •ì„± ê°•í™” (ê¸°ì¡´ 2.0 â†’ 6.0)
    "minute10": 8.0,  # 10ë¶„ë´‰: ì¶©ë¶„í•œ ëŒ€ê¸° ì‹œê°„ (ê¸°ì¡´ 2.5 â†’ 8.0)
    "minute15": 8.0,  # 15ë¶„ë´‰: ì•ˆì •ì„± ìµœìš°ì„  (ê¸°ì¡´ 2.5 â†’ 8.0)
    "minute30": 10.0, # 30ë¶„ë´‰: ì•ˆì •ì„± ìµœìš°ì„  (ê¸°ì¡´ 2.5 â†’ 10.0)
    "minute60": 10.0, # 60ë¶„ë´‰: ì•ˆì •ì„± ìµœìš°ì„  (ê¸°ì¡´ 3.0 â†’ 10.0)
    "day": 15.0,      # ì¼ë´‰: ì‹¤ì‹œê°„ì„±ë³´ë‹¤ ì•ˆì •ì„± ìš°ì„  (ê¸°ì¡´ 3.0 â†’ 15.0)
}

# --------- í•„ìˆ˜ ë°ì´í„° ê°œìˆ˜ ì •ì˜ (ëª©í‘œì¹˜) ---------
# âš ï¸ ì£¼ì˜: Upbit APIëŠ” ê³¼ê±° ë°ì´í„° ì œì•½ìœ¼ë¡œ ëª©í‘œì¹˜ë¥¼ ëª» ì±„ìš¸ ìˆ˜ ìˆìŒ
# â†’ ì ˆëŒ€ ìµœì†ŒëŸ‰(ABSOLUTE_MIN_CANDLES)ë§Œ ì¶©ì¡±í•˜ë©´ ì „ëµ ì‹¤í–‰ í—ˆìš©
REQUIRED_CANDLES = {
    "minute1": 2000,   # 1ë¶„ë´‰: 2000ê°œ (ëª©í‘œ, Upbit ì‹¤ì œ ì œì•½: ~800ê°œ)
    "minute3": 1500,   # 3ë¶„ë´‰: 1500ê°œ (ëª©í‘œ)
    "minute5": 1200,   # 5ë¶„ë´‰: 1200ê°œ (ëª©í‘œ)
    "minute10": 1000,  # 10ë¶„ë´‰: 1000ê°œ (ëª©í‘œ)
    "minute15": 800,   # 15ë¶„ë´‰: 800ê°œ (ëª©í‘œ)
    "minute30": 600,   # 30ë¶„ë´‰: 600ê°œ (ëª©í‘œ)
    "minute60": 500,   # 60ë¶„ë´‰: 500ê°œ (ëª©í‘œ)
    "day": 400,        # ì¼ë´‰: 400ê°œ (ëª©í‘œ)
}

# ì ˆëŒ€ ìµœì†Œ ìº”ë“¤ ê°œìˆ˜ (ì´ ê°’ ë¯¸ë§Œì´ë©´ ì „ëµ ì‹œì‘ ë¶ˆê°€)
# - ì „ëµë³„ë¡œ ë‹¤ë¥¸ ìµœì†Œê°’ ì ìš©
ABSOLUTE_MIN_CANDLES = {
    "MACD": 600,  # MACD: ìµœëŒ€ íŒŒë¼ë¯¸í„° Ã— 3
    "EMA": 200,   # EMA: ìµœëŒ€ íŒŒë¼ë¯¸í„° (slow_period=200 ê¸°ì¤€)
}
ABSOLUTE_MIN_CANDLES_DEFAULT = 600  # ì „ëµ ë¯¸ì§€ì • ì‹œ ê¸°ë³¸ê°’

# ëª©í‘œ ëŒ€ë¹„ ê²½ê³  ë¹„ìœ¨ (ì´ ë¹„ìœ¨ ë¯¸ë§Œì´ë©´ ê²½ê³ ë§Œ í‘œì‹œ)
WARNING_RATIO = 0.5  # 50%


# ë””í„°ë¯¸ë‹ˆì¦˜ ì²´í¬ ë¡œê·¸ í—¬í¼
def log_det(df: pd.DataFrame, tag: str):
    """
    dfê°€ í˜„ì¬ ë™ì¼í•œ ë´‰ ì§‘í•©ì¸ì§€ ë¹ ë¥´ê²Œ ê²€ì¦í•˜ê¸° ìœ„í•œ ë¡œê·¸.
    - rows/first/last + OHLCV ì²´í¬ì„¬ì„ ë‚¨ê¸´ë‹¤.
    - tag: í˜¸ì¶œ ì§€ì  êµ¬ë¶„ìš©(ex: PRE_INIT, LOOP_MERGED, ONCE_BEFORE_RETURN)
    """
    if df is None or df.empty:
        logger.info(f"[DET] {tag} | rows=0 (empty)")
        return
    try:
        rows = len(df)
        first_i, last_i = df.index[0], df.index[-1]
        # OHLCVë§Œ ì‚¬ìš©, ì†Œìˆ˜ 8ìë¦¬ ë°˜ì˜¬ë¦¼ í›„ ë¬¸ìì—´ â†’ í•´ì‹œ
        payload = df[["Open","High","Low","Close","Volume"]].round(8).to_csv(index=True, header=False)
        checksum = hash(payload)  # íŒŒì´ì¬ ë‚´ì¥ í•´ì‹œ(ì„¸ì…˜ë§ˆë‹¤ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ, ê°™ì€ í”„ë¡œì„¸ìŠ¤ ë¹„êµìš©)
        logger.info(f"[DET] {tag} | rows={rows} | first={first_i} | last={last_i} | checksum={checksum}")
    except Exception as e:
        logger.warning(f"[DET] {tag} | logging failed: {e}")


def _iv_min(interval: str) -> int:
    return _IV_MIN.get(interval, 10)

# v1.2025.10.18.2031
def _now_kst_naive() -> datetime:
    """
    âœ… ì‹œìŠ¤í…œ ë¡œì»¬íƒ€ì„(UTC ë“±)ì— ì˜ì¡´í•˜ì§€ ì•Šê³  KST ì‹œê°ì„ tz-awareë¡œ ë§Œë“  ë’¤ tz ì œê±°.
    - ëª¨ë“  ë°” ê²½ê³„ ê³„ì‚°ì„ 'KST-naive'ë¡œ í†µì¼í•˜ê¸° ìœ„í•¨.
    """
    kst_now = datetime.now(tz=ZoneInfo("Asia/Seoul"))
    return kst_now.replace(second=0, microsecond=0).replace(tzinfo=None)

def _floor_boundary(dt: datetime, interval: str) -> datetime:
    if interval == "day":
        return dt.replace(hour=0, minute=0, second=0, microsecond=0)
    iv = _iv_min(interval)
    m = (dt.minute // iv) * iv
    return dt.replace(minute=m, second=0, microsecond=0)

def _next_boundary(dt: datetime, interval: str) -> datetime:
    if interval == "day":
        nxt = dt.replace(hour=0, minute=0, second=0, microsecond=0)
        if dt >= nxt:
            nxt += timedelta(days=1)
        return nxt
    iv = _iv_min(interval)
    m = (dt.minute // iv + 1) * iv
    add_h = m // 60
    m = m % 60
    h = (dt.hour + add_h) % 24
    nxt = dt.replace(hour=h, minute=m, second=0, microsecond=0)
    if dt.hour + add_h >= 24:
        nxt += timedelta(days=1)
    return nxt

def _fmt_to_param(dt: datetime) -> str:
    return dt.strftime("%Y-%m-%d %H:%M:%S")


# --------- ë©”ëª¨ë¦¬ ìœ í‹¸ ---------
def _optimize_dataframe_memory(old_df, new_data, max_length):
    try:
        if len(old_df) >= max_length:
            old_df = old_df.iloc[-(max_length - 10):].copy()
        combined = pd.concat([old_df, new_data], ignore_index=False)
        result = combined.drop_duplicates().sort_index().iloc[-max_length:]
        memory_usage_mb = result.memory_usage(deep=True).sum() / 1024 / 1024
        if memory_usage_mb > 10:
            logger.warning(f"âš ï¸ DataFrame ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³¼ë‹¤: {memory_usage_mb:.2f}MB")
        return result
    except Exception as e:
        logger.error(f"âŒ DataFrame ìµœì í™” ì‹¤íŒ¨: {e}")
        return pd.concat([old_df, new_data]).drop_duplicates().sort_index().iloc[-max_length:]

def _force_memory_cleanup():
    try:
        collected = gc.collect()
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024
        logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: ê°ì²´ {collected}ê°œ ìˆ˜ì§‘, í˜„ì¬ ë©”ëª¨ë¦¬: {memory_mb:.1f}MB")
        if memory_mb > 500:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_mb:.1f}MB - ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ í•„ìš”")
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")


# --------- ë©”ì¸ ìŠ¤íŠ¸ë¦¼ ---------
def stream_candles(
    ticker: str,
    interval: str,
    q=None,
    max_retry: int = 5,
    retry_wait: int = 3,
    stop_event=None,
    max_length: int = 500,
    user_id: str = None,  # Phase 2: ìºì‹œ ì‚¬ìš©ì„ ìœ„í•œ user_id
    strategy_type: str = None,  # ì „ëµ íƒ€ì… (MACD/EMA)
):
    # âœ… ë°ì´í„° ìˆ˜ì§‘ ìƒíƒœ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ import
    if user_id:
        try:
            from services.db import update_data_collection_status, clear_data_collection_status
        except ImportError:
            update_data_collection_status = None
            clear_data_collection_status = None
    else:
        update_data_collection_status = None
        clear_data_collection_status = None
    def _log(level: str, msg: str):
        (logger.warning if level == "WARN" else logger.error if level == "ERROR" else logger.info)(msg)
        if q:
            # í•­ìƒ 3-íŠœí”Œ ìœ ì§€
            prefix = "âš ï¸" if level == "WARN" else "âŒ" if level == "ERROR" else "â„¹ï¸"
            q.put((time.time(), "LOG", f"{prefix} {msg}"))

    def standardize_ohlcv(df):
        if df is None or df.empty:
            raise ValueError(f"OHLCV ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {ticker}, {interval}")

        before_count = len(df)
        _log("INFO", f"[standardize] ì…ë ¥ ë°ì´í„°: {before_count}ê°œ, index type={type(df.index)}, tz={getattr(df.index, 'tz', 'N/A')}")

        df = df.rename(columns={"open":"Open","high":"High","low":"Low","close":"Close","volume":"Volume"})
        if "value" in df.columns:
            df = df.drop(columns=["value"])

        # ì¸ë±ìŠ¤ tz ì •ê·œí™”: KST naiveë¡œ í†µì¼
        # âš ï¸ ì¤‘ìš”: pyupbitì€ ì´ë¯¸ KST ì‹œê°„ëŒ€ë¡œ tz-naive ë°ì´í„°ë¥¼ ë°˜í™˜í•¨
        idx = pd.to_datetime(df.index)
        try:
            if getattr(idx, "tz", None) is None:
                # âœ… pyupbitì€ ì´ë¯¸ KST naiveë¡œ ë°˜í™˜í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                _log("INFO", f"[standardize] tz-naive ê°ì§€ â†’ pyupbitì€ ì´ë¯¸ KSTì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©")
            else:
                # tz-awareì¸ ê²½ìš°ì—ë§Œ KSTë¡œ ë³€í™˜ í›„ tz ì œê±°
                _log("INFO", f"[standardize] tz-aware ê°ì§€ (tz={idx.tz}) â†’ KSTë¡œ ë³€í™˜")
                idx = idx.tz_convert("Asia/Seoul").tz_localize(None)
                _log("INFO", f"[standardize] KST naiveë¡œ ë³€í™˜ ì™„ë£Œ")
        except Exception as e:
            # ì˜ˆì™¸ ë°œìƒ ì‹œ ìƒì„¸ ë¡œê·¸
            _log("ERROR", f"[standardize] íƒ€ì„ì¡´ ë³€í™˜ ì‹¤íŒ¨: {e}")
            # ë³€í™˜ ì‹¤íŒ¨ ì‹œì—ë„ ìµœì†Œí•œ ì •ë ¬ì€ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ idx ê·¸ëŒ€ë¡œ ì‚¬ìš©

        df.index = idx

        # dropna ì „ NaN ê°œìˆ˜ í™•ì¸
        na_counts = df.isna().sum()
        if na_counts.any():
            _log("WARN", f"[standardize] NaN ë°œê²¬: {na_counts[na_counts > 0].to_dict()}")

        # ì •ë ¬ í›„ ì¤‘ë³µ ì œê±° (dropnaëŠ” ë‚˜ì¤‘ì—)
        df = df.sort_index()
        before_dedup = len(df)
        df = df.loc[~df.index.duplicated(keep="last")]
        after_dedup = len(df)

        if before_dedup > after_dedup:
            _log("WARN", f"[standardize] ì¤‘ë³µ ì œê±°: {before_dedup - after_dedup}ê°œ ì‚­ì œ ({before_dedup} â†’ {after_dedup})")

        # NaN ì œê±°
        df = df.dropna()
        after_dropna = len(df)

        if after_dedup > after_dropna:
            _log("WARN", f"[standardize] NaN ì œê±°: {after_dedup - after_dropna}ê°œ ì‚­ì œ ({after_dedup} â†’ {after_dropna})")

        _log("INFO", f"[standardize] ìµœì¢… ì¶œë ¥: {after_dropna}ê°œ (ì†ì‹¤: {before_count - after_dropna}ê°œ, {100*(before_count-after_dropna)/before_count:.1f}%)")

        return df

    # â˜… ì´ˆê¸° íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ìš© í—¬í¼
    def _fetch_initial_history(to_param: str, retry_full: int = 3) -> pd.DataFrame:
        """
        UpbitëŠ” ë¶„ë´‰ ê¸°ì¤€ í•œ ë²ˆì— ìµœëŒ€ 200ê°œë§Œ ë°˜í™˜í•˜ë¯€ë¡œ,
        max_lengthê°€ 200ì„ ë„˜ëŠ” ê²½ìš° ì—¬ëŸ¬ ë²ˆ ë‚˜ëˆ ì„œ ê³¼ê±° íˆìŠ¤í† ë¦¬ë¥¼ ëª¨ì€ë‹¤.
        - MACD/EMAë¥¼ HTS ìˆ˜ì¤€ìœ¼ë¡œ ë§ì¶”ê¸° ìœ„í•œ ê¸´ íˆìŠ¤í† ë¦¬(ì˜ˆ: 3ë¶„ë´‰ 1500~2000ê°œ) í™•ë³´ìš©.
        - retry_full: ì „ì²´ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íšŸìˆ˜
        """
        iv_min = _iv_min(interval)
        remaining = max_length
        current_to = to_param
        chunks: list[pd.DataFrame] = []
        base_delay_local = retry_wait
        total_requested = max_length
        api_calls = 0
        start_time = time.time()

        expected_calls = (max_length + 199) // 200  # ì˜¬ë¦¼ ê³„ì‚°
        expected_time = expected_calls * 0.15  # API í˜¸ì¶œë‹¹ ì•½ 0.15ì´ˆ (0.1ì´ˆ ë”œë ˆì´ + ë„¤íŠ¸ì›Œí¬)
        _log("INFO", f"[ì´ˆê¸°-multi] íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì‹œì‘: max_length={max_length}, interval={interval}")
        _log("INFO", f"[ì´ˆê¸°-multi] ì˜ˆìƒ: API í˜¸ì¶œ {expected_calls}íšŒ, ì†Œìš” ì‹œê°„ ì•½ {expected_time:.1f}ì´ˆ")

        # âœ… ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ìƒíƒœ ì €ì¥
        if update_data_collection_status:
            update_data_collection_status(
                user_id=user_id,
                is_collecting=True,
                collected=0,
                target=max_length,
                progress=0.0,
                estimated_time=expected_time,
                message=f"ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ({interval}ë´‰, ëª©í‘œ: {max_length}ê°œ)"
            )

        while remaining > 0:
            if stop_event and stop_event.is_set():
                collected = sum(len(c) for c in chunks)
                _log("WARN", f"[ì´ˆê¸°-multi] stop_event ê°ì§€ â†’ ìˆ˜ì§‘ ì¤‘ë‹¨ (collected={collected}/{total_requested})")
                break

            per_call = min(200, remaining)  # Upbit ë¶„ë´‰ ìµœëŒ€ 200ê°œ
            df_part = None
            api_calls += 1

            for attempt in range(1, max_retry + 1):
                try:
                    _log("INFO", f"[ì´ˆê¸°-multi] API í˜¸ì¶œ #{api_calls}: count={per_call}, to={current_to}")
                    df_part = pyupbit.get_ohlcv(
                        ticker,
                        interval=interval,
                        count=per_call,
                        to=current_to,
                    )
                    if df_part is not None and not df_part.empty:
                        _log("INFO", f"[ì´ˆê¸°-multi] API ì‘ë‹µ ì„±ê³µ: {len(df_part)}ê°œ ìˆ˜ì‹ ")
                        # ğŸ” PRICE-DEBUG: multi-fetch ë§ˆì§€ë§‰ í˜¸ì¶œì˜ ì›ë³¸ ë°ì´í„° (api_calls==1ì¼ë•Œë§Œ)
                        if api_calls == 1:
                            try:
                                last_3 = df_part.tail(3)
                                for idx, row in last_3.iterrows():
                                    _log("INFO", f"[PRICE-API-RAW-MULTI] {idx} | O={row['open']:.0f} H={row['high']:.0f} L={row['low']:.0f} C={row['close']:.0f}")
                            except Exception as e_log:
                                _log("WARN", f"[PRICE-API-RAW-MULTI] ë¡œê¹… ì‹¤íŒ¨: {e_log}")
                        break
                    else:
                        _log("WARN", f"[ì´ˆê¸°-multi] API ì‘ë‹µì´ ë¹„ì–´ìˆìŒ (attempt {attempt}/{max_retry})")
                except Exception as e:
                    _log("ERROR", f"[ì´ˆê¸°-multi] API ì˜ˆì™¸ ë°œìƒ: {e} (attempt {attempt}/{max_retry})")

                # Upbit API rate limit ëŒ€ì‘: í˜¸ì¶œ ê°„ ìµœì†Œ 0.1ì´ˆ ë”œë ˆì´
                delay = min(base_delay_local * (2 ** (attempt - 1)), 60) + random.uniform(0.1, 1.0)
                _log("WARN", f"[ì´ˆê¸°-multi] API ì¬ì‹œë„ ëŒ€ê¸°: {delay:.1f}ì´ˆ")
                time.sleep(delay)
            else:
                # max_retry ì‹¤íŒ¨ ì‹œ - ë¶€ë¶„ ìˆ˜ì§‘ ë°ì´í„°ë¼ë„ ë°˜í™˜í•˜ë„ë¡ ê°œì„ 
                collected = sum(len(c) for c in chunks)
                _log("ERROR", f"[ì´ˆê¸°-multi] API ì—°ì† ì‹¤íŒ¨ (collected={collected}/{total_requested})")
                # break ëŒ€ì‹  ê²½ê³ ë§Œ ë‚¨ê¸°ê³  ìˆ˜ì§‘ëœ ë°ì´í„° ë°˜í™˜
                break

            if df_part is None or df_part.empty:
                collected = sum(len(c) for c in chunks)
                _log("WARN", f"[ì´ˆê¸°-multi] ë¹ˆ ì‘ë‹µìœ¼ë¡œ ìˆ˜ì§‘ ì¢…ë£Œ (collected={collected}/{total_requested})")
                break

            chunks.append(df_part)
            got = len(df_part)
            remaining -= got

            collected_so_far = sum(len(c) for c in chunks)
            progress = collected_so_far / total_requested
            remaining_time = remaining * 0.15
            _log("INFO", f"[ì´ˆê¸°-multi] ì§„í–‰: {collected_so_far}/{total_requested} ({100*progress:.1f}%)")

            # âœ… ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
            if update_data_collection_status:
                update_data_collection_status(
                    user_id=user_id,
                    is_collecting=True,
                    collected=collected_so_far,
                    target=total_requested,
                    progress=progress,
                    estimated_time=remaining_time,
                    message=f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ({collected_so_far}/{total_requested})"
                )

            if got < per_call:
                # Upbit APIê°€ ìš”ì²­ëŸ‰ë³´ë‹¤ ì ê²Œ ë°˜í™˜ = ë” ì´ìƒ ê³¼ê±° ë°ì´í„° ì—†ìŒ
                _log("WARN", f"[ì´ˆê¸°-multi] APIê°€ ìš”ì²­ëŸ‰ë³´ë‹¤ ì ê²Œ ë°˜í™˜ (got={got}, requested={per_call}) â†’ ê³¼ê±° ë°ì´í„° ì†Œì§„")
                break

            # ë‹¤ìŒ ìš”ì²­ìš© 'to'ëŠ” ì´ë²ˆ ê¸°ì¤€ì‹œê°„ì—ì„œ got*interval ë§Œí¼ ê³¼ê±°ë¡œ ì´ë™
            try:
                dt_to = datetime.strptime(current_to, "%Y-%m-%d %H:%M:%S")
                dt_to -= timedelta(minutes=iv_min * got)
                current_to = _fmt_to_param(dt_to)
            except Exception as e:
                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¶”ê°€ í˜ì´ì§•ì€ í•˜ì§€ ì•Šê³  ì¢…ë£Œ
                collected = sum(len(c) for c in chunks)
                _log("ERROR", f"[ì´ˆê¸°-multi] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {e} (collected={collected}/{total_requested})")
                break

            # API rate limit ì¤€ìˆ˜: í˜¸ì¶œ ê°„ 0.1ì´ˆ ë”œë ˆì´
            time.sleep(0.1)

        if not chunks:
            _log("ERROR", f"[ì´ˆê¸°-multi] ìˆ˜ì§‘ ì‹¤íŒ¨: ë°ì´í„° ì—†ìŒ")
            return pd.DataFrame(columns=["Open","High","Low","Close","Volume"])

        raw = pd.concat(chunks)
        final_count = len(raw)
        success_rate = 100 * final_count / total_requested if total_requested > 0 else 0
        elapsed_time = time.time() - start_time
        _log("INFO", f"[ì´ˆê¸°-multi] ìˆ˜ì§‘ ì™„ë£Œ: {final_count}/{total_requested} ({success_rate:.1f}%), API í˜¸ì¶œ {api_calls}íšŒ, ì†Œìš”ì‹œê°„ {elapsed_time:.2f}ì´ˆ")

        # ğŸ” PRICE-DEBUG: concat í›„ ìµœì¢… ì›ë³¸ ë°ì´í„° (ë³€í™˜ ì „)
        try:
            last_3 = raw.tail(3)
            for idx, row in last_3.iterrows():
                _log("INFO", f"[PRICE-API-CONCAT] {idx} | O={row['open']:.0f} H={row['high']:.0f} L={row['low']:.0f} C={row['close']:.0f}")
        except Exception as e_log:
            _log("WARN", f"[PRICE-API-CONCAT] ë¡œê¹… ì‹¤íŒ¨: {e_log}")

        return raw
    
    # ---- ì´ˆê¸° ë¡œë“œ: ë§‰ ë‹«íŒ ê²½ê³„ê¹Œì§€ ----
    base_delay = retry_wait
    df = None
    now = _now_kst_naive()
    bar_close = _floor_boundary(now, interval)
    to_param = _fmt_to_param(bar_close)

    # â˜… Phase 2: DB ìºì‹œ ìš°ì„  í™•ì¸
    # âš ï¸ TEMPORARY: íƒ€ì„ì¡´ ìˆ˜ì • í›„ ìºì‹œ ë¬´íš¨í™” (ì˜ëª»ëœ íƒ€ì„ìŠ¤íƒ¬í”„ ë°©ì§€)
    if False and user_id:  # ìºì‹œ ë¡œì§ ì„ì‹œ ë¹„í™œì„±í™”
        try:
            from services.db import load_candle_cache
            cached_df = load_candle_cache(user_id, ticker, interval, max_length)

            if cached_df is not None and len(cached_df) >= max_length * 0.9:  # 90% ì´ìƒì´ë©´ ì‚¬ìš©
                df = cached_df
                _log("INFO", f"[CACHE-HIT] {len(df)} candles loaded from DB cache (skip API)")
            elif cached_df is not None:
                _log("INFO", f"[CACHE-PARTIAL] {len(cached_df)} candles in cache (insufficient, will fetch from API)")
        except Exception as e:
            _log("WARN", f"[CACHE] Load failed, will use API: {e}")

    _log("INFO", "[CACHE] íƒ€ì„ì¡´ ìˆ˜ì • í›„ ìºì‹œ ì„ì‹œ ë¹„í™œì„±í™” - APIì—ì„œ ì§ì ‘ ìˆ˜ì§‘")

    # âœ… ì „ëµë³„ ìµœì†Œ ìº”ë“¤ ê°œìˆ˜ ê²°ì •
    strategy_tag = (strategy_type or "MACD").upper().strip()
    absolute_min = ABSOLUTE_MIN_CANDLES.get(strategy_tag, ABSOLUTE_MIN_CANDLES_DEFAULT)
    _log("INFO", f"[ì´ˆê¸°] strategy={strategy_tag}, absolute_min_candles={absolute_min}")

    # â˜… ìºì‹œ ë¯¸ìŠ¤ ë˜ëŠ” ë¶€ì¡±: API í˜¸ì¶œ
    if df is None:
        _log("INFO", f"[ì´ˆê¸°] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: ticker={ticker}, interval={interval}, max_length={max_length}")

        if max_length <= 200:
            for attempt in range(1, max_retry + 1):
                if stop_event and stop_event.is_set():
                    _log("WARN", "stream_candles ì¤‘ë‹¨ë¨: ì´ˆê¸° ìˆ˜ì§‘ ì¤‘ stop_event ê°ì§€")
                    return
                try:
                    # âœ… FIX: to íŒŒë¼ë¯¸í„° ì œê±° - í™•ì •ëœ ìµœê·¼ ë´‰ë§Œ ì¡°íšŒ
                    _log("INFO", f"[ì´ˆê¸°] API ë‹¨ì¼ í˜¸ì¶œ: count={max_length}")
                    df = pyupbit.get_ohlcv(ticker, interval=interval, count=max_length)
                    if df is not None and not df.empty:
                        _log("INFO", f"[ì´ˆê¸°] API ì‘ë‹µ ì„±ê³µ: {len(df)}ê°œ ìˆ˜ì‹ ")
                        # ğŸ” PRICE-DEBUG: pyupbit ì›ë³¸ ë°ì´í„° (ë³€í™˜ ì „)
                        try:
                            last_3 = df.tail(3)
                            for idx, row in last_3.iterrows():
                                _log("INFO", f"[PRICE-API-RAW] {idx} | O={row['open']:.0f} H={row['high']:.0f} L={row['low']:.0f} C={row['close']:.0f}")
                        except Exception as e_log:
                            _log("WARN", f"[PRICE-API-RAW] ë¡œê¹… ì‹¤íŒ¨: {e_log}")
                        break
                except Exception as e:
                    _log("ERROR", f"[ì´ˆê¸°] API ì˜ˆì™¸ ë°œìƒ: {e}")

                delay = min(base_delay * (2 ** (attempt - 1)), 60) + random.uniform(0, 5)
                _log("WARN", f"[ì´ˆê¸°] API ì‹¤íŒ¨ ({attempt}/{max_retry}), {delay:.1f}ì´ˆ í›„ ì¬ì‹œë„")
                time.sleep(delay)
        else:
            # â˜… MACD/EMA ì•ˆì •í™”ë¥¼ ìœ„í•´ ê¸´ íˆìŠ¤í† ë¦¬(max_length) í™•ë³´ + ì¬ì‹œë„
            _log("INFO", f"[ì´ˆê¸°] max_length > 200 â†’ multi-fetch ëª¨ë“œ ì‚¬ìš© (ìµœëŒ€ 3íšŒ ì¬ì‹œë„)")

            retry_count = 0
            max_full_retry = 3

            while retry_count < max_full_retry:
                df = _fetch_initial_history(to_param, retry_full=max_full_retry)

                if df is not None and not df.empty:
                    temp_len = len(df)
                    success_rate = 100 * temp_len / max_length if max_length > 0 else 0

                    # ì ˆëŒ€ ìµœì†ŒëŸ‰ ì´ìƒì´ë©´ ì„±ê³µ (Upbit API ì œì•½ ê³ ë ¤)
                    if temp_len >= absolute_min:
                        _log("INFO", f"[ì´ˆê¸°-ì¬ì‹œë„] ìˆ˜ì§‘ ì„±ê³µ: {temp_len}/{max_length} ({success_rate:.1f}%) - ì ˆëŒ€ ìµœì†ŒëŸ‰({absolute_min}) ì¶©ì¡±")
                        break
                    else:
                        retry_count += 1
                        if retry_count < max_full_retry:
                            retry_delay = 5 + random.uniform(0, 3)
                            _log("WARN", f"[ì´ˆê¸°-ì¬ì‹œë„] ì ˆëŒ€ ë¶€ì¡± ({temp_len}/{absolute_min}) - {retry_delay:.1f}ì´ˆ í›„ ì „ì²´ ì¬ì‹œë„ ({retry_count}/{max_full_retry})")
                            time.sleep(retry_delay)
                        else:
                            _log("ERROR", f"[ì´ˆê¸°-ì¬ì‹œë„] ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬: {temp_len}/{absolute_min} (ì ˆëŒ€ ìµœì†ŒëŸ‰ ë¯¸ë‹¬)")
                else:
                    retry_count += 1
                    if retry_count < max_full_retry:
                        retry_delay = 5 + random.uniform(0, 3)
                        _log("ERROR", f"[ì´ˆê¸°-ì¬ì‹œë„] ìˆ˜ì§‘ ì‹¤íŒ¨ - {retry_delay:.1f}ì´ˆ í›„ ì „ì²´ ì¬ì‹œë„ ({retry_count}/{max_full_retry})")
                        time.sleep(retry_delay)

        # â˜… Phase 2: API í˜¸ì¶œ í›„ DBì— ì €ì¥
        if user_id and df is not None and not df.empty:
            try:
                from services.db import save_candle_cache
                save_candle_cache(user_id, ticker, interval, df)
            except Exception as e:
                _log("WARN", f"[CACHE] Save failed (ignored): {e}")

    if df is None or df.empty:
        raise ValueError(f"[ì´ˆê¸°] ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: ticker={ticker}, interval={interval}")

    _log("INFO", f"[ì´ˆê¸°] ìˆ˜ì§‘ëœ ì›ë³¸ ë°ì´í„°: {len(df)}ê°œ")

    # ğŸ” PRICE-DEBUG: standardize ì „ ë°ì´í„° (API ì§í›„)
    try:
        last_3 = df.tail(3)
        for idx, row in last_3.iterrows():
            _log("INFO", f"[PRICE-BEFORE-STD] {idx} | O={row['open']:.0f} H={row['high']:.0f} L={row['low']:.0f} C={row['close']:.0f}")
    except Exception as e_log:
        _log("WARN", f"[PRICE-BEFORE-STD] ë¡œê¹… ì‹¤íŒ¨: {e_log}")

    df = standardize_ohlcv(df).drop_duplicates()
    final_len = len(df)

    # ğŸ” PRICE-DEBUG: standardize í›„ ë°ì´í„°
    try:
        last_3 = df.tail(3)
        for idx, row in last_3.iterrows():
            _log("INFO", f"[PRICE-AFTER-STD] {idx} | O={row['Open']:.0f} H={row['High']:.0f} L={row['Low']:.0f} C={row['Close']:.0f}")
    except Exception as e_log:
        _log("WARN", f"[PRICE-AFTER-STD] ë¡œê¹… ì‹¤íŒ¨: {e_log}")
    success_rate = 100 * final_len / max_length if max_length > 0 else 0

    _log("INFO", f"[ì´ˆê¸°] standardize í›„ ìµœì¢… ë°ì´í„°: {final_len}ê°œ (ëª©í‘œ: {max_length}ê°œ, ë‹¬ì„±ë¥ : {success_rate:.1f}%)")

    # â˜… ì ˆëŒ€ ìµœì†ŒëŸ‰ ê²€ì¦ (Upbit API ì œì•½ ê³ ë ¤)
    if final_len < absolute_min:
        raise ValueError(
            f"âŒ ë°ì´í„° ì ˆëŒ€ ë¶€ì¡±ìœ¼ë¡œ ì „ëµ ì‹œì‘ ì°¨ë‹¨: {final_len}/{absolute_min} (ì ˆëŒ€ ìµœì†ŒëŸ‰) "
            f"- MA ê³„ì‚°ì— ìµœì†Œ {absolute_min}ê°œ í•„ìš” (í˜„ì¬ {success_rate:.1f}%)"
        )

    # ëª©í‘œ ëŒ€ë¹„ 50% ë¯¸ë§Œì´ë©´ ê²½ê³  (ì „ëµì€ ì‹¤í–‰)
    if final_len < max_length * WARNING_RATIO:
        _log("WARN",
            f"âš ï¸ ëª©í‘œ ëŒ€ë¹„ {success_rate:.1f}% ë‹¬ì„± ({final_len}/{max_length}) - "
            f"Upbit API ì œì•½ìœ¼ë¡œ ì¶”ì •. ì ˆëŒ€ ìµœì†ŒëŸ‰({absolute_min})ì€ ì¶©ì¡±í•˜ì—¬ ì „ëµ ì‹¤í–‰"
        )

    # âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ - ìƒíƒœ ì´ˆê¸°í™”
    if clear_data_collection_status:
        clear_data_collection_status(user_id)
        _log("INFO", f"[ì´ˆê¸°] ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ! ì—”ì§„ ì‹œì‘í•©ë‹ˆë‹¤.")

    yield df

    last_open = df.index[-1]  # ìš°ë¦¬ê°€ ê°€ì§„ ë§ˆì§€ë§‰ bar_open (tz-naive)

    # ---- ì‹¤ì‹œê°„ ë£¨í”„: ê²½ê³„ ë™ê¸°í™” â†’ ë‹«íŒ ë´‰ ì¡°íšŒ â†’ ê°­ ë°±í•„ ----
    # âœ… intervalë³„ JITTER ê°’ ì„ íƒ
    jitter = JITTER_BY_INTERVAL.get(interval, 0.7)
    _log("INFO", f"[ì‹¤ì‹œê°„ ë£¨í”„] interval={interval}, jitter={jitter}ì´ˆ")

    while not (stop_event and stop_event.is_set()):
        # ğŸ”¥ FIX: sleep ê³„ì‚°ì€ ì‹¤ì œ ì‹œê°(ì´ˆ í¬í•¨) ì‚¬ìš©
        now_real = datetime.now(ZoneInfo("Asia/Seoul")).replace(tzinfo=None)
        now = _now_kst_naive()  # ê²½ê³„ ê³„ì‚°ìš© (ì´ˆ ì œê±°)
        next_close = _next_boundary(now, interval)
        sleep_sec = max(0.0, (next_close - now_real).total_seconds() + jitter)

        # ğŸ” DEBUG: ë£¨í”„ ì§„ì… í™•ì¸
        _log("INFO", f"[ì‹¤ì‹œê°„ ë£¨í”„] sleep={sleep_sec:.1f}ì´ˆ | now_real={now_real.strftime('%H:%M:%S')} | now={now} | next_close={next_close} | last_open={last_open}")
        time.sleep(sleep_sec)

        # ğŸ”¥ FIX: sleep í›„ í˜„ì¬ ì‹œê° ì¬ê³„ì‚° (next_close ì¬ì‚¬ìš© ê¸ˆì§€!)
        # - sleep ì¤‘ ì‹œê°„ì´ í˜ë €ìœ¼ë¯€ë¡œ í˜„ì¬ ì‹œê° ê¸°ì¤€ìœ¼ë¡œ boundary ì¬ê³„ì‚° í•„ìš”
        # - íŠ¹íˆ ì—”ì§„ ì¬ì‹œì‘ ì§í›„ ì§§ì€ sleep ì‹œ í•„ìˆ˜!
        now_after_sleep = _now_kst_naive()
        next_close_after = _next_boundary(now_after_sleep, interval)

        # ë§‰ ë‹«íŒ ë´‰ì˜ open
        iv = _iv_min(interval)
        boundary_open = next_close_after - timedelta(minutes=iv)

        # ğŸ” DEBUG: sleep ì „í›„ ì‹œê° ë¹„êµ (ë²„ê·¸ ë””ë²„ê¹…ìš©)
        if next_close != next_close_after:
            _log("INFO",
                f"[ì‹œê° ë™ê¸°í™”] sleep ì „: next_close={next_close} â†’ "
                f"sleep í›„: next_close_after={next_close_after} | "
                f"boundary_open={boundary_open}"
            )

        # ğŸ”¥ FIX: ì¤‘ê°„ ëˆ„ë½ë¶„ ê³„ì‚° (ì˜¬ë¦¼ ì²˜ë¦¬ë¡œ 1ë¶„ ê°­ë„ ê°ì§€)
        # ê¸°ì¡´: int() ì ˆì‚¬ â†’ 1ë¶„ ê°­ì´ 0ìœ¼ë¡œ ê³„ì‚°ë˜ì–´ ëˆ„ë½!
        # ê°œì„ : math.ceil() ì˜¬ë¦¼ â†’ 1ë¶„ ê°­ë„ 1ë¡œ ê³„ì‚°
        gap_seconds = (boundary_open - last_open).total_seconds()
        gap = math.ceil(gap_seconds / (iv * 60))  # ì˜¬ë¦¼ ì²˜ë¦¬

        # ğŸ›¡ï¸ ì•ˆì „ì¥ì¹˜: gapì´ 1 ì´í•˜ì—¬ë„ ìµœì†Œ 2ê°œ ë´‰ ìš”ì²­ (ì¤‘ë³µ ì œê±°)
        # - ì´ìœ : API ì‘ë‹µ ì§€ì—°ìœ¼ë¡œ ìµœì‹  ë´‰ì´ ëˆ„ë½ë  ìˆ˜ ìˆìŒ
        # - ì¤‘ë³µì€ ë‚˜ì¤‘ì— ìë™ ì œê±°ë˜ë¯€ë¡œ ì•ˆì „
        need = max(2, min(gap + 1, 200))  # ìµœì†Œ 2ê°œ, gap+1ê°œ ìš”ì²­

        # ğŸ” DEBUG: API í˜¸ì¶œ ì „ íŒŒë¼ë¯¸í„°
        _log("INFO", f"[ì‹¤ì‹œê°„ API] boundary_open={boundary_open} | gap={gap} | need={need} | last_open={last_open}")

        # ğŸ”¥ FIX: ì‘ë‹µ ì§€ì—° ì¬ì‹œë„ë¥¼ ë‚´ë¶€ ë£¨í”„ë¡œ êµ¬í˜„ (continue ë²„ê·¸ ìˆ˜ì •)
        # ê¸°ì¡´ ë¬¸ì œ: continue â†’ while ì²˜ìŒ ë³µê·€ â†’ sleep ë‹¤ì‹œ ì‹¤í–‰ â†’ ì¬ì‹œë„ ë¬´íš¨í™”!
        # í•´ê²°: ë‚´ë¶€ for ë£¨í”„ë¡œ ì¬ì‹œë„ â†’ API í˜¸ì¶œë§Œ ë°˜ë³µ â†’ sleep ê±´ë„ˆë›°ì§€ ì•ŠìŒ
        new = None
        max_delay_retry = 5  # ì‘ë‹µ ì§€ì—° ì¬ì‹œë„ ìµœëŒ€ íšŸìˆ˜

        for delay_retry_attempt in range(max_delay_retry):
            if stop_event and stop_event.is_set():
                _log("WARN", "stream_candles ì¤‘ë‹¨ë¨: ì‹¤ì‹œê°„ ë£¨í”„ ì¤‘ stop_event ê°ì§€")
                return

            # API í˜¸ì¶œ (ê¸°ë³¸ ì—°ê²° ì¬ì‹œë„ 5íšŒ)
            new = None
            for attempt in range(1, max_retry + 1):
                if stop_event and stop_event.is_set():
                    return
                try:
                    to_param = _fmt_to_param(boundary_open)
                    _log("INFO",
                        f"[ì‹¤ì‹œê°„ API] í˜¸ì¶œ #{delay_retry_attempt + 1}/{max_delay_retry} | "
                        f"count={need}, to={to_param}"
                    )
                    new = pyupbit.get_ohlcv(ticker, interval=interval, count=need, to=to_param)
                    if new is not None and not new.empty:
                        # ğŸ” PRICE-DEBUG: ì‹¤ì‹œê°„ API ì›ë³¸ ë°ì´í„°
                        try:
                            last_3 = new.tail(min(3, len(new)))
                            for idx, row in last_3.iterrows():
                                _log("INFO", f"[PRICE-REALTIME-RAW] {idx} | O={row['open']:.0f} H={row['high']:.0f} L={row['low']:.0f} C={row['close']:.0f}")
                        except Exception as e_log:
                            _log("WARN", f"[PRICE-REALTIME-RAW] ë¡œê¹… ì‹¤íŒ¨: {e_log}")
                        break
                except Exception as e:
                    _log("ERROR", f"[ì‹¤ì‹œê°„ API] ì˜ˆì™¸: {e} (attempt {attempt}/{max_retry})")

                delay = min(base_delay * (2 ** (attempt - 1)), 30) + random.uniform(0, 2)
                _log("WARN", f"[ì‹¤ì‹œê°„ API] {delay:.1f}ì´ˆ í›„ ì¬ì‹œë„ (ì—°ê²° ì‹¤íŒ¨)")
                time.sleep(delay)

            # API ì—°ê²° ìì²´ ì‹¤íŒ¨ ì‹œ ì™¸ë¶€ while ë£¨í”„ë¡œ (ê²½ê³„ ì¬ë™ê¸°í™”)
            if new is None or new.empty:
                backoff = min(30 + random.uniform(0, 10), 300)
                _log("ERROR", f"[ì‹¤ì‹œê°„ API] ì—°ê²° ì‹¤íŒ¨, {backoff:.1f}ì´ˆ í›„ ê²½ê³„ ì¬ë™ê¸°í™”")
                time.sleep(backoff)
                break  # ë‚´ë¶€ ë£¨í”„ íƒˆì¶œ â†’ while ì²˜ìŒìœ¼ë¡œ (ê²½ê³„ ì¬ê³„ì‚°)

            # ğŸ›¡ï¸ ì‘ë‹µ ê²€ì¦: ê¸°ëŒ€í•œ ë´‰ì„ ë°›ì•˜ëŠ”ê°€?
            _log("INFO", f"[ì‹¤ì‹œê°„ API ì‘ë‹µ] rows={len(new)} | first={new.index[0]} | last={new.index[-1]}")

            expected_last = boundary_open
            actual_last = new.index[-1]
            time_gap = (expected_last - actual_last).total_seconds() / 60
            time_gap_bars = time_gap / iv

            # ğŸ›¡ï¸ ì‘ë‹µ ì§€ì—° ê°ì§€: 0.5ë´‰ ì´ìƒ ì°¨ì´
            if time_gap_bars >= 0.5:
                _log("WARN",
                    f"[ì‹¤ì‹œê°„ API] ì‘ë‹µ ì§€ì—° ê°ì§€! "
                    f"ê¸°ëŒ€: {expected_last} | ì‹¤ì œ: {actual_last} | "
                    f"ê°­: {time_gap:.1f}ë¶„ ({time_gap_bars:.1f}ë´‰)"
                )

                # ìµœëŒ€ ì¬ì‹œë„ ì „ì´ë©´ ëŒ€ê¸° í›„ ì¬ì‹œë„
                if delay_retry_attempt < max_delay_retry - 1:
                    retry_delays = [3, 5, 8, 12, 15]
                    retry_delay = retry_delays[min(delay_retry_attempt, len(retry_delays) - 1)]
                    retry_delay += random.uniform(0, 2)

                    _log("WARN",
                        f"[ì‹¤ì‹œê°„ API] {retry_delay:.1f}ì´ˆ í›„ ì¬ì‹œë„ "
                        f"({delay_retry_attempt + 1}/{max_delay_retry}) - ëˆ„ë½ ë°©ì§€!"
                    )
                    time.sleep(retry_delay)
                    # continueë¡œ ë‚´ë¶€ for ë£¨í”„ ë°˜ë³µ (API ì¬í˜¸ì¶œ)
                    continue
                else:
                    _log("ERROR",
                        f"[ì‹¤ì‹œê°„ API] ìµœëŒ€ ì¬ì‹œë„ ë„ë‹¬ ({max_delay_retry}íšŒ) - "
                        f"ë°±í•„ ë¡œì§ìœ¼ë¡œ ë³µêµ¬ ì‹œë„"
                    )
                    # breakë¡œ ë‚´ë¶€ ë£¨í”„ íƒˆì¶œ â†’ ë°±í•„ ì‹œë„
                    break
            else:
                # ì •ìƒ ì‘ë‹µ: ë‚´ë¶€ ë£¨í”„ íƒˆì¶œ
                _log("INFO", f"[ì‹¤ì‹œê°„ API] ì •ìƒ ì‘ë‹µ í™•ì¸ (ê°­: {time_gap_bars:.2f}ë´‰)")
                break

        # API ì‘ë‹µ ì—†ìŒ ì‹œ ë‹¤ìŒ ë£¨í”„ë¡œ
        if new is None or new.empty:
            _log("WARN", f"[ì‹¤ì‹œê°„ API] ì‘ë‹µ ì—†ìŒ - last_open ìœ ì§€í•˜ì—¬ ë‹¤ìŒ ë£¨í”„ì—ì„œ ì¬ì‹œë„")
            continue

        new = standardize_ohlcv(new).drop_duplicates()

        # ğŸ” DEBUG: standardize í›„ ë°ì´í„°
        _log("INFO", f"[ì‹¤ì‹œê°„ í‘œì¤€í™” í›„] rows={len(new)} | first={new.index[0]} | last={new.index[-1]}")

        # ğŸ›¡ï¸ ë°©ì•ˆ 3: ê°•í™”ëœ ëˆ„ë½ ê°ì§€ ë° ê°•ì œ ë°±í•„
        if not new.empty:
            new_last = new.index[-1]

            # ì˜ˆìƒ ë²”ìœ„ ê³„ì‚°
            expected_last = boundary_open  # ë°©ê¸ˆ ë‹«íŒ ë´‰

            # ğŸ”¥ FIX: ëˆ„ë½ ê°ì§€ ê°•í™” (0.3ë´‰ ì´ìƒë„ ê°ì§€)
            # ê¸°ì¡´: 0.5ë´‰ ì´ìƒë§Œ ê°ì§€ â†’ 1ë¶„ ê°­ì˜ 33% ëˆ„ë½!
            # ê°œì„ : 0.3ë´‰ ì´ìƒ ê°ì§€ + math.ceilë¡œ ì˜¬ë¦¼
            time_gap_seconds = abs((expected_last - new_last).total_seconds())
            time_gap_bars = time_gap_seconds / (iv * 60)  # ë´‰ ë‹¨ìœ„

            # ğŸ›¡ï¸ ë” ì—„ê²©í•œ ëˆ„ë½ ê¸°ì¤€: 0.3ë´‰ ì´ìƒ (ê¸°ì¡´: 0.5ë´‰)
            if time_gap_bars >= 0.3:  # 0.3ë´‰ ì´ìƒ ì°¨ì´ë‚˜ë©´ ëˆ„ë½ ì˜ì‹¬
                missing_minutes = time_gap_seconds / 60
                # ğŸ”¥ FIX: ì˜¬ë¦¼ ì²˜ë¦¬ë¡œ 1ë¶„ ê°­ë„ 1ë´‰ìœ¼ë¡œ ê³„ì‚°
                missing_bars = math.ceil(missing_minutes / iv)  # ê¸°ì¡´: int(...)

                if missing_bars > 0:
                    _log("WARN",
                        f"âš ï¸ [ëˆ„ë½ ê°ì§€] ê¸°ëŒ€ ë§ˆì§€ë§‰ ë´‰: {expected_last} | "
                        f"ì‹¤ì œ ë§ˆì§€ë§‰ ë´‰: {new_last} | "
                        f"ëˆ„ë½: {missing_bars}ê°œ ë´‰ ({missing_minutes}ë¶„)"
                    )

                    # ğŸ›¡ï¸ ë°±í•„ ì‹œë„ ê°•í™”: ìµœëŒ€ 8íšŒ (ê¸°ì¡´: 5íšŒ)
                    # - 1ë¶„ ê°­ì€ ì¹˜ëª…ì ì´ë¯€ë¡œ ë” ê³µê²©ì ìœ¼ë¡œ ì¬ì‹œë„
                    # - ì¬ì‹œë„ ê°„ê²©: 2ì´ˆ â†’ 4ì´ˆ â†’ 6ì´ˆ â†’ 8ì´ˆ â†’ 10ì´ˆ â†’ 12ì´ˆ â†’ 15ì´ˆ â†’ 20ì´ˆ
                    backfill_success = False
                    max_backfill_retry = 8  # ê¸°ì¡´: 5
                    for backfill_attempt in range(1, max_backfill_retry + 1):
                        try:
                            _log("INFO",
                                f"[ë°±í•„ ì‹œë„ {backfill_attempt}/{max_backfill_retry}] "
                                f"{new_last} ~ {expected_last} êµ¬ê°„ | "
                                f"ëˆ„ë½: {missing_bars}ê°œ ë´‰"
                            )

                            # ğŸ›¡ï¸ ëˆ„ë½ëœ êµ¬ê°„ + ì—¬ìœ ë¶„(3ê°œ) ì¶”ê°€ ìš”ì²­ (ê¸°ì¡´: +2)
                            # - ì—¬ìœ ë¶„ì„ ë” ëŠ˜ë ¤ì„œ API ì‘ë‹µ ë¶ˆì•ˆì • ëŒ€ì‘
                            backfill_count = missing_bars + 3
                            backfill = pyupbit.get_ohlcv(
                                ticker,
                                interval=interval,
                                count=backfill_count,
                                to=_fmt_to_param(expected_last)
                            )

                            if backfill is not None and not backfill.empty:
                                backfill = standardize_ohlcv(backfill).drop_duplicates()

                                # ğŸ”¥ FIX: ì‹¤ì œë¡œ ëˆ„ë½ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                                # - newì— ì´ë¯¸ ìˆëŠ” ë´‰ì€ ì œì™¸
                                # - last_openê³¼ expected_last ì‚¬ì´ë§Œ ì¶”ì¶œ (ë¯¸ë˜ ë´‰ ì°¨ë‹¨)
                                existing_indices = set(new.index)
                                backfill_new = backfill[~backfill.index.isin(existing_indices)]
                                backfill_new = backfill_new[
                                    (backfill_new.index > last_open) &
                                    (backfill_new.index <= expected_last)
                                ]

                                if not backfill_new.empty:
                                    # newì— ë³‘í•©
                                    new = pd.concat([new, backfill_new]).drop_duplicates().sort_index()
                                    _log("INFO",
                                        f"âœ… [ë°±í•„ ì„±ê³µ] {len(backfill_new)}ê°œ ë´‰ ë³µêµ¬ ì™„ë£Œ | "
                                        f"ë³µêµ¬ ë²”ìœ„: {backfill_new.index[0]} ~ {backfill_new.index[-1]}"
                                    )
                                    backfill_success = True
                                    break
                                else:
                                    _log("WARN", f"[ë°±í•„] ì‘ë‹µ ë°ì´í„°ê°€ ì´ë¯¸ ë³´ìœ  ì¤‘ì¸ ë´‰ë§Œ í¬í•¨")
                            else:
                                _log("WARN", f"[ë°±í•„] API ì‘ë‹µ ì—†ìŒ (attempt {backfill_attempt}/{max_backfill_retry})")

                        except Exception as e:
                            _log("ERROR", f"[ë°±í•„ ì‹¤íŒ¨] {e} (attempt {backfill_attempt}/{max_backfill_retry})")

                        # ğŸ›¡ï¸ ì¬ì‹œë„ ì „ ëŒ€ê¸° ê°•í™”: ì ì§„ì  ì¦ê°€ (ê¸°ì¡´: 2ì´ˆ ê°„ê²©)
                        # - ê°„ê²©: 2ì´ˆ â†’ 4ì´ˆ â†’ 6ì´ˆ â†’ 8ì´ˆ â†’ 10ì´ˆ â†’ 12ì´ˆ â†’ 15ì´ˆ â†’ 20ì´ˆ
                        if backfill_attempt < max_backfill_retry:
                            wait_times = [2, 4, 6, 8, 10, 12, 15, 20]
                            wait_time = wait_times[min(backfill_attempt - 1, len(wait_times) - 1)]
                            _log("INFO", f"[ë°±í•„] {wait_time}ì´ˆ í›„ ì¬ì‹œë„... (ëˆ„ë½ ë°©ì§€ ìµœìš°ì„ )")
                            time.sleep(wait_time)

                    if not backfill_success:
                        _log("ERROR",
                            f"âŒ [ë°±í•„ í¬ê¸°] {missing_bars}ê°œ ë´‰ ì˜êµ¬ ëˆ„ë½ ê°€ëŠ¥! | "
                            f"ëˆ„ë½ êµ¬ê°„: {new_last} ~ {expected_last} | "
                            f"ìµœëŒ€ {max_backfill_retry}íšŒ ì¬ì‹œë„ ì‹¤íŒ¨ - ì¹˜ëª…ì  ë°ì´í„° ì†ì‹¤!"
                        )
                        # ğŸ›¡ï¸ ìµœí›„ì˜ ì•ˆì „ì¥ì¹˜: ë°±í•„ í¬ê¸° í›„ì—ë„ ë‹¤ìŒ ë£¨í”„ì—ì„œ gap ê³„ì‚°ìœ¼ë¡œ ìë™ ë³µêµ¬ ì‹œë„
                        # - last_openì„ ì—…ë°ì´íŠ¸í•˜ì§€ ì•Šìœ¼ë©´ ë‹¤ìŒ ë£¨í”„ì—ì„œ gapì´ ì»¤ì ¸ì„œ ë‹¤ì‹œ ì‹œë„ë¨
                        _log("WARN", f"[ë°±í•„ í¬ê¸°] ë‹¤ìŒ ë£¨í”„ì—ì„œ gap ê³„ì‚°ìœ¼ë¡œ ì¬ì‹œë„ ì˜ˆì • (last_open ìœ ì§€)")

        # ğŸ” PRICE-DEBUG: ì‹¤ì‹œê°„ standardize í›„ ë°ì´í„°
        try:
            last_3 = new.tail(min(3, len(new)))
            for idx, row in last_3.iterrows():
                _log("INFO", f"[PRICE-REALTIME-STD] {idx} | O={row['Open']:.0f} H={row['High']:.0f} L={row['Low']:.0f} C={row['Close']:.0f}")
        except Exception as e_log:
            _log("WARN", f"[PRICE-REALTIME-STD] ë¡œê¹… ì‹¤íŒ¨: {e_log}")

        # ğŸ”¥ FIX: ì˜ˆìƒ ë²”ìœ„ ë‚´ì˜ ë´‰ë§Œ í—ˆìš© (ë¯¸ë˜ ë´‰ ì°¨ë‹¨)
        # - last_open < index <= boundary_open
        # - boundary_open: ë°©ê¸ˆ ë‹«íŒ ë´‰ (ì´ë²ˆ ë£¨í”„ì—ì„œ ì²˜ë¦¬í•´ì•¼ í•  ìµœì‹  ë´‰)
        # - ì˜ˆ: last_open=21:24, boundary_open=21:25 â†’ 21:25ë§Œ í—ˆìš©, 21:26ì€ ì°¨ë‹¨
        before_filter_count = len(new)
        new = new[(new.index > last_open) & (new.index <= boundary_open)]

        # âœ… ì¤‘ë³µ ì œê±° (ê°™ì€ ì¸ë±ìŠ¤ëŠ” ìµœì‹  ê°’ ìœ ì§€)
        new = new.loc[~new.index.duplicated(keep='last')]

        # ğŸ” DEBUG: í•„í„°ë§ ê²°ê³¼
        _log("INFO", f"[ì‹¤ì‹œê°„ í•„í„°ë§] before={before_filter_count} | after={len(new)} | filter_condition: {last_open} < index <= {boundary_open}")

        # ğŸ›¡ï¸ ë°©ì•ˆ 3-2: í•„í„°ë§ í›„ empty ì‹œ ë³´í˜¸
        if new.empty:
            # APIëŠ” ì‘ë‹µí–ˆì§€ë§Œ í•„í„°ë§ í›„ ë¹„ì–´ìˆìŒ
            # â†’ ì´ë¯¸ ê°€ì§„ ë°ì´í„°ì™€ ì¤‘ë³µì´ê±°ë‚˜, API ì‘ë‹µì´ ê³¼ê±° ë°ì´í„°ë§Œ í¬í•¨

            # ì‹œê°„ì´ ì¶©ë¶„íˆ í˜ë €ìœ¼ë©´ last_open ê°•ì œ ì—…ë°ì´íŠ¸ (ëˆ„ë½ ë°©ì§€)
            elapsed_minutes = (boundary_open - last_open).total_seconds() / 60
            if elapsed_minutes >= iv:
                _log("WARN",
                    f"[ì‹¤ì‹œê°„ í•„í„°ë§] ìƒˆ ë°ì´í„° ì—†ì§€ë§Œ ì‹œê°„ ê²½ê³¼ ({elapsed_minutes:.0f}ë¶„ â‰¥ {iv}ë¶„) â†’ "
                    f"last_open ê°•ì œ ì—…ë°ì´íŠ¸: {last_open} â†’ {boundary_open}"
                )
                last_open = boundary_open
                # yield í•˜ì§€ ì•Šê³  ë‹¤ìŒ ë£¨í”„ ëŒ€ê¸° (ì‹¤ì œ ìƒˆ ë°ì´í„° ì—†ìœ¼ë¯€ë¡œ)
            else:
                _log("INFO",
                    f"[ì‹¤ì‹œê°„ í•„í„°ë§] ì‹œê°„ ê²½ê³¼ ë¶€ì¡± ({elapsed_minutes:.1f}ë¶„ < {iv}ë¶„), "
                    f"last_open ìœ ì§€: {last_open}"
                )

            continue

        # ì¤‘ë³µ/ì •ë ¬ì€ _optimize_dataframe_memory ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë˜ì§€ë§Œ
        # í˜¹ì‹œ ë‚¨ì€ ì¤‘ë³µì— ëŒ€í•´ ìµœì‹  ê°’ ìš°ì„ ìœ¼ë¡œ í•œ ë²ˆ ë” ë³´ì •
        # df = _optimize_dataframe_memory(df, new, max_length).loc[~_optimize_dataframe_memory(df, new, max_length).index.duplicated(keep="last")].sort_index()
        # âœ… í•œ ë²ˆë§Œ ê³„ì‚°í•œ ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ í˜¸ì¶œ/ë ˆì´ìŠ¤ ìœ„í—˜ ì œê±°
        tmp = _optimize_dataframe_memory(df, new, max_length)
        df = tmp.loc[~tmp.index.duplicated(keep="last")].sort_index()
        del tmp

        # ì‹¤ì‹œê°„ ë³‘í•© í›„ DET ë¡œê¹… (ë¡œì»¬/ì„œë²„ ë¹„êµ í•µì‹¬ ì§€ì )
        log_det(df, "LOOP_MERGED")

        # ğŸ” PRICE-DEBUG: ì‹¤ì‹œê°„ ë³‘í•© í›„ ìµœì¢… ë°ì´í„°
        try:
            last_3 = df.tail(3)
            for idx, row in last_3.iterrows():
                _log("INFO", f"[PRICE-REALTIME-MERGED] {idx} | O={row['Open']:.0f} H={row['High']:.0f} L={row['Low']:.0f} C={row['Close']:.0f}")
        except Exception as e_log:
            _log("WARN", f"[PRICE-REALTIME-MERGED] ë¡œê¹… ì‹¤íŒ¨: {e_log}")

        # â˜… Phase 2: ì‹¤ì‹œê°„ ë°ì´í„°ë„ DBì— ì €ì¥ (ì ì§„ì  íˆìŠ¤í† ë¦¬ ëˆ„ì )
        if user_id and not new.empty:
            try:
                from services.db import save_candle_cache
                save_candle_cache(user_id, ticker, interval, new)
            except Exception as e:
                # ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ë©”ì¸ ë£¨í”„ëŠ” ê³„ì† ì§„í–‰
                pass

        # ğŸ›¡ï¸ ë°©ì•ˆ 4: Yield ì§ì „ ìµœì¢… ì—°ì†ì„± ê²€ì¦
        if len(df) > 1:
            # 1) ì¸ë±ìŠ¤ ì—°ì†ì„± ì²´í¬ (interval ê°„ê²©ì´ì–´ì•¼ í•¨)
            time_diffs = df.index.to_series().diff().dt.total_seconds() / 60
            gaps_in_df = time_diffs[time_diffs > iv * 1.5]  # 1.5ë°° ì´ìƒ ì°¨ì´ë‚˜ë©´ ê°­

            if not gaps_in_df.empty:
                gap_details = []
                for gap_idx, gap_minutes in gaps_in_df.items():
                    prev_idx = df.index[df.index.get_loc(gap_idx) - 1]
                    gap_details.append(f"  - {prev_idx} â†’ {gap_idx} (ê°­: {gap_minutes:.0f}ë¶„, {gap_minutes/iv:.1f}ë´‰)")

                _log("ERROR",
                    f"âŒ [ì—°ì†ì„± ì˜¤ë¥˜] DataFrameì— {len(gaps_in_df)}ê°œ ê°­ ë°œê²¬!\n" +
                    "\n".join(gap_details)
                )

                # ğŸ”¥ ì„ íƒ 1) ì—ëŸ¬ ë°œìƒ (ì—„ê²© ëª¨ë“œ) - ìš´ì˜ í™˜ê²½ì—ì„œëŠ” ì£¼ì„ ì²˜ë¦¬
                # raise ValueError("DataFrame ì—°ì†ì„± ê²€ì¦ ì‹¤íŒ¨ - ë°ì´í„° ëˆ„ë½ ê°ì§€")

                # ğŸ”¥ ì„ íƒ 2) ê²½ê³ ë§Œ ë‚¨ê¸°ê³  ì§„í–‰ (ê´€ëŒ€ ëª¨ë“œ)
                _log("WARN", "âš ï¸ ì—°ì†ì„± ì˜¤ë¥˜ ê°ì§€ë˜ì—ˆìœ¼ë‚˜ ì§„í–‰ (ê´€ëŒ€ ëª¨ë“œ)")

        # 2) ì˜ˆìƒ ì‹œê°ê³¼ ì‹¤ì œ last_open ë¹„êµ
        expected_last = boundary_open
        actual_last = df.index[-1]
        time_diff_seconds = abs((actual_last - expected_last).total_seconds())

        if time_diff_seconds > iv * 60 * 0.5:  # 0.5ë´‰ ì´ìƒ ì°¨ì´
            time_diff_minutes = time_diff_seconds / 60
            _log("WARN",
                f"âš ï¸ [ì‹œê°„ ë¶ˆì¼ì¹˜] ê¸°ëŒ€ ë§ˆì§€ë§‰ ë´‰: {expected_last} | "
                f"ì‹¤ì œ ë§ˆì§€ë§‰ ë´‰: {actual_last} | "
                f"ì°¨ì´: {time_diff_minutes:.1f}ë¶„ ({time_diff_minutes/iv:.2f}ë´‰)"
            )

        last_open = df.index[-1]
        # ì‚¬ìš©ì í˜¼ë€ ë°©ì§€ìš© ë™ê¸°í™” ë¡œê·¸ (bar_open / bar_close ëª…ì‹œ)
        if q:
            last_close = last_open + timedelta(minutes=iv)
            # run_at = datetime.now()
            run_at = _now_kst_naive()  # âœ… KST-naiveë¡œ ê¸°ë¡ í†µì¼
            q.put((
                time.time(),
                "LOG",
                f"â± run_at={run_at:%Y-%m-%d %H:%M:%S} | bar_open={last_open} | bar_close={last_close} "
            ))

        # ì£¼ê¸°ì  GC
        if hasattr(_optimize_dataframe_memory, "last_gc_time"):
            if time.time() - _optimize_dataframe_memory.last_gc_time > 300:
                _force_memory_cleanup()
                _optimize_dataframe_memory.last_gc_time = time.time()
        else:
            _optimize_dataframe_memory.last_gc_time = time.time()

        yield df


_INTERVAL_MAP = {
    "minute1": "minute1",
    "minute3": "minute3",
    "minute5": "minute5",
    "minute10": "minute10",
    "minute15": "minute15",
    "minute30": "minute30",
    "minute60": "minute60",
    "minute240": "minute240",
    "day": "day",
    "week": "week",
}

# get_ohlcv_once() ì£¼ì„ ë° ì¸ë±ìŠ¤ ì •ê·œí™” ìˆ˜ì •
def get_ohlcv_once(ticker: str, interval_code: str, count: int = 500) -> pd.DataFrame:
    """
    ëŒ€ì‹œë³´ë“œìš© ì›ìƒ· OHLCV.
    âœ… ë°˜í™˜: columns = [Open, High, Low, Close, Volume], DatetimeIndex = 'KST-naive' (streamê³¼ ë™ì¼ ê¸°ì¤€)
    """
    interval = _INTERVAL_MAP.get(interval_code, "minute1")
    df = pyupbit.get_ohlcv(ticker=ticker, interval=interval, count=count)
    if df is None or df.empty:
        return pd.DataFrame(columns=["Open","High","Low","Close","Volume"])

    # âš ï¸ ì¤‘ìš”: pyupbit ì¸ë±ìŠ¤ëŠ” ì´ë¯¸ KST tz-naiveë¡œ ë°˜í™˜ë¨
    if isinstance(df.index, pd.DatetimeIndex):
        idx = pd.to_datetime(df.index)
        if getattr(idx, "tz", None) is None:
            # âœ… pyupbitì€ ì´ë¯¸ KST naiveë¡œ ë°˜í™˜í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            pass
        else:
            # tz-awareì¸ ê²½ìš°ì—ë§Œ KSTë¡œ ë³€í™˜ í›„ tz ì œê±°
            idx = idx.tz_convert("Asia/Seoul").tz_localize(None)
            df.index = idx

    out = df[["open","high","low","close","volume"]].rename(
        columns={"open":"Open","high":"High","low":"Low","close":"Close","volume":"Volume"}
    )

    try:
        log_det(out, "ONCE_BEFORE_RETURN")
    except Exception:
        pass

    return out
