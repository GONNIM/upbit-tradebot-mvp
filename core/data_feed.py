from __future__ import annotations
import pyupbit
import pandas as pd
import time
import logging
import random
import gc
import psutil
import os
from datetime import datetime, timedelta

from zoneinfo import ZoneInfo


logger = logging.getLogger(__name__)


# --------- ì‹œê°„/ê²½ê³„ ìœ í‹¸ (KST naiveë¡œ ì¼ê´€) ---------
_IV_MIN = {
    "minute1": 1,
    "minute3": 3,
    "minute5": 5,
    "minute10": 10,
    "minute15": 15,
    "minute30": 30,
    "minute60": 60,
    "day": 1440,
}


# ë””í„°ë¯¸ë‹ˆì¦˜ ì²´í¬ ë¡œê·¸ í—¬í¼
def log_det(df: pd.DataFrame, tag: str):
    """
    dfê°€ í˜„ì¬ ë™ì¼í•œ ë´‰ ì§‘í•©ì¸ì§€ ë¹ ë¥´ê²Œ ê²€ì¦í•˜ê¸° ìœ„í•œ ë¡œê·¸.
    - rows/first/last + OHLCV ì²´í¬ì„¬ì„ ë‚¨ê¸´ë‹¤.
    - tag: í˜¸ì¶œ ì§€ì  êµ¬ë¶„ìš©(ex: PRE_INIT, LOOP_MERGED, ONCE_BEFORE_RETURN)
    """
    if df is None or df.empty:
        logger.info(f"[DET] {tag} | rows=0 (empty)")
        return
    try:
        rows = len(df)
        first_i, last_i = df.index[0], df.index[-1]
        # OHLCVë§Œ ì‚¬ìš©, ì†Œìˆ˜ 8ìë¦¬ ë°˜ì˜¬ë¦¼ í›„ ë¬¸ìì—´ â†’ í•´ì‹œ
        payload = df[["Open","High","Low","Close","Volume"]].round(8).to_csv(index=True, header=False)
        checksum = hash(payload)  # íŒŒì´ì¬ ë‚´ì¥ í•´ì‹œ(ì„¸ì…˜ë§ˆë‹¤ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ, ê°™ì€ í”„ë¡œì„¸ìŠ¤ ë¹„êµìš©)
        logger.info(f"[DET] {tag} | rows={rows} | first={first_i} | last={last_i} | checksum={checksum}")
    except Exception as e:
        logger.warning(f"[DET] {tag} | logging failed: {e}")


def _iv_min(interval: str) -> int:
    return _IV_MIN.get(interval, 10)

# v1.2025.10.18.2031
def _now_kst_naive() -> datetime:
    """
    âœ… ì‹œìŠ¤í…œ ë¡œì»¬íƒ€ì„(UTC ë“±)ì— ì˜ì¡´í•˜ì§€ ì•Šê³  KST ì‹œê°ì„ tz-awareë¡œ ë§Œë“  ë’¤ tz ì œê±°.
    - ëª¨ë“  ë°” ê²½ê³„ ê³„ì‚°ì„ 'KST-naive'ë¡œ í†µì¼í•˜ê¸° ìœ„í•¨.
    """
    kst_now = datetime.now(tz=ZoneInfo("Asia/Seoul"))
    return kst_now.replace(second=0, microsecond=0).replace(tzinfo=None)

def _floor_boundary(dt: datetime, interval: str) -> datetime:
    if interval == "day":
        return dt.replace(hour=0, minute=0, second=0, microsecond=0)
    iv = _iv_min(interval)
    m = (dt.minute // iv) * iv
    return dt.replace(minute=m, second=0, microsecond=0)

def _next_boundary(dt: datetime, interval: str) -> datetime:
    if interval == "day":
        nxt = dt.replace(hour=0, minute=0, second=0, microsecond=0)
        if dt >= nxt:
            nxt += timedelta(days=1)
        return nxt
    iv = _iv_min(interval)
    m = (dt.minute // iv + 1) * iv
    add_h = m // 60
    m = m % 60
    h = (dt.hour + add_h) % 24
    nxt = dt.replace(hour=h, minute=m, second=0, microsecond=0)
    if dt.hour + add_h >= 24:
        nxt += timedelta(days=1)
    return nxt

def _fmt_to_param(dt: datetime) -> str:
    return dt.strftime("%Y-%m-%d %H:%M:%S")


# --------- ë©”ëª¨ë¦¬ ìœ í‹¸ ---------
def _optimize_dataframe_memory(old_df, new_data, max_length):
    try:
        if len(old_df) >= max_length:
            old_df = old_df.iloc[-(max_length - 10):].copy()
        combined = pd.concat([old_df, new_data], ignore_index=False)
        result = combined.drop_duplicates().sort_index().iloc[-max_length:]
        memory_usage_mb = result.memory_usage(deep=True).sum() / 1024 / 1024
        if memory_usage_mb > 10:
            logger.warning(f"âš ï¸ DataFrame ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³¼ë‹¤: {memory_usage_mb:.2f}MB")
        return result
    except Exception as e:
        logger.error(f"âŒ DataFrame ìµœì í™” ì‹¤íŒ¨: {e}")
        return pd.concat([old_df, new_data]).drop_duplicates().sort_index().iloc[-max_length:]

def _force_memory_cleanup():
    try:
        collected = gc.collect()
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024
        logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: ê°ì²´ {collected}ê°œ ìˆ˜ì§‘, í˜„ì¬ ë©”ëª¨ë¦¬: {memory_mb:.1f}MB")
        if memory_mb > 500:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_mb:.1f}MB - ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ í•„ìš”")
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")


# --------- ë©”ì¸ ìŠ¤íŠ¸ë¦¼ ---------
def stream_candles(
    ticker: str,
    interval: str,
    q=None,
    max_retry: int = 5,
    retry_wait: int = 3,
    stop_event=None,
    max_length: int = 500,
    user_id: str = None,  # Phase 2: ìºì‹œ ì‚¬ìš©ì„ ìœ„í•œ user_id
):
    def _log(level: str, msg: str):
        (logger.warning if level == "WARN" else logger.error if level == "ERROR" else logger.info)(msg)
        if q:
            # í•­ìƒ 3-íŠœí”Œ ìœ ì§€
            prefix = "âš ï¸" if level == "WARN" else "âŒ" if level == "ERROR" else "â„¹ï¸"
            q.put((time.time(), "LOG", f"{prefix} {msg}"))

    def standardize_ohlcv(df):
        if df is None or df.empty:
            raise ValueError(f"OHLCV ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {ticker}, {interval}")

        before_count = len(df)
        _log("INFO", f"[standardize] ì…ë ¥ ë°ì´í„°: {before_count}ê°œ, index type={type(df.index)}, tz={getattr(df.index, 'tz', 'N/A')}")

        df = df.rename(columns={"open":"Open","high":"High","low":"Low","close":"Close","volume":"Volume"})
        if "value" in df.columns:
            df = df.drop(columns=["value"])

        # ì¸ë±ìŠ¤ tz ì •ê·œí™”: UTC â†’ KST naiveë¡œ í†µì¼
        idx = pd.to_datetime(df.index)
        try:
            # âœ… ìˆ˜ì •: ì¡°ê±´ì„ ë°˜ëŒ€ë¡œ (tzê°€ Noneì´ë©´ = naiveì´ë©´)
            if getattr(idx, "tz", None) is None:
                # tz-naiveë¼ë©´ UTCë¡œ ê°„ì£¼í•˜ê³  localize
                idx = idx.tz_localize("UTC")
                _log("INFO", f"[standardize] tz-naive ê°ì§€ â†’ UTCë¡œ localize")
            else:
                _log("INFO", f"[standardize] ì´ë¯¸ tz-aware (tz={idx.tz})")

            # KSTë¡œ ë³€í™˜ í›„ tz ì œê±°í•˜ì—¬ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ 'KST-naive'ë¡œ í†µì¼
            idx = idx.tz_convert("Asia/Seoul").tz_localize(None)
            _log("INFO", f"[standardize] KST naiveë¡œ ë³€í™˜ ì™„ë£Œ")
        except Exception as e:
            # ì˜ˆì™¸ ë°œìƒ ì‹œ ìƒì„¸ ë¡œê·¸
            _log("ERROR", f"[standardize] íƒ€ì„ì¡´ ë³€í™˜ ì‹¤íŒ¨: {e}")
            # ë³€í™˜ ì‹¤íŒ¨ ì‹œì—ë„ ìµœì†Œí•œ ì •ë ¬ì€ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ idx ê·¸ëŒ€ë¡œ ì‚¬ìš©

        df.index = idx

        # dropna ì „ NaN ê°œìˆ˜ í™•ì¸
        na_counts = df.isna().sum()
        if na_counts.any():
            _log("WARN", f"[standardize] NaN ë°œê²¬: {na_counts[na_counts > 0].to_dict()}")

        # ì •ë ¬ í›„ ì¤‘ë³µ ì œê±° (dropnaëŠ” ë‚˜ì¤‘ì—)
        df = df.sort_index()
        before_dedup = len(df)
        df = df.loc[~df.index.duplicated(keep="last")]
        after_dedup = len(df)

        if before_dedup > after_dedup:
            _log("WARN", f"[standardize] ì¤‘ë³µ ì œê±°: {before_dedup - after_dedup}ê°œ ì‚­ì œ ({before_dedup} â†’ {after_dedup})")

        # NaN ì œê±°
        df = df.dropna()
        after_dropna = len(df)

        if after_dedup > after_dropna:
            _log("WARN", f"[standardize] NaN ì œê±°: {after_dedup - after_dropna}ê°œ ì‚­ì œ ({after_dedup} â†’ {after_dropna})")

        _log("INFO", f"[standardize] ìµœì¢… ì¶œë ¥: {after_dropna}ê°œ (ì†ì‹¤: {before_count - after_dropna}ê°œ, {100*(before_count-after_dropna)/before_count:.1f}%)")

        return df

    # â˜… ì´ˆê¸° íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ìš© í—¬í¼
    def _fetch_initial_history(to_param: str) -> pd.DataFrame:
        """
        UpbitëŠ” ë¶„ë´‰ ê¸°ì¤€ í•œ ë²ˆì— ìµœëŒ€ 200ê°œë§Œ ë°˜í™˜í•˜ë¯€ë¡œ,
        max_lengthê°€ 200ì„ ë„˜ëŠ” ê²½ìš° ì—¬ëŸ¬ ë²ˆ ë‚˜ëˆ ì„œ ê³¼ê±° íˆìŠ¤í† ë¦¬ë¥¼ ëª¨ì€ë‹¤.
        - MACD/EMAë¥¼ HTS ìˆ˜ì¤€ìœ¼ë¡œ ë§ì¶”ê¸° ìœ„í•œ ê¸´ íˆìŠ¤í† ë¦¬(ì˜ˆ: 3ë¶„ë´‰ 1500~2000ê°œ) í™•ë³´ìš©.
        """
        iv_min = _iv_min(interval)
        remaining = max_length
        current_to = to_param
        chunks: list[pd.DataFrame] = []
        base_delay_local = retry_wait
        total_requested = max_length
        api_calls = 0

        _log("INFO", f"[ì´ˆê¸°-multi] íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì‹œì‘: max_length={max_length}, interval={interval}")

        while remaining > 0:
            if stop_event and stop_event.is_set():
                collected = sum(len(c) for c in chunks)
                _log("WARN", f"[ì´ˆê¸°-multi] stop_event ê°ì§€ â†’ ìˆ˜ì§‘ ì¤‘ë‹¨ (collected={collected}/{total_requested})")
                break

            per_call = min(200, remaining)  # Upbit ë¶„ë´‰ ìµœëŒ€ 200ê°œ
            df_part = None
            api_calls += 1

            for attempt in range(1, max_retry + 1):
                try:
                    _log("INFO", f"[ì´ˆê¸°-multi] API í˜¸ì¶œ #{api_calls}: count={per_call}, to={current_to}")
                    df_part = pyupbit.get_ohlcv(
                        ticker,
                        interval=interval,
                        count=per_call,
                        to=current_to,
                    )
                    if df_part is not None and not df_part.empty:
                        _log("INFO", f"[ì´ˆê¸°-multi] API ì‘ë‹µ ì„±ê³µ: {len(df_part)}ê°œ ìˆ˜ì‹ ")
                        break
                    else:
                        _log("WARN", f"[ì´ˆê¸°-multi] API ì‘ë‹µì´ ë¹„ì–´ìˆìŒ (attempt {attempt}/{max_retry})")
                except Exception as e:
                    _log("ERROR", f"[ì´ˆê¸°-multi] API ì˜ˆì™¸ ë°œìƒ: {e} (attempt {attempt}/{max_retry})")

                # Upbit API rate limit ëŒ€ì‘: í˜¸ì¶œ ê°„ ìµœì†Œ 0.1ì´ˆ ë”œë ˆì´
                delay = min(base_delay_local * (2 ** (attempt - 1)), 60) + random.uniform(0.1, 1.0)
                _log("WARN", f"[ì´ˆê¸°-multi] API ì¬ì‹œë„ ëŒ€ê¸°: {delay:.1f}ì´ˆ")
                time.sleep(delay)
            else:
                # max_retry ì‹¤íŒ¨ ì‹œ - ë¶€ë¶„ ìˆ˜ì§‘ ë°ì´í„°ë¼ë„ ë°˜í™˜í•˜ë„ë¡ ê°œì„ 
                collected = sum(len(c) for c in chunks)
                _log("ERROR", f"[ì´ˆê¸°-multi] API ì—°ì† ì‹¤íŒ¨ (collected={collected}/{total_requested})")
                # break ëŒ€ì‹  ê²½ê³ ë§Œ ë‚¨ê¸°ê³  ìˆ˜ì§‘ëœ ë°ì´í„° ë°˜í™˜
                break

            if df_part is None or df_part.empty:
                collected = sum(len(c) for c in chunks)
                _log("WARN", f"[ì´ˆê¸°-multi] ë¹ˆ ì‘ë‹µìœ¼ë¡œ ìˆ˜ì§‘ ì¢…ë£Œ (collected={collected}/{total_requested})")
                break

            chunks.append(df_part)
            got = len(df_part)
            remaining -= got

            collected_so_far = sum(len(c) for c in chunks)
            _log("INFO", f"[ì´ˆê¸°-multi] ì§„í–‰: {collected_so_far}/{total_requested} ({100*collected_so_far/total_requested:.1f}%)")

            if got < per_call:
                # Upbit APIê°€ ìš”ì²­ëŸ‰ë³´ë‹¤ ì ê²Œ ë°˜í™˜ = ë” ì´ìƒ ê³¼ê±° ë°ì´í„° ì—†ìŒ
                _log("WARN", f"[ì´ˆê¸°-multi] APIê°€ ìš”ì²­ëŸ‰ë³´ë‹¤ ì ê²Œ ë°˜í™˜ (got={got}, requested={per_call}) â†’ ê³¼ê±° ë°ì´í„° ì†Œì§„")
                break

            # ë‹¤ìŒ ìš”ì²­ìš© 'to'ëŠ” ì´ë²ˆ ê¸°ì¤€ì‹œê°„ì—ì„œ got*interval ë§Œí¼ ê³¼ê±°ë¡œ ì´ë™
            try:
                dt_to = datetime.strptime(current_to, "%Y-%m-%d %H:%M:%S")
                dt_to -= timedelta(minutes=iv_min * got)
                current_to = _fmt_to_param(dt_to)
            except Exception as e:
                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¶”ê°€ í˜ì´ì§•ì€ í•˜ì§€ ì•Šê³  ì¢…ë£Œ
                collected = sum(len(c) for c in chunks)
                _log("ERROR", f"[ì´ˆê¸°-multi] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {e} (collected={collected}/{total_requested})")
                break

            # API rate limit ì¤€ìˆ˜: í˜¸ì¶œ ê°„ 0.1ì´ˆ ë”œë ˆì´
            time.sleep(0.1)

        if not chunks:
            _log("ERROR", f"[ì´ˆê¸°-multi] ìˆ˜ì§‘ ì‹¤íŒ¨: ë°ì´í„° ì—†ìŒ")
            return pd.DataFrame(columns=["Open","High","Low","Close","Volume"])

        raw = pd.concat(chunks)
        final_count = len(raw)
        success_rate = 100 * final_count / total_requested if total_requested > 0 else 0
        _log("INFO", f"[ì´ˆê¸°-multi] ìˆ˜ì§‘ ì™„ë£Œ: {final_count}/{total_requested} ({success_rate:.1f}%), API í˜¸ì¶œ {api_calls}íšŒ")

        return raw
    
    # ---- ì´ˆê¸° ë¡œë“œ: ë§‰ ë‹«íŒ ê²½ê³„ê¹Œì§€ ----
    base_delay = retry_wait
    df = None
    now = _now_kst_naive()
    bar_close = _floor_boundary(now, interval)
    to_param = _fmt_to_param(bar_close)

    # â˜… Phase 2: DB ìºì‹œ ìš°ì„  í™•ì¸
    cache_used = False
    if user_id:
        try:
            from services.db import load_candle_cache
            cached_df = load_candle_cache(user_id, ticker, interval, max_length)

            if cached_df is not None and len(cached_df) >= max_length * 0.9:  # 90% ì´ìƒì´ë©´ ì‚¬ìš©
                df = cached_df
                cache_used = True
                _log("INFO", f"[CACHE-HIT] {len(df)} candles loaded from DB cache (skip API)")
            elif cached_df is not None:
                _log("INFO", f"[CACHE-PARTIAL] {len(cached_df)} candles in cache (insufficient, will fetch from API)")
        except Exception as e:
            _log("WARN", f"[CACHE] Load failed, will use API: {e}")

    # â˜… ìºì‹œ ë¯¸ìŠ¤ ë˜ëŠ” ë¶€ì¡±: API í˜¸ì¶œ
    if df is None:
        _log("INFO", f"[ì´ˆê¸°] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: ticker={ticker}, interval={interval}, max_length={max_length}")

        if max_length <= 200:
            for attempt in range(1, max_retry + 1):
                if stop_event and stop_event.is_set():
                    _log("WARN", "stream_candles ì¤‘ë‹¨ë¨: ì´ˆê¸° ìˆ˜ì§‘ ì¤‘ stop_event ê°ì§€")
                    return
                try:
                    _log("INFO", f"[ì´ˆê¸°] API ë‹¨ì¼ í˜¸ì¶œ: count={max_length}, to={to_param}")
                    df = pyupbit.get_ohlcv(ticker, interval=interval, count=max_length, to=to_param)
                    if df is not None and not df.empty:
                        _log("INFO", f"[ì´ˆê¸°] API ì‘ë‹µ ì„±ê³µ: {len(df)}ê°œ ìˆ˜ì‹ ")
                        break
                except Exception as e:
                    _log("ERROR", f"[ì´ˆê¸°] API ì˜ˆì™¸ ë°œìƒ: {e}")

                delay = min(base_delay * (2 ** (attempt - 1)), 60) + random.uniform(0, 5)
                _log("WARN", f"[ì´ˆê¸°] API ì‹¤íŒ¨ ({attempt}/{max_retry}), {delay:.1f}ì´ˆ í›„ ì¬ì‹œë„")
                time.sleep(delay)
        else:
            # â˜… MACD/EMA ì•ˆì •í™”ë¥¼ ìœ„í•´ ê¸´ íˆìŠ¤í† ë¦¬(max_length) í™•ë³´
            _log("INFO", f"[ì´ˆê¸°] max_length > 200 â†’ multi-fetch ëª¨ë“œ ì‚¬ìš©")
            df = _fetch_initial_history(to_param)

        # â˜… Phase 2: API í˜¸ì¶œ í›„ DBì— ì €ì¥
        if user_id and df is not None and not df.empty:
            try:
                from services.db import save_candle_cache
                save_candle_cache(user_id, ticker, interval, df)
            except Exception as e:
                _log("WARN", f"[CACHE] Save failed (ignored): {e}")

    if df is None or df.empty:
        _log("ERROR", "[ì´ˆê¸°] ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨, ë¹ˆ DataFrameìœ¼ë¡œ ì‹œì‘")
        df = pd.DataFrame(columns=["Open","High","Low","Close","Volume"])
        df.index = pd.to_datetime([])
    else:
        _log("INFO", f"[ì´ˆê¸°] ìˆ˜ì§‘ëœ ì›ë³¸ ë°ì´í„°: {len(df)}ê°œ")

    df = standardize_ohlcv(df).drop_duplicates()
    final_len = len(df)
    _log("INFO", f"[ì´ˆê¸°] standardize í›„ ìµœì¢… ë°ì´í„°: {final_len}ê°œ (ìš”ì²­: {max_length}ê°œ, ë‹¬ì„±ë¥ : {100*final_len/max_length if max_length > 0 else 0:.1f}%)")

    if final_len < max_length * 0.8:
        _log("WARN", f"âš ï¸ ë°ì´í„° ë¶€ì¡±: {final_len}/{max_length} ({100*final_len/max_length:.1f}%) - Upbit API ì œì•½ ë˜ëŠ” ê³¼ê±° ë°ì´í„° ë¶€ì¡± ê°€ëŠ¥ì„±")

    yield df

    last_open = df.index[-1]  # ìš°ë¦¬ê°€ ê°€ì§„ ë§ˆì§€ë§‰ bar_open (tz-naive)

    # ---- ì‹¤ì‹œê°„ ë£¨í”„: ê²½ê³„ ë™ê¸°í™” â†’ ë‹«íŒ ë´‰ ì¡°íšŒ â†’ ê°­ ë°±í•„ ----
    JITTER = 0.7
    while not (stop_event and stop_event.is_set()):
        now = _now_kst_naive()
        next_close = _next_boundary(now, interval)
        sleep_sec = max(0.0, (next_close - now).total_seconds() + JITTER)
        time.sleep(sleep_sec)

        # ë§‰ ë‹«íŒ ë´‰ì˜ open
        iv = _iv_min(interval)
        boundary_open = next_close - timedelta(minutes=iv)  # ë‘˜ ë‹¤ tz-naive

        # ì¤‘ê°„ ëˆ„ë½ë¶„ ê³„ì‚°(ë¶„ ë‹¨ìœ„)
        gap = int((boundary_open - last_open).total_seconds() // (iv * 60))
        need = max(1, min(gap, 200))

        # ì¬ì‹œë„ ë£¨í”„
        new = None
        for attempt in range(1, max_retry + 1):
            if stop_event and stop_event.is_set():
                _log("WARN", "stream_candles ì¤‘ë‹¨ë¨: ì‹¤ì‹œê°„ ë£¨í”„ ì¤‘ stop_event ê°ì§€")
                return
            try:
                new = pyupbit.get_ohlcv(ticker, interval=interval, count=need, to=_fmt_to_param(next_close))
                if new is not None and not new.empty:
                    break
            except Exception as e:
                _log("ERROR", f"[ì‹¤ì‹œê°„] API ì˜ˆì™¸: {e}")
            delay = min(base_delay * (2 ** (attempt - 1)), 30) + random.uniform(0, 2)
            _log("WARN", f"[ì‹¤ì‹œê°„] API ì‹¤íŒ¨ ({attempt}/{max_retry}), {delay:.1f}ì´ˆ í›„ ì¬ì‹œë„")
            time.sleep(delay)
        else:
            backoff = min(30 + random.uniform(0, 10), 300)
            _log("ERROR", f"[ì‹¤ì‹œê°„] API ì—°ê²° ì‹¤íŒ¨, {backoff:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
            time.sleep(backoff)
            continue

        new = standardize_ohlcv(new).drop_duplicates()
        # ìš°ë¦¬ê°€ ê°€ì§„ ë§ˆì§€ë§‰ ì´í›„ ê²ƒë§Œ
        new = new[new.index > last_open]
        if new.empty:
            continue

        # ì¤‘ë³µ/ì •ë ¬ì€ _optimize_dataframe_memory ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë˜ì§€ë§Œ
        # í˜¹ì‹œ ë‚¨ì€ ì¤‘ë³µì— ëŒ€í•´ ìµœì‹  ê°’ ìš°ì„ ìœ¼ë¡œ í•œ ë²ˆ ë” ë³´ì •
        # df = _optimize_dataframe_memory(df, new, max_length).loc[~_optimize_dataframe_memory(df, new, max_length).index.duplicated(keep="last")].sort_index()
        # âœ… í•œ ë²ˆë§Œ ê³„ì‚°í•œ ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ í˜¸ì¶œ/ë ˆì´ìŠ¤ ìœ„í—˜ ì œê±°
        tmp = _optimize_dataframe_memory(df, new, max_length)
        df = tmp.loc[~tmp.index.duplicated(keep="last")].sort_index()
        del tmp

        # ì‹¤ì‹œê°„ ë³‘í•© í›„ DET ë¡œê¹… (ë¡œì»¬/ì„œë²„ ë¹„êµ í•µì‹¬ ì§€ì )
        log_det(df, "LOOP_MERGED")

        # â˜… Phase 2: ì‹¤ì‹œê°„ ë°ì´í„°ë„ DBì— ì €ì¥ (ì ì§„ì  íˆìŠ¤í† ë¦¬ ëˆ„ì )
        if user_id and not new.empty:
            try:
                from services.db import save_candle_cache
                save_candle_cache(user_id, ticker, interval, new)
            except Exception as e:
                # ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ë©”ì¸ ë£¨í”„ëŠ” ê³„ì† ì§„í–‰
                pass

        last_open = df.index[-1]
        # ì‚¬ìš©ì í˜¼ë€ ë°©ì§€ìš© ë™ê¸°í™” ë¡œê·¸ (bar_open / bar_close ëª…ì‹œ)
        if q:
            last_close = last_open + timedelta(minutes=iv)
            # run_at = datetime.now()
            run_at = _now_kst_naive()  # âœ… KST-naiveë¡œ ê¸°ë¡ í†µì¼
            q.put((
                time.time(),
                "LOG",
                f"â± run_at={run_at:%Y-%m-%d %H:%M:%S} | bar_open={last_open} | bar_close={last_close} "
            ))

        # ì£¼ê¸°ì  GC
        if hasattr(_optimize_dataframe_memory, "last_gc_time"):
            if time.time() - _optimize_dataframe_memory.last_gc_time > 300:
                _force_memory_cleanup()
                _optimize_dataframe_memory.last_gc_time = time.time()
        else:
            _optimize_dataframe_memory.last_gc_time = time.time()

        yield df


_INTERVAL_MAP = {
    "minute1": "minute1",
    "minute3": "minute3",
    "minute5": "minute5",
    "minute10": "minute10",
    "minute15": "minute15",
    "minute30": "minute30",
    "minute60": "minute60",
    "minute240": "minute240",
    "day": "day",
    "week": "week",
}

# get_ohlcv_once() ì£¼ì„ ë° ì¸ë±ìŠ¤ ì •ê·œí™” ìˆ˜ì •
def get_ohlcv_once(ticker: str, interval_code: str, count: int = 500) -> pd.DataFrame:
    """
    ëŒ€ì‹œë³´ë“œìš© ì›ìƒ· OHLCV.
    âœ… ë°˜í™˜: columns = [Open, High, Low, Close, Volume], DatetimeIndex = 'KST-naive' (streamê³¼ ë™ì¼ ê¸°ì¤€)
    """
    interval = _INTERVAL_MAP.get(interval_code, "minute1")
    df = pyupbit.get_ohlcv(ticker=ticker, interval=interval, count=count)
    if df is None or df.empty:
        return pd.DataFrame(columns=["Open","High","Low","Close","Volume"])

    # pyupbit ì¸ë±ìŠ¤ê°€ tz-naive(=UTC)ì¼ ê°€ëŠ¥ì„± ë†’ìŒ â†’ KST-naiveë¡œ í†µì¼
    if isinstance(df.index, pd.DatetimeIndex):
        idx = pd.to_datetime(df.index)
        if getattr(idx, "tz", None) is None:
            idx = idx.tz_localize("UTC")
        idx = idx.tz_convert("Asia/Seoul").tz_localize(None)
        df.index = idx

    out = df[["open","high","low","close","volume"]].rename(
        columns={"open":"Open","high":"High","low":"Low","close":"Close","volume":"Volume"}
    )

    try:
        log_det(out, "ONCE_BEFORE_RETURN")
    except Exception:
        pass

    return out
