import pyupbit
import pandas as pd
import time
import streamlit as st
import logging
import random
import gc
import psutil
import os


secs = {
    "minute1": 10,
    # "minute1": 60,
    "minute3": 180,
    "minute5": 300,
    "minute10": 600,
    "minute15": 900,
    "minute30": 1800,
    "minute60": 3600,
    "day": 86400,
}

def _optimize_dataframe_memory(old_df, new_data, max_length):
    """
    ğŸ§  24ì‹œê°„ ìš´ì˜: ë©”ëª¨ë¦¬ íš¨ìœ¨ì  DataFrame ê´€ë¦¬
    """
    try:
        # ğŸ”„ ê¸°ì¡´ ë°©ì‹ë³´ë‹¤ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë³‘í•©
        if len(old_df) >= max_length:
            # ì˜¤ë˜ëœ ë°ì´í„° ì œê±° (ë©”ëª¨ë¦¬ ì ˆì•½)
            old_df = old_df.iloc[-(max_length-10):].copy()
        
        # ğŸ”— íš¨ìœ¨ì  ë³‘í•©
        combined = pd.concat([old_df, new_data], ignore_index=False)
        result = combined.drop_duplicates().sort_index().iloc[-max_length:]
        
        # ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        memory_usage_mb = result.memory_usage(deep=True).sum() / 1024 / 1024
        if memory_usage_mb > 10:  # 10MB ì´ˆê³¼ ì‹œ ê²½ê³ 
            logger.warning(f"âš ï¸ DataFrame ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³¼ë‹¤: {memory_usage_mb:.2f}MB")
            
        return result
        
    except Exception as e:
        logger.error(f"âŒ DataFrame ìµœì í™” ì‹¤íŒ¨: {e}")
        # í´ë°±: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        return pd.concat([old_df, new_data]).drop_duplicates().sort_index().iloc[-max_length:]


def _force_memory_cleanup():
    """
    ğŸ§¹ 24ì‹œê°„ ìš´ì˜: ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
    """
    try:
        # Python GC ê°•ì œ ì‹¤í–‰
        collected = gc.collect()
        
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        memory_mb = memory_info.rss / 1024 / 1024
        
        logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: ê°ì²´ {collected}ê°œ ìˆ˜ì§‘, í˜„ì¬ ë©”ëª¨ë¦¬: {memory_mb:.1f}MB")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ 500MB ì´ˆê³¼ ì‹œ ê²½ê³ 
        if memory_mb > 500:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_mb:.1f}MB - ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ í•„ìš”")
            
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")


logger = logging.getLogger(__name__)


def stream_candles(
    ticker: str,
    interval: str,
    q=None,
    max_retry: int = 5,
    retry_wait: int = 3,
    stop_event=None,
    max_length: int = 500,
):
    def standardize_ohlcv(df):
        if df is None or df.empty:
            raise ValueError(f"OHLCV ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {ticker}, {interval}")
        df = df.rename(
            columns={
                "open": "Open",
                "high": "High",
                "low": "Low",
                "close": "Close",
                "volume": "Volume",
            }
        )
        if "value" in df.columns:
            df = df.drop(columns=["value"])
        df.index = pd.to_datetime(df.index)
        return df.dropna().sort_index()

    def log_warning(msg):
        logger.warning(msg)
        if q:
            q.put(("WARNING", msg))

    def log_error(msg):
        logger.error(msg)
        if q:
            q.put(("ERROR", msg))

    # âœ… ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ (ì§€ìˆ˜ ë°±ì˜¤í”„ ì „ëµ ì ìš©)
    retry_cnt = 0
    df = None
    base_delay = retry_wait
    
    while retry_cnt < max_retry:
        if stop_event and stop_event.is_set():
            log_warning("stream_candles ì¤‘ë‹¨ë¨: ì´ˆê¸° ìˆ˜ì§‘ ì¤‘ stop_event ê°ì§€")
            return
            
        try:
            df = pyupbit.get_ohlcv(ticker, interval=interval, count=max_length)
            if df is not None and not df.empty:
                break
        except Exception as e:
            log_error(f"[ì´ˆê¸°] API ì˜ˆì™¸ ë°œìƒ: {e}")
            
        retry_cnt += 1
        # ğŸ”„ ì§€ìˆ˜ ë°±ì˜¤í”„: 3ì´ˆ, 6ì´ˆ, 12ì´ˆ, 24ì´ˆ, 48ì´ˆ...
        delay = min(base_delay * (2 ** (retry_cnt - 1)), 60) + random.uniform(0, 5)
        log_warning(f"[ì´ˆê¸°] API ì‹¤íŒ¨ ({retry_cnt}/{max_retry}), {delay:.1f}ì´ˆ í›„ ì¬ì‹œë„")
        time.sleep(delay)
        
    if df is None or df.empty:
        # ğŸ”„ ì´ˆê¸° ì‹¤íŒ¨ ì‹œ ë¹ˆ DataFrameìœ¼ë¡œ ì‹œì‘ (ì—”ì§„ ì¤‘ë‹¨ ë°©ì§€)
        log_error("[ì´ˆê¸°] ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨, ë¹ˆ DataFrameìœ¼ë¡œ ì‹œì‘")
        df = pd.DataFrame(columns=['Open', 'High', 'Low', 'Close', 'Volume'])
        df.index = pd.to_datetime([])

    df = standardize_ohlcv(df)
    df = df.drop_duplicates()
    yield df

    last_candle_time = df.index[-1]

    # âœ… ì‹¤ì‹œê°„ ë£¨í”„
    while not (stop_event and stop_event.is_set()):
        time.sleep(max(secs[interval] // 3, 3))

        retry_cnt = 0
        new = None
        base_delay = retry_wait
        
        while retry_cnt < max_retry:
            if stop_event and stop_event.is_set():
                log_warning("stream_candles ì¤‘ë‹¨ë¨: ì‹¤ì‹œê°„ ë£¨í”„ ì¤‘ stop_event ê°ì§€")
                return
                
            try:
                new = pyupbit.get_ohlcv(ticker, interval=interval, count=1)
                if new is not None and not new.empty:
                    break
            except Exception as e:
                log_error(f"[ì‹¤ì‹œê°„] API ì˜ˆì™¸: {e}")
                
            retry_cnt += 1
            # ğŸ”„ ì§€ìˆ˜ ë°±ì˜¤í”„ ì ìš©
            delay = min(base_delay * (2 ** (retry_cnt - 1)), 30) + random.uniform(0, 2)
            log_warning(f"[ì‹¤ì‹œê°„] API ì‹¤íŒ¨ ({retry_cnt}/{max_retry}), {delay:.1f}ì´ˆ í›„ ì¬ì‹œë„")
            time.sleep(delay)
        else:
            # ğŸ”„ 24ì‹œê°„ ìš´ì˜: API ì‹¤íŒ¨ ì‹œ ì—”ì§„ ì¤‘ë‹¨ ë°©ì§€
            # ì§€ìˆ˜ ë°±ì˜¤í”„ ì „ëµìœ¼ë¡œ ëŒ€ê¸° í›„ ì¬ì‹œë„
            backoff_delay = min(30 + random.uniform(0, 10), 300)  # 30~300ì´ˆ ëŒ€ê¸°
            log_error(f"[ì‹¤ì‹œê°„] API ì—°ê²° ì‹¤íŒ¨, {backoff_delay:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
            time.sleep(backoff_delay)
            continue  # return ëŒ€ì‹  continueë¡œ ì—”ì§„ ìœ ì§€

        new = standardize_ohlcv(new)
        new = new.drop_duplicates()
        new_candle_time = new.index[-1]

        if new_candle_time == last_candle_time:
            continue  # ì•„ì§ ìƒˆ ìº”ë“¤ ìƒì„± ì•ˆ ë¨

        last_candle_time = new_candle_time
        
        # ğŸ“Š 24ì‹œê°„ ìš´ì˜: ë©”ëª¨ë¦¬ íš¨ìœ¨ì  DataFrame ê´€ë¦¬
        old_df = df
        df = _optimize_dataframe_memory(df, new, max_length)
        
        # ğŸ—‘ï¸ ì´ì „ DataFrame ëª…ì‹œì  ì‚­ì œ
        del old_df
        
        # ğŸ”„ ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ (5ë¶„ë§ˆë‹¤)
        if hasattr(_optimize_dataframe_memory, 'last_gc_time'):
            if time.time() - _optimize_dataframe_memory.last_gc_time > 300:
                _force_memory_cleanup()
                _optimize_dataframe_memory.last_gc_time = time.time()
        else:
            _optimize_dataframe_memory.last_gc_time = time.time()
        
        yield df
