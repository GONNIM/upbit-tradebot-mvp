from __future__ import annotations
import pyupbit
import pandas as pd
import time
import logging
import random
import gc
import psutil
import os
import math
from datetime import datetime, timedelta
from typing import Tuple, Optional
from zoneinfo import ZoneInfo

# Phase 2: Redis & WebSocket í†µí•©
try:
    from core.redis_cache import get_redis_cache
    from core.websocket_feed import get_websocket_aggregator
    from config import REDIS_ENABLED, REDIS_HOST, REDIS_PORT, REDIS_DB, REDIS_PASSWORD
    from config import WEBSOCKET_ENABLED, CANDLE_CACHE_TTL
    PHASE2_AVAILABLE = True
except ImportError as e:
    PHASE2_AVAILABLE = False
    logging.warning(f"âš ï¸ [PHASE2] Redis/WebSocket ê¸°ëŠ¥ ë¹„í™œì„±í™”: {e}")

logger = logging.getLogger(__name__)


# --------- ì‹œê°„/ê²½ê³„ ìœ í‹¸ (KST naiveë¡œ ì¼ê´€) ---------
_IV_MIN = {
    "minute1": 1,
    "minute3": 3,
    "minute5": 5,
    "minute10": 10,
    "minute15": 15,
    "minute30": 30,
    "minute60": 60,
    "day": 1440,
}

# --------- JITTER ê°’ (intervalë³„ ì°¨ë“± ì ìš©) ---------
# ë´‰ ì¢…ê°€ í™•ì • í›„ ì¶”ê°€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
# âš ï¸ ì¤‘ìš”: Upbit APIëŠ” ë´‰ ì¢…ê°€ í™•ì • í›„ ë°ì´í„° ì¤€ë¹„ê¹Œì§€ ì‹œê°„ì´ ê±¸ë¦¼
# - ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼: ì›¹ì‚¬ì´íŠ¸ì—ëŠ” ë°ì´í„°ê°€ ìˆì§€ë§Œ APIëŠ” 4~5ì´ˆ ì§€ì—°
# - ë„ˆë¬´ ì§§ìœ¼ë©´: ë°ì´í„° ëˆ„ë½ â†’ ë°±í•„ ì‹¤íŒ¨ â†’ ì˜êµ¬ ëˆ„ë½ (ì¹˜ëª…ì !)
# - ê¶Œì¥: 1ë¶„ë´‰ 3ì´ˆ, 3ë¶„ë´‰ 6ì´ˆ, ì¥ê¸°ë´‰ 8~15ì´ˆ
# - ì‹¤ì‹œê°„ì„±ë³´ë‹¤ ì•ˆì •ì„± ìš°ì„  (ëˆ„ë½ ë°©ì§€ê°€ ìµœìš°ì„ )
# - ë°±í•„ ë¡œì§(5íšŒ ì¬ì‹œë„)ì´ ì¶”ê°€ ì•ˆì „ì¥ì¹˜ ì—­í• 
JITTER_BY_INTERVAL = {
    "minute1": 8.0,   # 1ë¶„ë´‰: ì¢…ê°€ í™•ì • ëŒ€ê¸° (5.0 â†’ 8.0) - ì„ì‹œ ì¢…ê°€ íšŒí”¼
    "minute3": 15.0,  # 3ë¶„ë´‰: ì¢…ê°€ í™•ì • ëŒ€ê¸° (8.0 â†’ 15.0) - ì„ì‹œ ì¢…ê°€ íšŒí”¼
    "minute5": 15.0,  # 5ë¶„ë´‰: ì¢…ê°€ í™•ì • ëŒ€ê¸° (8.0 â†’ 15.0) - ì„ì‹œ ì¢…ê°€ íšŒí”¼
    "minute10": 15.0, # 10ë¶„ë´‰: ì¢…ê°€ í™•ì • ëŒ€ê¸° (10.0 â†’ 15.0) - ì„ì‹œ ì¢…ê°€ íšŒí”¼
    "minute15": 15.0, # 15ë¶„ë´‰: ì•ˆì •ì„± ìµœìš°ì„  (8.0 â†’ 10.0)
    "minute30": 15.0, # 30ë¶„ë´‰: ì•ˆì •ì„± ìµœìš°ì„  (10.0 â†’ 12.0)
    "minute60": 15.0, # 60ë¶„ë´‰: ì•ˆì •ì„± ìµœìš°ì„  (10.0 â†’ 12.0)
    "day": 15.0,      # ì¼ë´‰: ì‹¤ì‹œê°„ì„±ë³´ë‹¤ ì•ˆì •ì„± ìš°ì„  (ìœ ì§€)
}

# --------- í•„ìˆ˜ ë°ì´í„° ê°œìˆ˜ ì •ì˜ (ëª©í‘œì¹˜) ---------
# âš ï¸ ì£¼ì˜: Upbit APIëŠ” ê³¼ê±° ë°ì´í„° ì œì•½ìœ¼ë¡œ ëª©í‘œì¹˜ë¥¼ ëª» ì±„ìš¸ ìˆ˜ ìˆìŒ
# â†’ ì ˆëŒ€ ìµœì†ŒëŸ‰(ABSOLUTE_MIN_CANDLES)ë§Œ ì¶©ì¡±í•˜ë©´ ì „ëµ ì‹¤í–‰ í—ˆìš©
REQUIRED_CANDLES = {
    "minute1": 2000,   # 1ë¶„ë´‰: 2000ê°œ (ëª©í‘œ, Upbit ì‹¤ì œ ì œì•½: ~800ê°œ)
    "minute3": 1500,   # 3ë¶„ë´‰: 1500ê°œ (ëª©í‘œ)
    "minute5": 1200,   # 5ë¶„ë´‰: 1200ê°œ (ëª©í‘œ)
    "minute10": 1000,  # 10ë¶„ë´‰: 1000ê°œ (ëª©í‘œ)
    "minute15": 800,   # 15ë¶„ë´‰: 800ê°œ (ëª©í‘œ)
    "minute30": 600,   # 30ë¶„ë´‰: 600ê°œ (ëª©í‘œ)
    "minute60": 500,   # 60ë¶„ë´‰: 500ê°œ (ëª©í‘œ)
    "day": 400,        # ì¼ë´‰: 400ê°œ (ëª©í‘œ)
}

# ì ˆëŒ€ ìµœì†Œ ìº”ë“¤ ê°œìˆ˜ (ì´ ê°’ ë¯¸ë§Œì´ë©´ ì „ëµ ì‹œì‘ ë¶ˆê°€)
# - ì „ëµë³„ë¡œ ë‹¤ë¥¸ ìµœì†Œê°’ ì ìš©
# âš ï¸ Upbit API ì œí•œ: ìµœëŒ€ 200ê°œë§Œ ì¡°íšŒ ê°€ëŠ¥
# - EMA ì „ëµ: 200ê°œë¡œ ì‹œì‘ (ë¶ˆì™„ì „í•˜ì§€ë§Œ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°ì´í„° ì¶•ì )
ABSOLUTE_MIN_CANDLES = {
    "MACD": 600,  # MACD: ìµœëŒ€ íŒŒë¼ë¯¸í„° Ã— 3
    "EMA": 195,   # EMA: Upbit API ì œí•œ (200ê°œ ìˆ˜ì§‘ â†’ dropna/ì¤‘ë³µì œê±°ë¡œ 195ê°œ ì´ìƒ, ì‹¤ì‹œê°„ ì¶•ì )
}
ABSOLUTE_MIN_CANDLES_DEFAULT = 600  # ì „ëµ ë¯¸ì§€ì • ì‹œ ê¸°ë³¸ê°’

# ëª©í‘œ ëŒ€ë¹„ ê²½ê³  ë¹„ìœ¨ (ì´ ë¹„ìœ¨ ë¯¸ë§Œì´ë©´ ê²½ê³ ë§Œ í‘œì‹œ)
WARNING_RATIO = 0.5  # 50%


# ë””í„°ë¯¸ë‹ˆì¦˜ ì²´í¬ ë¡œê·¸ í—¬í¼
def log_det(df: pd.DataFrame, tag: str):
    """
    dfê°€ í˜„ì¬ ë™ì¼í•œ ë´‰ ì§‘í•©ì¸ì§€ ë¹ ë¥´ê²Œ ê²€ì¦í•˜ê¸° ìœ„í•œ ë¡œê·¸.
    - rows/first/last + OHLCV ì²´í¬ì„¬ì„ ë‚¨ê¸´ë‹¤.
    - tag: í˜¸ì¶œ ì§€ì  êµ¬ë¶„ìš©(ex: PRE_INIT, LOOP_MERGED, ONCE_BEFORE_RETURN)
    """
    if df is None or df.empty:
        logger.info(f"[DET] {tag} | rows=0 (empty)")
        return
    try:
        rows = len(df)
        first_i, last_i = df.index[0], df.index[-1]
        # OHLCVë§Œ ì‚¬ìš©, ì†Œìˆ˜ 8ìë¦¬ ë°˜ì˜¬ë¦¼ í›„ ë¬¸ìì—´ â†’ í•´ì‹œ
        payload = df[["Open","High","Low","Close","Volume"]].round(8).to_csv(index=True, header=False)
        checksum = hash(payload)  # íŒŒì´ì¬ ë‚´ì¥ í•´ì‹œ(ì„¸ì…˜ë§ˆë‹¤ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ, ê°™ì€ í”„ë¡œì„¸ìŠ¤ ë¹„êµìš©)
        logger.info(f"[DET] {tag} | rows={rows} | first={first_i} | last={last_i} | checksum={checksum}")
    except Exception as e:
        logger.warning(f"[DET] {tag} | logging failed: {e}")


def _forward_fill_missing_candles(df, expected_last, interval_min, _log):
    """
    ëˆ„ë½ëœ ë´‰ì„ ì´ì „ ë´‰ ê°’ìœ¼ë¡œ ì„ì‹œ ì±„ì›€ (ìµœí›„ì˜ ìˆ˜ë‹¨).
    âš ï¸ ì£¼ì˜: ì´ëŠ” ì‹¤ì œ ì‹œì¥ ë°ì´í„°ê°€ ì•„ë‹ˆë©°, ê°ì‚¬ ë¡œê·¸ì— ëª…ì‹œì ìœ¼ë¡œ í‘œì‹œë¨.

    Args:
        df: í˜„ì¬ DataFrame
        expected_last: ê¸°ëŒ€í•˜ëŠ” ë§ˆì§€ë§‰ ë´‰ì˜ íƒ€ì„ìŠ¤íƒ¬í”„
        interval_min: ë´‰ ê°„ê²© (ë¶„)
        _log: ë¡œê·¸ í•¨ìˆ˜

    Returns:
        pd.DataFrame: Forward Fillì´ ì ìš©ëœ DataFrame
    """
    if df is None or df.empty:
        return df

    # ì˜ˆìƒ ì¸ë±ìŠ¤ ìƒì„±
    last_index = df.index[-1]

    try:
        expected_index_range = pd.date_range(
            start=last_index,
            end=expected_last,
            freq=f'{interval_min}min'
        )[1:]  # ì²« ë²ˆì§¸ëŠ” ì´ë¯¸ ìˆìœ¼ë¯€ë¡œ ì œì™¸
    except Exception as e:
        _log("ERROR", f"[FORWARD-FILL] date_range ìƒì„± ì‹¤íŒ¨: {e}")
        return df

    # ëˆ„ë½ëœ ì¸ë±ìŠ¤ ì°¾ê¸°
    missing_indices = expected_index_range.difference(df.index)

    if len(missing_indices) == 0:
        return df

    _log("WARN",
        f"âš ï¸ [FORWARD-FILL] {len(missing_indices)}ê°œ ë´‰ì„ ì´ì „ ë´‰ ê°’ìœ¼ë¡œ ì„ì‹œ ì±„ì›€ "
        f"(ì‹¤ì œ ì‹œì¥ ë°ì´í„° ì•„ë‹˜! ê°ì‚¬ ë¡œê·¸ í™•ì¸ í•„ìš”)"
    )

    # ì´ì „ ë´‰ ê°’ìœ¼ë¡œ ìƒˆ í–‰ ìƒì„±
    last_row = df.iloc[-1]
    filled_rows = []

    for idx in missing_indices:
        new_row = last_row.copy()
        new_row.name = idx
        filled_rows.append(new_row)

        # ê°ì‚¬ ë¡œê·¸ì— ê¸°ë¡ (ë§¤ë§¤ ì „ëµì—ì„œ ê±¸ëŸ¬ë‚¼ ìˆ˜ ìˆë„ë¡)
        _log("WARN", f"[FORWARD-FILL] {idx} | OHLCV={last_row['Close']:.2f} (âš ï¸ ë³µì œ ë°ì´í„°)")

    if filled_rows:
        filled_df = pd.DataFrame(filled_rows)
        df = pd.concat([df, filled_df]).sort_index()

    return df


def fill_gaps_sync(
    ticker: str,
    interval: str,
    df: pd.DataFrame,
    gap_details: list,
    max_retry: int = 2,
    retry_sleep: float = 1.0
) -> Tuple[bool, pd.DataFrame]:
    """
    ê²½ë¯¸í•œ ê°­(1~2ê°œ ë´‰ ëˆ„ë½)ì„ ì¦‰ì‹œ ë™ê¸° ë°±í•„ë¡œ ë³µêµ¬.

    EMA Golden Cross íƒ€ì´ë°ì„ ë†“ì¹˜ì§€ ì•Šê¸° ìœ„í•´, 60ì´ˆ ì§€ì—° ë°±í•„ì´ ì•„ë‹Œ
    ì¦‰ì‹œ ë™ê¸° ë°©ì‹ìœ¼ë¡œ ëˆ„ë½ëœ ê³¼ê±° í™•ì • ë´‰ì„ ì¡°íšŒí•˜ì—¬ ë³µêµ¬.

    Args:
        ticker: í‹°ì»¤ (ì˜ˆ: "KRW-SUI")
        interval: ë´‰ ê°„ê²© (ì˜ˆ: "minute3")
        df: í˜„ì¬ DataFrame
        gap_details: _validate_candle_continuity ê²°ê³¼
            ê° í•­ëª©: {'prev': datetime, 'current': datetime, 'gap_minutes': float, 'missing_bars': int}
        max_retry: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        retry_sleep: ì¬ì‹œë„ ê°„ ëŒ€ê¸° ì‹œê°„(ì´ˆ)

    Returns:
        (success: bool, df: pd.DataFrame)
        - success: ëª¨ë“  ê°­ì´ ì„±ê³µì ìœ¼ë¡œ ë³µêµ¬ë˜ì—ˆìœ¼ë©´ True
        - df: ë³µêµ¬ëœ DataFrame (ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜)
    """
    if not gap_details:
        return True, df

    logger.info(f"ğŸ”„ [SYNC-FILL] ì¦‰ì‹œ ë™ê¸° ë°±í•„ ì‹œì‘: {len(gap_details)}ê°œ ê°­ ê°ì§€")

    original_df = df.copy()
    success_count = 0

    for gap in gap_details:
        prev_time = gap['prev']
        curr_time = gap['current']
        missing_bars = gap['missing_bars']

        logger.info(
            f"ğŸ”„ [SYNC-FILL] ê°­ ë³µêµ¬ ì‹œë„ | "
            f"{prev_time} â†’ {curr_time} | "
            f"ëˆ„ë½: {missing_bars}ê°œ ë´‰"
        )

        # ëˆ„ë½ êµ¬ê°„ ë³µêµ¬ ì‹œë„
        filled = False
        for attempt in range(1, max_retry + 1):
            try:
                # ëˆ„ë½ëœ êµ¬ê°„ + ì—¬ìœ ë¶„(2ê°œ) ìš”ì²­
                count = missing_bars + 2
                to_param = _fmt_to_param(curr_time)

                logger.info(
                    f"ğŸ”„ [SYNC-FILL] API í˜¸ì¶œ ({attempt}/{max_retry}) | "
                    f"count={count}, to={to_param}"
                )

                gap_data = pyupbit.get_ohlcv(
                    ticker,
                    interval=interval,
                    count=count,
                    to=to_param
                )

                if gap_data is not None and not gap_data.empty:
                    # í‘œì¤€í™” (standardize_ohlcvì™€ ë™ì¼í•œ ë¡œì§)
                    gap_data = gap_data.rename(columns={
                        "open": "Open",
                        "high": "High",
                        "low": "Low",
                        "close": "Close",
                        "volume": "Volume"
                    })
                    if "value" in gap_data.columns:
                        gap_data = gap_data.drop(columns=["value"])

                    # ì¸ë±ìŠ¤ KST naiveë¡œ í†µì¼
                    idx = pd.to_datetime(gap_data.index)
                    if getattr(idx, "tz", None) is not None:
                        idx = idx.tz_convert("Asia/Seoul").tz_localize(None)
                    gap_data.index = idx

                    gap_data = gap_data.sort_index().dropna()

                    # ì‹¤ì œ ëˆ„ë½ êµ¬ê°„ë§Œ ì¶”ì¶œ (ê¸°ì¡´ ë°ì´í„° ì œì™¸)
                    existing_indices = set(df.index)
                    gap_data_new = gap_data[~gap_data.index.isin(existing_indices)]
                    gap_data_new = gap_data_new[
                        (gap_data_new.index > prev_time) &
                        (gap_data_new.index <= curr_time)
                    ]

                    if not gap_data_new.empty:
                        # dfì— ë³‘í•©
                        df = pd.concat([df, gap_data_new]).drop_duplicates().sort_index()

                        logger.info(
                            f"âœ… [SYNC-FILL] ê°­ ë³µêµ¬ ì„±ê³µ | "
                            f"{len(gap_data_new)}ê°œ ë´‰ ì¶”ê°€ | "
                            f"ë²”ìœ„: {gap_data_new.index[0]} ~ {gap_data_new.index[-1]}"
                        )
                        filled = True
                        success_count += 1
                        break
                    else:
                        logger.warning(
                            f"âš ï¸ [SYNC-FILL] API ì‘ë‹µì´ ì´ë¯¸ ë³´ìœ  ì¤‘ì¸ ë´‰ë§Œ í¬í•¨ | "
                            f"attempt={attempt}/{max_retry}"
                        )
                else:
                    logger.warning(
                        f"âš ï¸ [SYNC-FILL] API ì‘ë‹µ ì—†ìŒ | "
                        f"attempt={attempt}/{max_retry}"
                    )

            except Exception as e:
                logger.warning(
                    f"âš ï¸ [SYNC-FILL] API ì˜ˆì™¸ | "
                    f"attempt={attempt}/{max_retry} | "
                    f"error={e}"
                )

            # ì¬ì‹œë„ ì „ ëŒ€ê¸°
            if attempt < max_retry:
                time.sleep(retry_sleep)

        if not filled:
            logger.error(
                f"âŒ [SYNC-FILL] ê°­ ë³µêµ¬ ì‹¤íŒ¨ | "
                f"{prev_time} â†’ {curr_time} | "
                f"ìµœëŒ€ {max_retry}íšŒ ì¬ì‹œë„ ì‹¤íŒ¨"
            )
            # í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ ë°˜í™˜
            return False, original_df

    if success_count == len(gap_details):
        logger.info(
            f"âœ… [SYNC-FILL] ëª¨ë“  ê°­ ë³µêµ¬ ì™„ë£Œ | "
            f"{success_count}/{len(gap_details)} ì„±ê³µ | "
            f"ìµœì¢… ë´‰ ê°œìˆ˜: {len(original_df)} â†’ {len(df)}"
        )
        return True, df
    else:
        logger.error(
            f"âŒ [SYNC-FILL] ì¼ë¶€ ê°­ ë³µêµ¬ ì‹¤íŒ¨ | "
            f"{success_count}/{len(gap_details)} ì„±ê³µ"
        )
        return False, original_df


def _iv_min(interval: str) -> int:
    return _IV_MIN.get(interval, 10)

# v1.2025.10.18.2031
def _now_kst_naive() -> datetime:
    """
    âœ… ì‹œìŠ¤í…œ ë¡œì»¬íƒ€ì„(UTC ë“±)ì— ì˜ì¡´í•˜ì§€ ì•Šê³  KST ì‹œê°ì„ tz-awareë¡œ ë§Œë“  ë’¤ tz ì œê±°.
    - ëª¨ë“  ë°” ê²½ê³„ ê³„ì‚°ì„ 'KST-naive'ë¡œ í†µì¼í•˜ê¸° ìœ„í•¨.
    """
    kst_now = datetime.now(tz=ZoneInfo("Asia/Seoul"))
    return kst_now.replace(second=0, microsecond=0).replace(tzinfo=None)

def _floor_boundary(dt: datetime, interval: str) -> datetime:
    if interval == "day":
        return dt.replace(hour=0, minute=0, second=0, microsecond=0)
    iv = _iv_min(interval)
    m = (dt.minute // iv) * iv
    return dt.replace(minute=m, second=0, microsecond=0)

def _next_boundary(dt: datetime, interval: str) -> datetime:
    if interval == "day":
        nxt = dt.replace(hour=0, minute=0, second=0, microsecond=0)
        if dt >= nxt:
            nxt += timedelta(days=1)
        return nxt
    iv = _iv_min(interval)
    m = (dt.minute // iv + 1) * iv
    add_h = m // 60
    m = m % 60
    h = (dt.hour + add_h) % 24
    nxt = dt.replace(hour=h, minute=m, second=0, microsecond=0)
    if dt.hour + add_h >= 24:
        nxt += timedelta(days=1)
    return nxt

def _fmt_to_param(dt: datetime) -> str:
    return dt.strftime("%Y-%m-%d %H:%M:%S")


# --------- ë©”ëª¨ë¦¬ ìœ í‹¸ ---------
def _optimize_dataframe_memory(old_df, new_data, max_length):
    try:
        if len(old_df) >= max_length:
            old_df = old_df.iloc[-(max_length - 10):].copy()
        combined = pd.concat([old_df, new_data], ignore_index=False)
        # âœ… ì¸ë±ìŠ¤(timestamp) ê¸°ì¤€ ì¤‘ë³µ ì œê±° - OHLCV ê°’ì´ ë™ì¼í•´ë„ ì‹œê°„ì´ ë‹¤ë¥´ë©´ ìœ ì§€
        result = combined[~combined.index.duplicated(keep='last')].sort_index().iloc[-max_length:]
        memory_usage_mb = result.memory_usage(deep=True).sum() / 1024 / 1024
        if memory_usage_mb > 10:
            logger.warning(f"âš ï¸ DataFrame ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³¼ë‹¤: {memory_usage_mb:.2f}MB")
        return result
    except Exception as e:
        logger.error(f"âŒ DataFrame ìµœì í™” ì‹¤íŒ¨: {e}")
        combined_fallback = pd.concat([old_df, new_data], ignore_index=False)
        return combined_fallback[~combined_fallback.index.duplicated(keep='last')].sort_index().iloc[-max_length:]

def _force_memory_cleanup():
    try:
        collected = gc.collect()
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024
        logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: ê°ì²´ {collected}ê°œ ìˆ˜ì§‘, í˜„ì¬ ë©”ëª¨ë¦¬: {memory_mb:.1f}MB")
        if memory_mb > 500:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_mb:.1f}MB - ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ í•„ìš”")
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")


# --------- ë©”ì¸ ìŠ¤íŠ¸ë¦¼ ---------
def stream_candles(
    ticker: str,
    interval: str,
    q=None,
    max_retry: int = 5,
    retry_wait: int = 3,
    stop_event=None,
    max_length: int = 500,
    user_id: str = None,  # Phase 2: ìºì‹œ ì‚¬ìš©ì„ ìœ„í•œ user_id
    strategy_type: str = None,  # ì „ëµ íƒ€ì… (MACD/EMA)
):
    # âœ… Phase 2: Redis & WebSocket ì´ˆê¸°í™”
    redis_cache = None
    ws_aggregator = None

    if PHASE2_AVAILABLE:
        try:
            if REDIS_ENABLED:
                redis_cache = get_redis_cache(REDIS_HOST, REDIS_PORT, REDIS_DB, REDIS_PASSWORD)
                if redis_cache.enabled:
                    logger.info(f"âœ… [PHASE2] Redis ìºì‹œ í™œì„±í™”: {ticker}/{interval}")

            if WEBSOCKET_ENABLED and interval == "minute1":  # minute1ë§Œ WebSocket ì§€ì›
                ws_aggregator = get_websocket_aggregator(ticker, redis_cache)
                logger.info(f"âœ… [PHASE2] WebSocket ì§‘ê³„ê¸° í™œì„±í™”: {ticker}")
        except Exception as e:
            logger.warning(f"âš ï¸ [PHASE2] ì´ˆê¸°í™” ì‹¤íŒ¨ (REST API ì „ìš© ëª¨ë“œ): {e}")

    # âœ… ë°ì´í„° ìˆ˜ì§‘ ìƒíƒœ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ import
    if user_id:
        try:
            from services.db import update_data_collection_status, clear_data_collection_status
        except ImportError:
            update_data_collection_status = None
            clear_data_collection_status = None
    else:
        update_data_collection_status = None
        clear_data_collection_status = None
    def _log(level: str, msg: str):
        (logger.warning if level == "WARN" else logger.error if level == "ERROR" else logger.info)(msg)
        if q:
            # í•­ìƒ 3-íŠœí”Œ ìœ ì§€
            prefix = "âš ï¸" if level == "WARN" else "âŒ" if level == "ERROR" else "â„¹ï¸"
            q.put((time.time(), "LOG", f"{prefix} {msg}"))

    def standardize_ohlcv(df):
        if df is None or df.empty:
            raise ValueError(f"OHLCV ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {ticker}, {interval}")

        before_count = len(df)
        _log("INFO", f"[standardize] ì…ë ¥ ë°ì´í„°: {before_count}ê°œ, index type={type(df.index)}, tz={getattr(df.index, 'tz', 'N/A')}")

        df = df.rename(columns={"open":"Open","high":"High","low":"Low","close":"Close","volume":"Volume"})
        if "value" in df.columns:
            df = df.drop(columns=["value"])

        # ì¸ë±ìŠ¤ tz ì •ê·œí™”: KST naiveë¡œ í†µì¼
        # âš ï¸ ì¤‘ìš”: pyupbitì€ ì´ë¯¸ KST ì‹œê°„ëŒ€ë¡œ tz-naive ë°ì´í„°ë¥¼ ë°˜í™˜í•¨
        idx = pd.to_datetime(df.index)
        try:
            if getattr(idx, "tz", None) is None:
                # âœ… pyupbitì€ ì´ë¯¸ KST naiveë¡œ ë°˜í™˜í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                _log("INFO", f"[standardize] tz-naive ê°ì§€ â†’ pyupbitì€ ì´ë¯¸ KSTì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©")
            else:
                # tz-awareì¸ ê²½ìš°ì—ë§Œ KSTë¡œ ë³€í™˜ í›„ tz ì œê±°
                _log("INFO", f"[standardize] tz-aware ê°ì§€ (tz={idx.tz}) â†’ KSTë¡œ ë³€í™˜")
                idx = idx.tz_convert("Asia/Seoul").tz_localize(None)
                _log("INFO", f"[standardize] KST naiveë¡œ ë³€í™˜ ì™„ë£Œ")
        except Exception as e:
            # ì˜ˆì™¸ ë°œìƒ ì‹œ ìƒì„¸ ë¡œê·¸
            _log("ERROR", f"[standardize] íƒ€ì„ì¡´ ë³€í™˜ ì‹¤íŒ¨: {e}")
            # ë³€í™˜ ì‹¤íŒ¨ ì‹œì—ë„ ìµœì†Œí•œ ì •ë ¬ì€ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ idx ê·¸ëŒ€ë¡œ ì‚¬ìš©

        df.index = idx

        # dropna ì „ NaN ê°œìˆ˜ í™•ì¸
        na_counts = df.isna().sum()
        if na_counts.any():
            _log("WARN", f"[standardize] NaN ë°œê²¬: {na_counts[na_counts > 0].to_dict()}")

        # ì •ë ¬ í›„ ì¤‘ë³µ ì œê±° (dropnaëŠ” ë‚˜ì¤‘ì—)
        df = df.sort_index()
        before_dedup = len(df)
        df = df.loc[~df.index.duplicated(keep="last")]
        after_dedup = len(df)

        if before_dedup > after_dedup:
            _log("WARN", f"[standardize] ì¤‘ë³µ ì œê±°: {before_dedup - after_dedup}ê°œ ì‚­ì œ ({before_dedup} â†’ {after_dedup})")

        # NaN ì œê±°
        df = df.dropna()
        after_dropna = len(df)

        if after_dedup > after_dropna:
            _log("WARN", f"[standardize] NaN ì œê±°: {after_dedup - after_dropna}ê°œ ì‚­ì œ ({after_dedup} â†’ {after_dropna})")

        _log("INFO", f"[standardize] ìµœì¢… ì¶œë ¥: {after_dropna}ê°œ (ì†ì‹¤: {before_count - after_dropna}ê°œ, {100*(before_count-after_dropna)/before_count:.1f}%)")

        return df

    # â˜… ì´ˆê¸° íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ìš© í—¬í¼
    def _fetch_initial_history(to_param: str, retry_full: int = 3) -> pd.DataFrame:
        """
        UpbitëŠ” ë¶„ë´‰ ê¸°ì¤€ í•œ ë²ˆì— ìµœëŒ€ 200ê°œë§Œ ë°˜í™˜í•˜ë¯€ë¡œ,
        max_lengthê°€ 200ì„ ë„˜ëŠ” ê²½ìš° ì—¬ëŸ¬ ë²ˆ ë‚˜ëˆ ì„œ ê³¼ê±° íˆìŠ¤í† ë¦¬ë¥¼ ëª¨ì€ë‹¤.
        - MACD/EMAë¥¼ HTS ìˆ˜ì¤€ìœ¼ë¡œ ë§ì¶”ê¸° ìœ„í•œ ê¸´ íˆìŠ¤í† ë¦¬(ì˜ˆ: 3ë¶„ë´‰ 1500~2000ê°œ) í™•ë³´ìš©.
        - retry_full: ì „ì²´ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íšŸìˆ˜
        """
        iv_min = _iv_min(interval)
        remaining = max_length
        current_to = to_param
        chunks: list[pd.DataFrame] = []
        base_delay_local = retry_wait
        total_requested = max_length
        api_calls = 0
        start_time = time.time()

        expected_calls = (max_length + 199) // 200  # ì˜¬ë¦¼ ê³„ì‚°
        expected_time = expected_calls * 0.15  # API í˜¸ì¶œë‹¹ ì•½ 0.15ì´ˆ (0.1ì´ˆ ë”œë ˆì´ + ë„¤íŠ¸ì›Œí¬)
        _log("INFO", f"[ì´ˆê¸°-multi] íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì‹œì‘: max_length={max_length}, interval={interval}")
        _log("INFO", f"[ì´ˆê¸°-multi] ì˜ˆìƒ: API í˜¸ì¶œ {expected_calls}íšŒ, ì†Œìš” ì‹œê°„ ì•½ {expected_time:.1f}ì´ˆ")

        # âœ… ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ìƒíƒœ ì €ì¥
        if update_data_collection_status:
            update_data_collection_status(
                user_id=user_id,
                is_collecting=True,
                collected=0,
                target=max_length,
                progress=0.0,
                estimated_time=expected_time,
                message=f"ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ({interval}ë´‰, ëª©í‘œ: {max_length}ê°œ)"
            )

        while remaining > 0:
            if stop_event and stop_event.is_set():
                collected = sum(len(c) for c in chunks)
                _log("WARN", f"[ì´ˆê¸°-multi] stop_event ê°ì§€ â†’ ìˆ˜ì§‘ ì¤‘ë‹¨ (collected={collected}/{total_requested})")
                break

            per_call = min(200, remaining)  # Upbit ë¶„ë´‰ ìµœëŒ€ 200ê°œ
            df_part = None
            api_calls += 1

            for attempt in range(1, max_retry + 1):
                try:
                    _log("INFO", f"[ì´ˆê¸°-multi] API í˜¸ì¶œ #{api_calls}: count={per_call}, to={current_to}")
                    df_part = pyupbit.get_ohlcv(
                        ticker,
                        interval=interval,
                        count=per_call,
                        to=current_to,
                    )
                    if df_part is not None and not df_part.empty:
                        _log("INFO", f"[ì´ˆê¸°-multi] API ì‘ë‹µ ì„±ê³µ: {len(df_part)}ê°œ ìˆ˜ì‹ ")
                        # ğŸ” PRICE-DEBUG: multi-fetch ë§ˆì§€ë§‰ í˜¸ì¶œì˜ ì›ë³¸ ë°ì´í„° (api_calls==1ì¼ë•Œë§Œ)
                        if api_calls == 1:
                            try:
                                last_3 = df_part.tail(3)
                                for idx, row in last_3.iterrows():
                                    _log("INFO", f"[PRICE-API-RAW-MULTI] {idx} | O={row['open']:.0f} H={row['high']:.0f} L={row['low']:.0f} C={row['close']:.0f}")
                            except Exception as e_log:
                                _log("WARN", f"[PRICE-API-RAW-MULTI] ë¡œê¹… ì‹¤íŒ¨: {e_log}")
                        break
                    else:
                        _log("WARN", f"[ì´ˆê¸°-multi] API ì‘ë‹µì´ ë¹„ì–´ìˆìŒ (attempt {attempt}/{max_retry})")
                except Exception as e:
                    _log("ERROR", f"[ì´ˆê¸°-multi] API ì˜ˆì™¸ ë°œìƒ: {e} (attempt {attempt}/{max_retry})")

                # Upbit API rate limit ëŒ€ì‘: í˜¸ì¶œ ê°„ ìµœì†Œ 0.1ì´ˆ ë”œë ˆì´
                delay = min(base_delay_local * (2 ** (attempt - 1)), 60) + random.uniform(0.1, 1.0)
                _log("WARN", f"[ì´ˆê¸°-multi] API ì¬ì‹œë„ ëŒ€ê¸°: {delay:.1f}ì´ˆ")
                time.sleep(delay)
            else:
                # max_retry ì‹¤íŒ¨ ì‹œ - ë¶€ë¶„ ìˆ˜ì§‘ ë°ì´í„°ë¼ë„ ë°˜í™˜í•˜ë„ë¡ ê°œì„ 
                collected = sum(len(c) for c in chunks)
                _log("ERROR", f"[ì´ˆê¸°-multi] API ì—°ì† ì‹¤íŒ¨ (collected={collected}/{total_requested})")
                # break ëŒ€ì‹  ê²½ê³ ë§Œ ë‚¨ê¸°ê³  ìˆ˜ì§‘ëœ ë°ì´í„° ë°˜í™˜
                break

            if df_part is None or df_part.empty:
                collected = sum(len(c) for c in chunks)
                _log("WARN", f"[ì´ˆê¸°-multi] ë¹ˆ ì‘ë‹µìœ¼ë¡œ ìˆ˜ì§‘ ì¢…ë£Œ (collected={collected}/{total_requested})")
                break

            chunks.append(df_part)
            got = len(df_part)
            remaining -= got

            collected_so_far = sum(len(c) for c in chunks)
            progress = collected_so_far / total_requested
            remaining_time = remaining * 0.15
            _log("INFO", f"[ì´ˆê¸°-multi] ì§„í–‰: {collected_so_far}/{total_requested} ({100*progress:.1f}%)")

            # âœ… ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
            if update_data_collection_status:
                update_data_collection_status(
                    user_id=user_id,
                    is_collecting=True,
                    collected=collected_so_far,
                    target=total_requested,
                    progress=progress,
                    estimated_time=remaining_time,
                    message=f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ({collected_so_far}/{total_requested})"
                )

            if got < per_call:
                # Upbit APIê°€ ìš”ì²­ëŸ‰ë³´ë‹¤ ì ê²Œ ë°˜í™˜ = ë” ì´ìƒ ê³¼ê±° ë°ì´í„° ì—†ìŒ
                _log("WARN", f"[ì´ˆê¸°-multi] APIê°€ ìš”ì²­ëŸ‰ë³´ë‹¤ ì ê²Œ ë°˜í™˜ (got={got}, requested={per_call}) â†’ ê³¼ê±° ë°ì´í„° ì†Œì§„")
                break

            # âœ… FIX: ë‹¤ìŒ ìš”ì²­ìš© 'to'ëŠ” ì‹¤ì œ ë°›ì€ ë°ì´í„°ì˜ ì²« ë²ˆì§¸ ë´‰ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
            # ì´ì „ ë°©ì‹(current_to ê¸°ì¤€)ì€ ì¤‘ë³µ ë°ì´í„° ë°œìƒ ê°€ëŠ¥
            try:
                # ì‹¤ì œ ë°›ì€ ë°ì´í„°ì˜ ì²« ë²ˆì§¸ ì‹œê°„
                first_timestamp = df_part.index[0]
                # 1ë¶„(ë˜ëŠ” interval) ì „ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
                dt_to = first_timestamp - timedelta(minutes=iv_min)
                current_to = _fmt_to_param(dt_to)
                _log("INFO", f"[ì´ˆê¸°-multi] ë‹¤ìŒ ìš”ì²­ ê¸°ì¤€: {current_to} (ì´ë²ˆ chunk ì²« ë´‰: {first_timestamp})")
            except Exception as e:
                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¶”ê°€ í˜ì´ì§•ì€ í•˜ì§€ ì•Šê³  ì¢…ë£Œ
                collected = sum(len(c) for c in chunks)
                _log("ERROR", f"[ì´ˆê¸°-multi] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {e} (collected={collected}/{total_requested})")
                break

            # API rate limit ì¤€ìˆ˜: í˜¸ì¶œ ê°„ 0.1ì´ˆ ë”œë ˆì´
            time.sleep(0.1)

        if not chunks:
            _log("ERROR", f"[ì´ˆê¸°-multi] ìˆ˜ì§‘ ì‹¤íŒ¨: ë°ì´í„° ì—†ìŒ")
            return pd.DataFrame(columns=["Open","High","Low","Close","Volume"])

        raw = pd.concat(chunks)
        final_count = len(raw)
        success_rate = 100 * final_count / total_requested if total_requested > 0 else 0
        elapsed_time = time.time() - start_time
        _log("INFO", f"[ì´ˆê¸°-multi] ìˆ˜ì§‘ ì™„ë£Œ: {final_count}/{total_requested} ({success_rate:.1f}%), API í˜¸ì¶œ {api_calls}íšŒ, ì†Œìš”ì‹œê°„ {elapsed_time:.2f}ì´ˆ")

        # ğŸ” PRICE-DEBUG: concat í›„ ìµœì¢… ì›ë³¸ ë°ì´í„° (ë³€í™˜ ì „)
        try:
            last_3 = raw.tail(3)
            for idx, row in last_3.iterrows():
                _log("INFO", f"[PRICE-API-CONCAT] {idx} | O={row['open']:.0f} H={row['high']:.0f} L={row['low']:.0f} C={row['close']:.0f}")
        except Exception as e_log:
            _log("WARN", f"[PRICE-API-CONCAT] ë¡œê¹… ì‹¤íŒ¨: {e_log}")

        return raw
    
    # ---- ì´ˆê¸° ë¡œë“œ: ë§‰ ë‹«íŒ ê²½ê³„ê¹Œì§€ ----
    base_delay = retry_wait
    df = None
    now = _now_kst_naive()
    bar_close = _floor_boundary(now, interval)
    to_param = _fmt_to_param(bar_close)

    # â˜… Phase 2: DB ìºì‹œ ìš°ì„  í™•ì¸ (íƒ€ì„ì¡´ ê²€ì¦ ì™„ë£Œ - í™œì„±í™”)
    if user_id:
        try:
            from services.db import load_candle_cache
            cached_df = load_candle_cache(user_id, ticker, interval, max_length)

            if cached_df is not None and len(cached_df) >= max_length:
                # âœ… ìºì‹œì— ì¶©ë¶„í•œ ë°ì´í„° ì¡´ì¬ - ì¦‰ì‹œ ì‚¬ìš©
                df = cached_df.tail(max_length)
                _log("INFO", f"[CACHE-HIT] {len(df)}ê°œ ë¡œë“œ ì™„ë£Œ (ì¦‰ì‹œ ì „ëµ ì‹œì‘ ê°€ëŠ¥)")
            elif cached_df is not None and len(cached_df) > 0:
                # âœ… ìºì‹œ ë¶€ì¡± - APIë¡œ ìµœì‹  ë°ì´í„° ìˆ˜ì§‘í•˜ì—¬ ë³‘í•©
                needed = max_length - len(cached_df)
                _log("INFO", f"[CACHE-PARTIAL] DB {len(cached_df)}ê°œ ì¡´ì¬, APIë¡œ ìµœì‹  {needed}ê°œ ì¶”ê°€ ìˆ˜ì§‘")

                # APIë¡œ ìµœì‹  ë°ì´í„° ìˆ˜ì§‘
                api_df = pyupbit.get_ohlcv(ticker, interval=interval, count=needed)
                if api_df is not None and not api_df.empty:
                    # ì»¬ëŸ¼ëª… í†µì¼
                    api_df = api_df.rename(columns={
                        "open": "Open", "high": "High", "low": "Low",
                        "close": "Close", "volume": "Volume"
                    })

                    # ë³‘í•© ë° ì¤‘ë³µ ì œê±°
                    df = pd.concat([cached_df, api_df])
                    df = df[~df.index.duplicated(keep='last')].sort_index()
                    df = df.tail(max_length)
                    _log("INFO", f"[CACHE-MERGE] ë³‘í•© ì™„ë£Œ: ìµœì¢… {len(df)}ê°œ (DB + API)")
                else:
                    # API ì‹¤íŒ¨ ì‹œ ìºì‹œë§Œ ì‚¬ìš©
                    df = cached_df
                    _log("WARN", f"[CACHE-MERGE] API ì‹¤íŒ¨, ìºì‹œ {len(df)}ê°œë§Œ ì‚¬ìš©")
            else:
                _log("INFO", f"[CACHE-MISS] ìºì‹œ ì—†ìŒ, APIë¡œ ì „ì²´ ìˆ˜ì§‘")
        except Exception as e:
            _log("WARN", f"[CACHE] ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, APIë¡œ ì „ì²´ ìˆ˜ì§‘: {e}")

    # âœ… ì „ëµë³„ ìµœì†Œ ìº”ë“¤ ê°œìˆ˜ ê²°ì •
    strategy_tag = (strategy_type or "MACD").upper().strip()
    absolute_min = ABSOLUTE_MIN_CANDLES.get(strategy_tag, ABSOLUTE_MIN_CANDLES_DEFAULT)
    _log("INFO", f"[ì´ˆê¸°] strategy={strategy_tag}, absolute_min_candles={absolute_min}")

    # â˜… ìºì‹œ ë¯¸ìŠ¤ ë˜ëŠ” ë¶€ì¡±: API í˜¸ì¶œ
    if df is None:
        _log("INFO", f"[ì´ˆê¸°] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: ticker={ticker}, interval={interval}, max_length={max_length}")

        # âœ… pyupbitëŠ” ë‚´ë¶€ì ìœ¼ë¡œ multi-fetch ì§€ì› (200ê°œì”© ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œ)
        # - max_length=400 ìš”ì²­ ì‹œ ìë™ìœ¼ë¡œ 2ë²ˆ í˜¸ì¶œí•˜ì—¬ 400ê°œ ë°˜í™˜
        # - ì´ˆê¸° ìˆ˜ì§‘ ì‹œê°„: ì•½ 10ì´ˆ ì†Œìš” (200ê°œ: 2ì´ˆ â†’ 400ê°œ: 10ì´ˆ)
        effective_count = max_length
        _log("INFO", f"[ì´ˆê¸°] pyupbit multi-fetch í™œì„±í™”: {effective_count}ê°œ ìš”ì²­")

        if True:  # í•­ìƒ ë‹¨ì¼ í˜¸ì¶œ ì‚¬ìš©
            for attempt in range(1, max_retry + 1):
                if stop_event and stop_event.is_set():
                    _log("WARN", "stream_candles ì¤‘ë‹¨ë¨: ì´ˆê¸° ìˆ˜ì§‘ ì¤‘ stop_event ê°ì§€")
                    return
                try:
                    # âœ… FIX: to íŒŒë¼ë¯¸í„° ì œê±° - í™•ì •ëœ ìµœê·¼ ë´‰ë§Œ ì¡°íšŒ
                    _log("INFO", f"[ì´ˆê¸°] API ë‹¨ì¼ í˜¸ì¶œ: count={effective_count}")
                    df = pyupbit.get_ohlcv(ticker, interval=interval, count=effective_count)
                    if df is not None and not df.empty:
                        _log("INFO", f"[ì´ˆê¸°] API ì‘ë‹µ ì„±ê³µ: {len(df)}ê°œ ìˆ˜ì‹ ")
                        # ğŸ” PRICE-DEBUG: pyupbit ì›ë³¸ ë°ì´í„° (ë³€í™˜ ì „)
                        try:
                            last_3 = df.tail(3)
                            for idx, row in last_3.iterrows():
                                _log("INFO", f"[PRICE-API-RAW] {idx} | O={row['open']:.0f} H={row['high']:.0f} L={row['low']:.0f} C={row['close']:.0f}")
                        except Exception as e_log:
                            _log("WARN", f"[PRICE-API-RAW] ë¡œê¹… ì‹¤íŒ¨: {e_log}")
                        break
                except Exception as e:
                    _log("ERROR", f"[ì´ˆê¸°] API ì˜ˆì™¸ ë°œìƒ: {e}")

                delay = min(base_delay * (2 ** (attempt - 1)), 60) + random.uniform(0, 5)
                _log("WARN", f"[ì´ˆê¸°] API ì‹¤íŒ¨ ({attempt}/{max_retry}), {delay:.1f}ì´ˆ í›„ ì¬ì‹œë„")
                time.sleep(delay)
        else:
            # â˜… MACD/EMA ì•ˆì •í™”ë¥¼ ìœ„í•´ ê¸´ íˆìŠ¤í† ë¦¬(max_length) í™•ë³´ + ì¬ì‹œë„
            _log("INFO", f"[ì´ˆê¸°] max_length > 200 â†’ multi-fetch ëª¨ë“œ ì‚¬ìš© (ìµœëŒ€ 3íšŒ ì¬ì‹œë„)")

            retry_count = 0
            max_full_retry = 3

            while retry_count < max_full_retry:
                df = _fetch_initial_history(to_param, retry_full=max_full_retry)

                if df is not None and not df.empty:
                    temp_len = len(df)
                    success_rate = 100 * temp_len / max_length if max_length > 0 else 0

                    # ì ˆëŒ€ ìµœì†ŒëŸ‰ ì´ìƒì´ë©´ ì„±ê³µ (Upbit API ì œì•½ ê³ ë ¤)
                    if temp_len >= absolute_min:
                        _log("INFO", f"[ì´ˆê¸°-ì¬ì‹œë„] ìˆ˜ì§‘ ì„±ê³µ: {temp_len}/{max_length} ({success_rate:.1f}%) - ì ˆëŒ€ ìµœì†ŒëŸ‰({absolute_min}) ì¶©ì¡±")
                        break
                    else:
                        retry_count += 1
                        if retry_count < max_full_retry:
                            retry_delay = 5 + random.uniform(0, 3)
                            _log("WARN", f"[ì´ˆê¸°-ì¬ì‹œë„] ì ˆëŒ€ ë¶€ì¡± ({temp_len}/{absolute_min}) - {retry_delay:.1f}ì´ˆ í›„ ì „ì²´ ì¬ì‹œë„ ({retry_count}/{max_full_retry})")
                            time.sleep(retry_delay)
                        else:
                            _log("ERROR", f"[ì´ˆê¸°-ì¬ì‹œë„] ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬: {temp_len}/{absolute_min} (ì ˆëŒ€ ìµœì†ŒëŸ‰ ë¯¸ë‹¬)")
                else:
                    retry_count += 1
                    if retry_count < max_full_retry:
                        retry_delay = 5 + random.uniform(0, 3)
                        _log("ERROR", f"[ì´ˆê¸°-ì¬ì‹œë„] ìˆ˜ì§‘ ì‹¤íŒ¨ - {retry_delay:.1f}ì´ˆ í›„ ì „ì²´ ì¬ì‹œë„ ({retry_count}/{max_full_retry})")
                        time.sleep(retry_delay)

        # â˜… Phase 2: API í˜¸ì¶œ í›„ DBì— ì €ì¥
        if user_id and df is not None and not df.empty:
            try:
                from services.db import save_candle_cache
                save_candle_cache(user_id, ticker, interval, df)
            except Exception as e:
                _log("WARN", f"[CACHE] Save failed (ignored): {e}")

    if df is None or df.empty:
        raise ValueError(f"[ì´ˆê¸°] ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: ticker={ticker}, interval={interval}")

    _log("INFO", f"[ì´ˆê¸°] ìˆ˜ì§‘ëœ ì›ë³¸ ë°ì´í„°: {len(df)}ê°œ")

    # ğŸ” PRICE-DEBUG: standardize ì „ ë°ì´í„° (API ì§í›„)
    try:
        last_3 = df.tail(3)
        for idx, row in last_3.iterrows():
            _log("INFO", f"[PRICE-BEFORE-STD] {idx} | O={row['open']:.0f} H={row['high']:.0f} L={row['low']:.0f} C={row['close']:.0f}")
    except Exception as e_log:
        _log("WARN", f"[PRICE-BEFORE-STD] ë¡œê¹… ì‹¤íŒ¨: {e_log}")

    df = standardize_ohlcv(df).drop_duplicates()
    final_len = len(df)

    # ğŸ” PRICE-DEBUG: standardize í›„ ë°ì´í„°
    try:
        last_3 = df.tail(3)
        for idx, row in last_3.iterrows():
            _log("INFO", f"[PRICE-AFTER-STD] {idx} | O={row['Open']:.0f} H={row['High']:.0f} L={row['Low']:.0f} C={row['Close']:.0f}")
    except Exception as e_log:
        _log("WARN", f"[PRICE-AFTER-STD] ë¡œê¹… ì‹¤íŒ¨: {e_log}")
    success_rate = 100 * final_len / max_length if max_length > 0 else 0

    _log("INFO", f"[ì´ˆê¸°] standardize í›„ ìµœì¢… ë°ì´í„°: {final_len}ê°œ (ëª©í‘œ: {max_length}ê°œ, ë‹¬ì„±ë¥ : {success_rate:.1f}%)")

    # â˜… ë°ì´í„° ë¶€ì¡± ê²½ê³  (ì—”ì§„ì€ ê³„ì† ì‹¤í–‰í•˜ë©´ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°ì´í„° ì¶•ì )
    if final_len < absolute_min:
        _log("WARN", "")
        _log("WARN", "=" * 80)
        _log("WARN", f"âš ï¸  ì´ˆê¸° ë°ì´í„° ë¶€ì¡±: {final_len}/{absolute_min}ê°œ (ê¶Œì¥: {absolute_min}ê°œ)")
        _log("WARN", "=" * 80)
        _log("WARN", f"   - í˜„ì¬ {final_len}ê°œë¡œ ì „ëµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        _log("WARN", f"   - ì§€í‘œê°€ ì´ˆê¸°ì— ë¶ˆì™„ì „í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        _log("WARN", f"   - ì‹¤ì‹œê°„ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ë©° ì ì§„ì ìœ¼ë¡œ ì •í™•ë„ê°€ í–¥ìƒë©ë‹ˆë‹¤.")
        _log("WARN", f"   - ì•½ {absolute_min - final_len}ë¶„ í›„ ê¶Œì¥ ë°ì´í„°ëŸ‰ ë‹¬ì„±")
        _log("WARN", "=" * 80)
        _log("WARN", "")

    # ëª©í‘œ ëŒ€ë¹„ 50% ë¯¸ë§Œì´ë©´ ê²½ê³  (ì „ëµì€ ì‹¤í–‰)
    if final_len < max_length * WARNING_RATIO:
        _log("WARN",
            f"âš ï¸ ëª©í‘œ ëŒ€ë¹„ {success_rate:.1f}% ë‹¬ì„± ({final_len}/{max_length}) - "
            f"Upbit API ì œì•½ìœ¼ë¡œ ì¶”ì •. ì ˆëŒ€ ìµœì†ŒëŸ‰({absolute_min})ì€ ì¶©ì¡±í•˜ì—¬ ì „ëµ ì‹¤í–‰"
        )

        # âš ï¸ EMA ì „ëµ + 200ê°œ ë°ì´í„° + ëª©í‘œ > 200ì¸ ê²½ìš° ì¶”ê°€ ê²½ê³ 
        if strategy_tag == "EMA" and final_len <= 200 and max_length > 200:
            _log("WARN", "")
            _log("WARN", "=" * 80)
            _log("WARN", "âš ï¸  [EMA ì „ëµ] ì´ˆê¸° ë°ì´í„° ë¶€ì¡± ì•ˆë‚´")
            _log("WARN", "=" * 80)
            _log("WARN", f"   - Upbit API ì œí•œìœ¼ë¡œ ìµœëŒ€ 200ê°œ ë´‰ë§Œ ì¡°íšŒ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
            _log("WARN", f"   - í˜„ì¬ {final_len}ê°œ ë°ì´í„°ë¡œ ì „ëµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            _log("WARN", f"   - 200ì¼ ì´ë™í‰ê·  ë“± ê¸´ ê¸°ê°„ ì§€í‘œëŠ” ì´ˆê¸°ì— ë¶ˆì™„ì „í•©ë‹ˆë‹¤.")
            _log("WARN", f"   - ì‹¤ì‹œê°„ ë°ì´í„°ê°€ ìŒ“ì´ë©´ì„œ ì ì§„ì ìœ¼ë¡œ ì •í™•ë„ê°€ í–¥ìƒë©ë‹ˆë‹¤.")
            _log("WARN", f"   - ì™„ì „í•œ ì§€í‘œ ê³„ì‚°ì€ ì•½ {max_length - final_len}ë¶„ í›„ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
            _log("WARN", "=" * 80)
            _log("WARN", "")

    # âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ - ìƒíƒœ ì´ˆê¸°í™”
    if clear_data_collection_status:
        clear_data_collection_status(user_id)
        _log("INFO", f"[ì´ˆê¸°] ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ! ì—”ì§„ ì‹œì‘í•©ë‹ˆë‹¤.")

    yield df

    last_open = df.index[-1]  # ìš°ë¦¬ê°€ ê°€ì§„ ë§ˆì§€ë§‰ bar_open (tz-naive)

    # ---- ì‹¤ì‹œê°„ ë£¨í”„: ê²½ê³„ ë™ê¸°í™” â†’ ë‹«íŒ ë´‰ ì¡°íšŒ â†’ ê°­ ë°±í•„ ----
    # âœ… intervalë³„ JITTER ê°’ ì„ íƒ
    jitter = JITTER_BY_INTERVAL.get(interval, 0.7)
    _log("INFO", f"[ì‹¤ì‹œê°„ ë£¨í”„] interval={interval}, jitter={jitter}ì´ˆ")

    while not (stop_event and stop_event.is_set()):
        # âœ… ì§€ì—°ëœ ë°±í•„ ì²˜ë¦¬ (ë£¨í”„ ì´ˆë°˜ ì‹¤í–‰)
        # - ì¶©ë¶„í•œ ì‹œê°„ì´ ì§€ë‚œ í›„ ê³¼ê±° ëˆ„ë½ êµ¬ê°„ì„ ì•ˆì •ì ìœ¼ë¡œ ì¬ì¡°íšŒ
        if hasattr(stream_candles, '_pending_backfill') and stream_candles._pending_backfill:
            current_time = time.time()
            completed_backfills = []

            for pending in stream_candles._pending_backfill[:]:  # ë³µì‚¬ë³¸ìœ¼ë¡œ ìˆœíšŒ
                if current_time >= pending['retry_after']:
                    try:
                        _log("INFO",
                            f"[ì§€ì—° ë°±í•„ ì‹œë„] {pending['missing_bars']}ê°œ ë´‰ | "
                            f"êµ¬ê°„: {pending['start']} ~ {pending['end']}"
                        )

                        # ì¶©ë¶„í•œ ì‹œê°„ì´ ì§€ë‚¬ìœ¼ë¯€ë¡œ ê³¼ê±° êµ¬ê°„ ì¬ì¡°íšŒ
                        delayed_fill = pyupbit.get_ohlcv(
                            pending['ticker'],
                            interval=pending['interval'],
                            count=pending['missing_bars'] + 5,  # ì—¬ìœ ë¶„ ì¶”ê°€
                            to=_fmt_to_param(pending['end'])
                        )

                        if delayed_fill is not None and not delayed_fill.empty:
                            delayed_fill = standardize_ohlcv(delayed_fill).drop_duplicates()

                            # ì‹¤ì œë¡œ ëˆ„ë½ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì¤‘ë³µ ë°©ì§€)
                            existing_indices = set(df.index)
                            delayed_fill_new = delayed_fill[~delayed_fill.index.isin(existing_indices)]
                            delayed_fill_new = delayed_fill_new[
                                (delayed_fill_new.index > pending['start']) &
                                (delayed_fill_new.index <= pending['end'])
                            ]

                            if not delayed_fill_new.empty:
                                # dfì— ë³‘í•© (ê³¼ê±° êµ¬ê°„ì´ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì‚½ì… ê°€ëŠ¥)
                                df = pd.concat([df, delayed_fill_new]).drop_duplicates().sort_index()

                                _log("INFO",
                                    f"âœ… [ì§€ì—° ë°±í•„ ì„±ê³µ] {len(delayed_fill_new)}ê°œ ë´‰ ë³µêµ¬ ì™„ë£Œ | "
                                    f"êµ¬ê°„: {delayed_fill_new.index[0]} ~ {delayed_fill_new.index[-1]}"
                                )
                                completed_backfills.append(pending)
                            else:
                                _log("WARN", f"[ì§€ì—° ë°±í•„] ì‘ë‹µ ë°ì´í„°ê°€ ì´ë¯¸ ë³´ìœ  ì¤‘ì¸ ë´‰ë§Œ í¬í•¨ â†’ ì™„ë£Œ ì²˜ë¦¬")
                                completed_backfills.append(pending)
                        else:
                            # ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ë‹¤ì‹œ 30ì´ˆ í›„ë¡œ ì—°ê¸° (ìµœëŒ€ 5íšŒê¹Œì§€)
                            retry_count = pending.get('retry_count', 0) + 1
                            if retry_count < 5:
                                pending['retry_after'] = current_time + 30
                                pending['retry_count'] = retry_count
                                _log("WARN", f"[ì§€ì—° ë°±í•„] ì¬ì‹œë„ ì‹¤íŒ¨ ({retry_count}/5) â†’ 30ì´ˆ í›„ ì¬ì‹œë„")
                            else:
                                _log("ERROR", f"[ì§€ì—° ë°±í•„] ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬ ({retry_count}íšŒ) â†’ í¬ê¸°")
                                completed_backfills.append(pending)

                    except Exception as e:
                        _log("ERROR", f"[ì§€ì—° ë°±í•„ ì‹¤íŒ¨] {e}")
                        # ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ì¬ì‹œë„ ì¹´ìš´íŠ¸ ì¦ê°€
                        retry_count = pending.get('retry_count', 0) + 1
                        if retry_count < 5:
                            pending['retry_after'] = current_time + 30
                            pending['retry_count'] = retry_count
                        else:
                            completed_backfills.append(pending)

            # ì™„ë£Œëœ ë°±í•„ í•­ëª© ì œê±°
            for completed in completed_backfills:
                if completed in stream_candles._pending_backfill:
                    stream_candles._pending_backfill.remove(completed)

        # ğŸ”¥ FIX: sleep ê³„ì‚°ì€ ì‹¤ì œ ì‹œê°(ì´ˆ í¬í•¨) ì‚¬ìš©
        now_real = datetime.now(ZoneInfo("Asia/Seoul")).replace(tzinfo=None)
        now = _now_kst_naive()  # ê²½ê³„ ê³„ì‚°ìš© (ì´ˆ ì œê±°)
        next_close = _next_boundary(now, interval)
        sleep_sec = max(0.0, (next_close - now_real).total_seconds() + jitter)

        # ğŸ” DEBUG: ë£¨í”„ ì§„ì… í™•ì¸
        _log("INFO", f"[ì‹¤ì‹œê°„ ë£¨í”„] sleep={sleep_sec:.1f}ì´ˆ | now_real={now_real.strftime('%H:%M:%S')} | now={now} | next_close={next_close} | last_open={last_open}")
        time.sleep(sleep_sec)

        # ğŸ”¥ FIX: sleep í›„ í˜„ì¬ ì‹œê° ì¬ê³„ì‚° (next_close ì¬ì‚¬ìš© ê¸ˆì§€!)
        # - sleep ì¤‘ ì‹œê°„ì´ í˜ë €ìœ¼ë¯€ë¡œ í˜„ì¬ ì‹œê° ê¸°ì¤€ìœ¼ë¡œ boundary ì¬ê³„ì‚° í•„ìš”
        # - íŠ¹íˆ ì—”ì§„ ì¬ì‹œì‘ ì§í›„ ì§§ì€ sleep ì‹œ í•„ìˆ˜!
        now_after_sleep = _now_kst_naive()
        next_close_after = _next_boundary(now_after_sleep, interval)

        # ë§‰ ë‹«íŒ ë´‰ì˜ open
        iv = _iv_min(interval)
        boundary_open = next_close_after - timedelta(minutes=iv)

        # ğŸ” DEBUG: sleep ì „í›„ ì‹œê° ë¹„êµ (ë²„ê·¸ ë””ë²„ê¹…ìš©)
        if next_close != next_close_after:
            _log("INFO",
                f"[ì‹œê° ë™ê¸°í™”] sleep ì „: next_close={next_close} â†’ "
                f"sleep í›„: next_close_after={next_close_after} | "
                f"boundary_open={boundary_open}"
            )

        # ğŸ”¥ FIX: ì¤‘ê°„ ëˆ„ë½ë¶„ ê³„ì‚° (ì˜¬ë¦¼ ì²˜ë¦¬ë¡œ 1ë¶„ ê°­ë„ ê°ì§€)
        # ê¸°ì¡´: int() ì ˆì‚¬ â†’ 1ë¶„ ê°­ì´ 0ìœ¼ë¡œ ê³„ì‚°ë˜ì–´ ëˆ„ë½!
        # ê°œì„ : math.ceil() ì˜¬ë¦¼ â†’ 1ë¶„ ê°­ë„ 1ë¡œ ê³„ì‚°
        gap_seconds = (boundary_open - last_open).total_seconds()
        gap = math.ceil(gap_seconds / (iv * 60))  # ì˜¬ë¦¼ ì²˜ë¦¬

        # ğŸ›¡ï¸ ì•ˆì „ì¥ì¹˜: gapì´ 1 ì´í•˜ì—¬ë„ ìµœì†Œ 2ê°œ ë´‰ ìš”ì²­ (ì¤‘ë³µ ì œê±°)
        # - ì´ìœ : API ì‘ë‹µ ì§€ì—°ìœ¼ë¡œ ìµœì‹  ë´‰ì´ ëˆ„ë½ë  ìˆ˜ ìˆìŒ
        # - ì¤‘ë³µì€ ë‚˜ì¤‘ì— ìë™ ì œê±°ë˜ë¯€ë¡œ ì•ˆì „
        need = max(2, min(gap + 1, 200))  # ìµœì†Œ 2ê°œ, gap+1ê°œ ìš”ì²­

        # ğŸ” DEBUG: API í˜¸ì¶œ ì „ íŒŒë¼ë¯¸í„°
        _log("INFO", f"[ì‹¤ì‹œê°„ API] boundary_open={boundary_open} | gap={gap} | need={need} | last_open={last_open}")

        # ğŸ”¥ FIX: ì‘ë‹µ ì§€ì—° ì¬ì‹œë„ë¥¼ ë‚´ë¶€ ë£¨í”„ë¡œ êµ¬í˜„ (continue ë²„ê·¸ ìˆ˜ì •)
        # ê¸°ì¡´ ë¬¸ì œ: continue â†’ while ì²˜ìŒ ë³µê·€ â†’ sleep ë‹¤ì‹œ ì‹¤í–‰ â†’ ì¬ì‹œë„ ë¬´íš¨í™”!
        # í•´ê²°: ë‚´ë¶€ for ë£¨í”„ë¡œ ì¬ì‹œë„ â†’ API í˜¸ì¶œë§Œ ë°˜ë³µ â†’ sleep ê±´ë„ˆë›°ì§€ ì•ŠìŒ
        new = None
        max_delay_retry = 10  # âœ… CTO ìŠ¹ì¸: 5íšŒ â†’ 10íšŒ (API ì¥ì•  ëŒ€ì‘ ê°•í™”)

        for delay_retry_attempt in range(max_delay_retry):
            if stop_event and stop_event.is_set():
                _log("WARN", "stream_candles ì¤‘ë‹¨ë¨: ì‹¤ì‹œê°„ ë£¨í”„ ì¤‘ stop_event ê°ì§€")
                return

            # âœ… Phase 2: ë‹¤ì¤‘ ì†ŒìŠ¤ ì¡°íšŒ (Redis ìºì‹œ â†’ REST API)
            new = None
            cache_hit = False

            # 1ë‹¨ê³„: Redis ìºì‹œ í™•ì¸ (ë‹¨ì¼ ë´‰ ì¡°íšŒ)
            if redis_cache and redis_cache.enabled and gap == 1:
                try:
                    cached_data = redis_cache.get_candle(ticker, interval, boundary_open)
                    if cached_data:
                        # ìºì‹œ íˆíŠ¸: DataFrameìœ¼ë¡œ ë³€í™˜
                        cached_ts = pd.to_datetime(cached_data["timestamp"])
                        new = pd.DataFrame([{
                            "Open": cached_data["Open"],
                            "High": cached_data["High"],
                            "Low": cached_data["Low"],
                            "Close": cached_data["Close"],
                            "Volume": cached_data["Volume"],
                        }], index=[cached_ts])
                        cache_hit = True
                        _log("INFO", f"âœ… [REDIS-HIT] {boundary_open} | C={cached_data['Close']:.0f}")
                except Exception as e:
                    _log("WARN", f"âš ï¸ [REDIS] ì¡°íšŒ ì‹¤íŒ¨ (REST APIë¡œ ëŒ€ì²´): {e}")

            # 2ë‹¨ê³„: REST API í˜¸ì¶œ (ìºì‹œ ë¯¸ìŠ¤ ë˜ëŠ” ì—¬ëŸ¬ ë´‰ í•„ìš”)
            if not cache_hit:
                for attempt in range(1, max_retry + 1):
                    if stop_event and stop_event.is_set():
                        return
                    try:
                        # âœ… to íŒŒë¼ë¯¸í„° ì œê±° - í•­ìƒ ìµœì‹  í™•ì • ë´‰ë§Œ ì¡°íšŒ (ì„ì‹œ ì¢…ê°€ íšŒí”¼)
                        _log("INFO",
                            f"[ì‹¤ì‹œê°„ API] í˜¸ì¶œ #{delay_retry_attempt + 1}/{max_delay_retry} | "
                            f"count={need} (ìµœì‹  í™•ì • ë´‰)"
                        )
                        new = pyupbit.get_ohlcv(ticker, interval=interval, count=need)
                        if new is not None and not new.empty:
                            # ğŸ” PRICE-DEBUG: ì‹¤ì‹œê°„ API ì›ë³¸ ë°ì´í„°
                            try:
                                last_3 = new.tail(min(3, len(new)))
                                for idx, row in last_3.iterrows():
                                    _log("INFO", f"[PRICE-REALTIME-RAW] {idx} | O={row['open']:.0f} H={row['high']:.0f} L={row['low']:.0f} C={row['close']:.0f}")
                            except Exception as e_log:
                                _log("WARN", f"[PRICE-REALTIME-RAW] ë¡œê¹… ì‹¤íŒ¨: {e_log}")
                            break
                    except Exception as e:
                        _log("ERROR", f"[ì‹¤ì‹œê°„ API] ì˜ˆì™¸: {e} (attempt {attempt}/{max_retry})")

                    delay = min(base_delay * (2 ** (attempt - 1)), 30) + random.uniform(0, 2)
                    _log("WARN", f"[ì‹¤ì‹œê°„ API] {delay:.1f}ì´ˆ í›„ ì¬ì‹œë„ (ì—°ê²° ì‹¤íŒ¨)")
                    time.sleep(delay)

            # API ì—°ê²° ìì²´ ì‹¤íŒ¨ ì‹œ ì™¸ë¶€ while ë£¨í”„ë¡œ (ê²½ê³„ ì¬ë™ê¸°í™”)
            if new is None or new.empty:
                backoff = min(30 + random.uniform(0, 10), 300)
                _log("ERROR", f"[ì‹¤ì‹œê°„ API] ì—°ê²° ì‹¤íŒ¨, {backoff:.1f}ì´ˆ í›„ ê²½ê³„ ì¬ë™ê¸°í™”")
                time.sleep(backoff)
                break  # ë‚´ë¶€ ë£¨í”„ íƒˆì¶œ â†’ while ì²˜ìŒìœ¼ë¡œ (ê²½ê³„ ì¬ê³„ì‚°)

            # ğŸ›¡ï¸ ì‘ë‹µ ê²€ì¦: ê¸°ëŒ€í•œ ë´‰ì„ ë°›ì•˜ëŠ”ê°€?
            _log("INFO", f"[ì‹¤ì‹œê°„ API ì‘ë‹µ] rows={len(new)} | first={new.index[0]} | last={new.index[-1]}")

            expected_last = boundary_open
            actual_last = new.index[-1]
            time_gap = (expected_last - actual_last).total_seconds() / 60
            time_gap_bars = time_gap / iv

            # ğŸ›¡ï¸ ì‘ë‹µ ì§€ì—° ê°ì§€: 0.5ë´‰ ì´ìƒ ì°¨ì´
            if time_gap_bars >= 0.5:
                _log("WARN",
                    f"[ì‹¤ì‹œê°„ API] ì‘ë‹µ ì§€ì—° ê°ì§€! "
                    f"ê¸°ëŒ€: {expected_last} | ì‹¤ì œ: {actual_last} | "
                    f"ê°­: {time_gap:.1f}ë¶„ ({time_gap_bars:.1f}ë´‰)"
                )

                # ìµœëŒ€ ì¬ì‹œë„ ì „ì´ë©´ ëŒ€ê¸° í›„ ì¬ì‹œë„
                if delay_retry_attempt < max_delay_retry - 1:
                    retry_delays = [3, 5, 8, 12, 15]
                    retry_delay = retry_delays[min(delay_retry_attempt, len(retry_delays) - 1)]
                    retry_delay += random.uniform(0, 2)

                    _log("WARN",
                        f"[ì‹¤ì‹œê°„ API] {retry_delay:.1f}ì´ˆ í›„ ì¬ì‹œë„ "
                        f"({delay_retry_attempt + 1}/{max_delay_retry}) - ëˆ„ë½ ë°©ì§€!"
                    )
                    time.sleep(retry_delay)
                    # continueë¡œ ë‚´ë¶€ for ë£¨í”„ ë°˜ë³µ (API ì¬í˜¸ì¶œ)
                    continue
                else:
                    _log("ERROR",
                        f"[ì‹¤ì‹œê°„ API] ìµœëŒ€ ì¬ì‹œë„ ë„ë‹¬ ({max_delay_retry}íšŒ) - "
                        f"ë°±í•„ ë¡œì§ìœ¼ë¡œ ë³µêµ¬ ì‹œë„"
                    )
                    # breakë¡œ ë‚´ë¶€ ë£¨í”„ íƒˆì¶œ â†’ ë°±í•„ ì‹œë„
                    break
            else:
                # ì •ìƒ ì‘ë‹µ: ë‚´ë¶€ ë£¨í”„ íƒˆì¶œ
                _log("INFO", f"[ì‹¤ì‹œê°„ API] ì •ìƒ ì‘ë‹µ í™•ì¸ (ê°­: {time_gap_bars:.2f}ë´‰)")
                break

        # API ì‘ë‹µ ì—†ìŒ ì‹œ ë‹¤ìŒ ë£¨í”„ë¡œ
        if new is None or new.empty:
            _log("WARN", f"[ì‹¤ì‹œê°„ API] ì‘ë‹µ ì—†ìŒ - last_open ìœ ì§€í•˜ì—¬ ë‹¤ìŒ ë£¨í”„ì—ì„œ ì¬ì‹œë„")
            continue

        new = standardize_ohlcv(new).drop_duplicates()

        # âœ… Phase 2: Redisì— ì €ì¥ (ìºì‹œ ë¯¸ìŠ¤ì¸ ê²½ìš°ë§Œ)
        if not cache_hit and redis_cache and redis_cache.enabled and not new.empty:
            try:
                redis_cache.save_candles_bulk(ticker, interval, new, ttl=CANDLE_CACHE_TTL)
            except Exception as e:
                _log("WARN", f"âš ï¸ [REDIS-SAVE] ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")

        # ğŸ” DEBUG: standardize í›„ ë°ì´í„°
        _log("INFO", f"[ì‹¤ì‹œê°„ í‘œì¤€í™” í›„] rows={len(new)} | first={new.index[0]} | last={new.index[-1]}")

        # ğŸ›¡ï¸ ë°©ì•ˆ 3: ê°•í™”ëœ ëˆ„ë½ ê°ì§€ ë° ê°•ì œ ë°±í•„
        if not new.empty:
            new_last = new.index[-1]

            # ì˜ˆìƒ ë²”ìœ„ ê³„ì‚°
            expected_last = boundary_open  # ë°©ê¸ˆ ë‹«íŒ ë´‰

            # ğŸ”¥ FIX: ëˆ„ë½ ê°ì§€ ê°•í™” (0.3ë´‰ ì´ìƒë„ ê°ì§€)
            # ê¸°ì¡´: 0.5ë´‰ ì´ìƒë§Œ ê°ì§€ â†’ 1ë¶„ ê°­ì˜ 33% ëˆ„ë½!
            # ê°œì„ : 0.3ë´‰ ì´ìƒ ê°ì§€ + math.ceilë¡œ ì˜¬ë¦¼
            time_gap_seconds = abs((expected_last - new_last).total_seconds())
            time_gap_bars = time_gap_seconds / (iv * 60)  # ë´‰ ë‹¨ìœ„

            # ğŸ›¡ï¸ ë” ì—„ê²©í•œ ëˆ„ë½ ê¸°ì¤€: 0.3ë´‰ ì´ìƒ (ê¸°ì¡´: 0.5ë´‰)
            if time_gap_bars >= 0.3:  # 0.3ë´‰ ì´ìƒ ì°¨ì´ë‚˜ë©´ ëˆ„ë½ ì˜ì‹¬
                missing_minutes = time_gap_seconds / 60
                # ğŸ”¥ FIX: ì˜¬ë¦¼ ì²˜ë¦¬ë¡œ 1ë¶„ ê°­ë„ 1ë´‰ìœ¼ë¡œ ê³„ì‚°
                missing_bars = math.ceil(missing_minutes / iv)  # ê¸°ì¡´: int(...)

                if missing_bars > 0:
                    _log("WARN",
                        f"âš ï¸ [ëˆ„ë½ ê°ì§€] ê¸°ëŒ€ ë§ˆì§€ë§‰ ë´‰: {expected_last} | "
                        f"ì‹¤ì œ ë§ˆì§€ë§‰ ë´‰: {new_last} | "
                        f"ëˆ„ë½: {missing_bars}ê°œ ë´‰ ({missing_minutes}ë¶„)"
                    )

                    # âœ… Interval ê¸°ë°˜ ë°±í•„ ì „ëµ (1ë¶„ë´‰ì€ ë¹ ë¥¸ í¬ê¸° í•„ìˆ˜!)
                    # âœ… CTO ìŠ¹ì¸: ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€ë¡œ API ì¥ì•  ëŒ€ì‘ ê°•í™”
                    # - 1ë¶„ë´‰: ìµœëŒ€ 5íšŒ (3â†’5), ê°„ê²© 1~3ì´ˆ â†’ ë‹¤ìŒ ë´‰ ì „ì— ì™„ë£Œ
                    # - 3ë¶„ë´‰: ìµœëŒ€ 8íšŒ (5â†’8), ê°„ê²© 2~5ì´ˆ
                    # - 5ë¶„ ì´ìƒ: ìµœëŒ€ 10íšŒ (8â†’10), ê°„ê²© 2~20ì´ˆ
                    if iv == 1:
                        max_backfill_retry = 5  # 3 â†’ 5
                        wait_times = [1, 2, 2, 3, 3]  # ì´ 11ì´ˆ + API í˜¸ì¶œ ì‹œê°„
                    elif iv <= 3:
                        max_backfill_retry = 8  # 5 â†’ 8
                        wait_times = [2, 3, 4, 5, 6, 6, 7, 8]  # ì´ 41ì´ˆ
                    else:
                        max_backfill_retry = 10  # 8 â†’ 10
                        wait_times = [2, 4, 6, 8, 10, 12, 15, 20, 20, 20]  # ì´ 117ì´ˆ

                    _log("DEBUG", f"[ë°±í•„ ì „ëµ] interval={iv}ë¶„ â†’ max_retry={max_backfill_retry}")

                    backfill_success = False
                    for backfill_attempt in range(1, max_backfill_retry + 1):
                        try:
                            _log("INFO",
                                f"[ë°±í•„ ì‹œë„ {backfill_attempt}/{max_backfill_retry}] "
                                f"{new_last} ~ {expected_last} êµ¬ê°„ | "
                                f"ëˆ„ë½: {missing_bars}ê°œ ë´‰"
                            )

                            # ğŸ›¡ï¸ ëˆ„ë½ëœ êµ¬ê°„ + ì—¬ìœ ë¶„(3ê°œ) ì¶”ê°€ ìš”ì²­ (ê¸°ì¡´: +2)
                            # - ì—¬ìœ ë¶„ì„ ë” ëŠ˜ë ¤ì„œ API ì‘ë‹µ ë¶ˆì•ˆì • ëŒ€ì‘
                            backfill_count = missing_bars + 3
                            backfill = pyupbit.get_ohlcv(
                                ticker,
                                interval=interval,
                                count=backfill_count,
                                to=_fmt_to_param(expected_last)
                            )

                            if backfill is not None and not backfill.empty:
                                backfill = standardize_ohlcv(backfill).drop_duplicates()

                                # ğŸ”¥ FIX: ì‹¤ì œë¡œ ëˆ„ë½ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                                # - newì— ì´ë¯¸ ìˆëŠ” ë´‰ì€ ì œì™¸
                                # - last_openê³¼ expected_last ì‚¬ì´ë§Œ ì¶”ì¶œ (ë¯¸ë˜ ë´‰ ì°¨ë‹¨)
                                existing_indices = set(new.index)
                                backfill_new = backfill[~backfill.index.isin(existing_indices)]
                                backfill_new = backfill_new[
                                    (backfill_new.index > last_open) &
                                    (backfill_new.index <= expected_last)
                                ]

                                if not backfill_new.empty:
                                    # newì— ë³‘í•©
                                    new = pd.concat([new, backfill_new]).drop_duplicates().sort_index()
                                    _log("INFO",
                                        f"âœ… [ë°±í•„ ì„±ê³µ] {len(backfill_new)}ê°œ ë´‰ ë³µêµ¬ ì™„ë£Œ | "
                                        f"ë³µêµ¬ ë²”ìœ„: {backfill_new.index[0]} ~ {backfill_new.index[-1]}"
                                    )
                                    backfill_success = True
                                    break
                                else:
                                    _log("WARN", f"[ë°±í•„] ì‘ë‹µ ë°ì´í„°ê°€ ì´ë¯¸ ë³´ìœ  ì¤‘ì¸ ë´‰ë§Œ í¬í•¨")
                            else:
                                _log("WARN", f"[ë°±í•„] API ì‘ë‹µ ì—†ìŒ (attempt {backfill_attempt}/{max_backfill_retry})")

                        except Exception as e:
                            _log("ERROR", f"[ë°±í•„ ì‹¤íŒ¨] {e} (attempt {backfill_attempt}/{max_backfill_retry})")

                        # âœ… ì¬ì‹œë„ ì „ ëŒ€ê¸° (interval ê¸°ë°˜ wait_times ì‚¬ìš©)
                        if backfill_attempt < max_backfill_retry:
                            wait_time = wait_times[min(backfill_attempt - 1, len(wait_times) - 1)]
                            _log("INFO", f"[ë°±í•„] {wait_time}ì´ˆ í›„ ì¬ì‹œë„... (ëˆ„ë½ ë°©ì§€ ìµœìš°ì„ )")
                            time.sleep(wait_time)

                    if not backfill_success:
                        _log("ERROR",
                            f"âŒ [ë°±í•„ í¬ê¸°] {missing_bars}ê°œ ë´‰ ëˆ„ë½! | "
                            f"ëˆ„ë½ êµ¬ê°„: {new_last} ~ {expected_last} | "
                            f"ìµœëŒ€ {max_backfill_retry}íšŒ ì¬ì‹œë„ ì‹¤íŒ¨"
                        )

                        # âœ… Interval ê¸°ë°˜ ë°±í•„ í¬ê¸° í›„ ì „ëµ
                        if iv == 1:
                            # 1ë¶„ë´‰: í•©ì„± ë´‰ìœ¼ë¡œ ì¦‰ì‹œ ëŒ€ì²´ (ì•„ë˜ ë¡œì§ìœ¼ë¡œ ì²˜ë¦¬ë¨)
                            _log("WARN", f"[ë°±í•„ í¬ê¸°-1ë¶„ë´‰] í•©ì„± ë´‰ìœ¼ë¡œ ëŒ€ì²´ ì²˜ë¦¬")
                            # new.emptyê°€ ì•„ë‹ˆë¯€ë¡œ ì•„ë˜ `len(new) < expected_bars` ì¡°ê±´ìœ¼ë¡œ í•©ì„± ë´‰ ìƒì„±ë¨
                        else:
                            # 3ë¶„ ì´ìƒ: ì§€ì—° ë°±í•„ ì˜ˆì•½ (ë‹¤ìŒ ë£¨í”„ì—ì„œ ì¬ì‹œë„ ì—¬ìœ  ìˆìŒ)
                            _log("WARN", f"[ë°±í•„ í¬ê¸°-{iv}ë¶„ë´‰] ë‹¤ìŒ ë£¨í”„ì—ì„œ gap ê³„ì‚°ìœ¼ë¡œ ì¬ì‹œë„ ì˜ˆì • (last_open ìœ ì§€)")

                            # âœ… ì§€ì—°ëœ ë°±í•„: ëˆ„ë½ êµ¬ê°„ì„ ê¸°ë¡í•˜ì—¬ ë‹¤ìŒ ë£¨í”„ì—ì„œ ì¬ì¡°íšŒ
                            if not hasattr(stream_candles, '_pending_backfill'):
                                stream_candles._pending_backfill = []

                            stream_candles._pending_backfill.append({
                                'start': new_last,
                                'end': expected_last,
                                'missing_bars': missing_bars,
                                'retry_after': time.time() + 30,  # 30ì´ˆ í›„ ì¬ì‹œë„
                                'ticker': ticker,
                                'interval': interval,
                            })

                            _log("INFO", f"âœ… [ì§€ì—° ë°±í•„ ì˜ˆì•½] {missing_bars}ê°œ ë´‰ | 30ì´ˆ í›„ ì¬ì‹œë„ ì˜ˆì •")

        # ğŸ” PRICE-DEBUG: ì‹¤ì‹œê°„ standardize í›„ ë°ì´í„°
        try:
            last_3 = new.tail(min(3, len(new)))
            for idx, row in last_3.iterrows():
                _log("INFO", f"[PRICE-REALTIME-STD] {idx} | O={row['Open']:.0f} H={row['High']:.0f} L={row['Low']:.0f} C={row['Close']:.0f}")
        except Exception as e_log:
            _log("WARN", f"[PRICE-REALTIME-STD] ë¡œê¹… ì‹¤íŒ¨: {e_log}")

        # ğŸ”¥ FIX: ì˜ˆìƒ ë²”ìœ„ ë‚´ì˜ ë´‰ë§Œ í—ˆìš© (ë¯¸ë˜ ë´‰ ì°¨ë‹¨)
        # - last_open < index <= boundary_open
        # - boundary_open: ë°©ê¸ˆ ë‹«íŒ ë´‰ (ì´ë²ˆ ë£¨í”„ì—ì„œ ì²˜ë¦¬í•´ì•¼ í•  ìµœì‹  ë´‰)
        # - ì˜ˆ: last_open=21:24, boundary_open=21:25 â†’ 21:25ë§Œ í—ˆìš©, 21:26ì€ ì°¨ë‹¨
        before_filter_count = len(new)
        new = new[(new.index > last_open) & (new.index <= boundary_open)]

        # âœ… ì¤‘ë³µ ì œê±° (ê°™ì€ ì¸ë±ìŠ¤ëŠ” ìµœì‹  ê°’ ìœ ì§€)
        new = new.loc[~new.index.duplicated(keep='last')]

        # ğŸ” DEBUG: í•„í„°ë§ ê²°ê³¼
        _log("INFO", f"[ì‹¤ì‹œê°„ í•„í„°ë§] before={before_filter_count} | after={len(new)} | filter_condition: {last_open} < index <= {boundary_open}")

        # âœ… ì¤‘ê°„ ë´‰ ëˆ„ë½ ê°ì§€ (ë¶€ë¶„ ë°ì´í„° ë°˜í™˜ ëŒ€ì‘)
        elapsed_minutes = (boundary_open - last_open).total_seconds() / 60
        expected_bars = int(elapsed_minutes / iv)

        # ğŸ›¡ï¸ ë°©ì•ˆ 3-2: í•„í„°ë§ í›„ emptyì´ê±°ë‚˜ ì¤‘ê°„ ë´‰ ëˆ„ë½ ì‹œ ë³´í˜¸
        if new.empty or len(new) < expected_bars:
            # APIê°€ ë¶€ë¶„ ë°ì´í„°ë§Œ ë°˜í™˜ (ì˜ˆ: 18:49ë§Œ ìˆê³  18:48 ì—†ìŒ) ë˜ëŠ” ì‘ë‹µ ì—†ìŒ
            _log("DEBUG", f"[ëˆ„ë½ ê°ì§€] expected={expected_bars}ê°œ | actual={len(new)}ê°œ | elapsed={elapsed_minutes:.2f}ë¶„")

            # ì‹œê°„ì´ ì¶©ë¶„íˆ í˜ë €ìœ¼ë©´ í•©ì„± ë´‰ ìƒì„±
            if elapsed_minutes >= iv:
                # âœ… ê±°ë˜ê°€ ì—†ì–´ë„ ì´ì „ ì¢…ê°€ë¡œ í•©ì„± ë´‰ ìƒì„± (BUY í‰ê°€ ê¸°ë¡ìš©)
                if not df.empty:
                    last_close = float(df.iloc[-1]['Close'])

                    # ğŸ”¥ FIX: ì¤‘ê°„ì— ëˆ„ë½ëœ ëª¨ë“  ë´‰ì— ëŒ€í•´ í•©ì„± ë´‰ ìƒì„± (BUY í‰ê°€ ì—°ì†ì„± ë³´ì¥)
                    missing_bars_count = int(elapsed_minutes / iv)
                    _log("DEBUG", f"[í•©ì„± ë´‰ ë””ë²„ê·¸] last_open={last_open} | boundary_open={boundary_open} | elapsed_minutes={elapsed_minutes:.2f} | missing_bars_count={missing_bars_count}")

                    if missing_bars_count > 1:
                        _log("WARN",
                            f"[í•©ì„± ë´‰ ë‹¤ì¤‘ ìƒì„±] {missing_bars_count}ê°œ ë´‰ ìƒì„± í•„ìš” | "
                            f"êµ¬ê°„: {last_open} ~ {boundary_open}"
                        )
                        synthetic_bars = []
                        synthetic_indices = []
                        for i in range(1, missing_bars_count + 1):
                            synthetic_time = last_open + timedelta(minutes=iv * i)
                            _log("DEBUG", f"[í•©ì„± ë´‰ ë£¨í”„] i={i} | synthetic_time={synthetic_time} | boundary_open={boundary_open} | ì¡°ê±´={synthetic_time <= boundary_open}")
                            if synthetic_time <= boundary_open:
                                synthetic_bars.append({
                                    'Open': last_close,
                                    'High': last_close,
                                    'Low': last_close,
                                    'Close': last_close,
                                    'Volume': 0.0
                                })
                                synthetic_indices.append(synthetic_time)
                                _log("INFO", f"[í•©ì„± ë´‰] {synthetic_time} | OHLC={last_close:.2f} (Volume=0)")

                        if synthetic_bars:
                            synthetic_df = pd.DataFrame(synthetic_bars, index=synthetic_indices)
                            # âœ… ê¸°ì¡´ API ë°ì´í„°ì™€ ë³‘í•© (API ë°ì´í„° ìš°ì„ , ì¤‘ë³µ ì‹œ API ê°’ ìœ ì§€)
                            if not new.empty:
                                _log("DEBUG", f"[í•©ì„± ë´‰ ë³‘í•©] synthetic={len(synthetic_df)}ê°œ | api={len(new)}ê°œ")
                                combined = pd.concat([synthetic_df, new])
                                new = combined[~combined.index.duplicated(keep='last')].sort_index()
                                _log("INFO", f"âœ… [í•©ì„± ë´‰ ìƒì„±+ë³‘í•© ì™„ë£Œ] {len(new)}ê°œ | ì²«ë´‰={new.index[0]} | ë§ˆì§€ë§‰={new.index[-1]}")
                            else:
                                new = synthetic_df
                                _log("INFO", f"âœ… [í•©ì„± ë´‰ ìƒì„± ì™„ë£Œ] {len(synthetic_bars)}ê°œ | ì²«ë´‰={new.index[0]} | ë§ˆì§€ë§‰={new.index[-1]}")
                        else:
                            _log("ERROR", f"âŒ [í•©ì„± ë´‰ ì‹¤íŒ¨] synthetic_bars ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŒ! (missing_bars_count={missing_bars_count})")
                    else:
                        # ë‹¨ì¼ ë´‰ë§Œ ëˆ„ë½
                        _log("WARN",
                            f"[ì‹¤ì‹œê°„ í•„í„°ë§] ìƒˆ ë°ì´í„° ì—†ì§€ë§Œ ì‹œê°„ ê²½ê³¼ â†’ í•©ì„± ë´‰ ìƒì„± | "
                            f"time={boundary_open} | OHLC={last_close:.2f} (ì´ì „ ì¢…ê°€)"
                        )
                        # í•©ì„± ë´‰: Open=High=Low=Close=ì´ì „ì¢…ê°€, Volume=0
                        synthetic_bar = pd.DataFrame({
                            'Open': [last_close],
                            'High': [last_close],
                            'Low': [last_close],
                            'Close': [last_close],
                            'Volume': [0.0]
                        }, index=[boundary_open])

                        # âœ… ê¸°ì¡´ API ë°ì´í„°ì™€ ë³‘í•© (ë‹¨ì¼ ë´‰ë„ ë™ì¼ ë¡œì§ ì ìš©)
                        if not new.empty:
                            _log("DEBUG", f"[í•©ì„± ë´‰ ë³‘í•©-ë‹¨ì¼] synthetic=1ê°œ | api={len(new)}ê°œ")
                            combined = pd.concat([synthetic_bar, new])
                            new = combined[~combined.index.duplicated(keep='last')].sort_index()
                        else:
                            new = synthetic_bar

                    # ğŸ”¥ ì¤‘ìš”: last_openì€ ë‚˜ì¤‘ì— df.index[-1]ë¡œ ì—…ë°ì´íŠ¸ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë³€ê²½ ì•ˆ í•¨
                    # yield ê³„ì† ì§„í–‰ (ì•„ë˜ df ë³‘í•© ë¡œì§ìœ¼ë¡œ)
                else:
                    _log("WARN",
                        f"[ì‹¤ì‹œê°„ í•„í„°ë§] dfê°€ ë¹„ì–´ìˆì–´ í•©ì„± ë´‰ ìƒì„± ë¶ˆê°€ â†’ last_openë§Œ ì—…ë°ì´íŠ¸"
                    )
                    last_open = boundary_open
                    continue
            else:
                _log("INFO",
                    f"[ì‹¤ì‹œê°„ í•„í„°ë§] ì‹œê°„ ê²½ê³¼ ë¶€ì¡± ({elapsed_minutes:.1f}ë¶„ < {iv}ë¶„), "
                    f"last_open ìœ ì§€: {last_open}"
                )
                continue

        # ğŸ” MERGE-DEBUG: ë³‘í•© ì „ DataFrame ìƒíƒœ
        try:
            _log("DEBUG", f"[ë³‘í•© ì „] df.shape={df.shape} | df ë§ˆì§€ë§‰ 3ê°œ: {list(df.tail(3).index) if len(df) >= 3 else list(df.index)}")
            _log("DEBUG", f"[ë³‘í•© ì „] new.shape={new.shape} | new.empty={new.empty}")
            if not new.empty:
                _log("DEBUG", f"[ë³‘í•© ì „] new ì¸ë±ìŠ¤: {list(new.index[:3])}...{list(new.index[-3:])} (ì´ {len(new)}ê°œ)")
        except Exception as e_merge_log:
            _log("WARN", f"[ë³‘í•© ì „] ë¡œê¹… ì‹¤íŒ¨: {e_merge_log}")

        # ì¤‘ë³µ/ì •ë ¬ì€ _optimize_dataframe_memory ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë˜ì§€ë§Œ
        # í˜¹ì‹œ ë‚¨ì€ ì¤‘ë³µì— ëŒ€í•´ ìµœì‹  ê°’ ìš°ì„ ìœ¼ë¡œ í•œ ë²ˆ ë” ë³´ì •
        # df = _optimize_dataframe_memory(df, new, max_length).loc[~_optimize_dataframe_memory(df, new, max_length).index.duplicated(keep="last")].sort_index()
        # âœ… í•œ ë²ˆë§Œ ê³„ì‚°í•œ ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ í˜¸ì¶œ/ë ˆì´ìŠ¤ ìœ„í—˜ ì œê±°
        tmp = _optimize_dataframe_memory(df, new, max_length)
        df = tmp.loc[~tmp.index.duplicated(keep="last")].sort_index()
        del tmp

        # ğŸ” MERGE-DEBUG: ë³‘í•© í›„ DataFrame ìƒíƒœ
        try:
            _log("DEBUG", f"[ë³‘í•© í›„] df.shape={df.shape} | df ë§ˆì§€ë§‰ 3ê°œ: {list(df.tail(3).index)}")
        except Exception as e_merge_log:
            _log("WARN", f"[ë³‘í•© í›„] ë¡œê¹… ì‹¤íŒ¨: {e_merge_log}")

        # ì‹¤ì‹œê°„ ë³‘í•© í›„ DET ë¡œê¹… (ë¡œì»¬/ì„œë²„ ë¹„êµ í•µì‹¬ ì§€ì )
        log_det(df, "LOOP_MERGED")

        # ğŸ” PRICE-DEBUG: ì‹¤ì‹œê°„ ë³‘í•© í›„ ìµœì¢… ë°ì´í„°
        try:
            last_3 = df.tail(3)
            for idx, row in last_3.iterrows():
                _log("INFO", f"[PRICE-REALTIME-MERGED] {idx} | O={row['Open']:.0f} H={row['High']:.0f} L={row['Low']:.0f} C={row['Close']:.0f}")
        except Exception as e_log:
            _log("WARN", f"[PRICE-REALTIME-MERGED] ë¡œê¹… ì‹¤íŒ¨: {e_log}")

        # â˜… Phase 2: ì‹¤ì‹œê°„ ë°ì´í„°ë„ DBì— ì €ì¥ (ì ì§„ì  íˆìŠ¤í† ë¦¬ ëˆ„ì )
        if user_id and not new.empty:
            try:
                from services.db import save_candle_cache
                save_candle_cache(user_id, ticker, interval, new)
            except Exception as e:
                # ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ë©”ì¸ ë£¨í”„ëŠ” ê³„ì† ì§„í–‰
                pass

        # ğŸ›¡ï¸ ë°©ì•ˆ 4: Yield ì§ì „ ìµœì¢… ì—°ì†ì„± ê²€ì¦
        if len(df) > 1:
            # 1) ì¸ë±ìŠ¤ ì—°ì†ì„± ì²´í¬ (interval ê°„ê²©ì´ì–´ì•¼ í•¨)
            time_diffs = df.index.to_series().diff().dt.total_seconds() / 60
            gaps_in_df = time_diffs[time_diffs > iv * 1.5]  # 1.5ë°° ì´ìƒ ì°¨ì´ë‚˜ë©´ ê°­

            if not gaps_in_df.empty:
                gap_details = []
                for gap_idx, gap_minutes in gaps_in_df.items():
                    prev_idx = df.index[df.index.get_loc(gap_idx) - 1]
                    gap_details.append(f"  - {prev_idx} â†’ {gap_idx} (ê°­: {gap_minutes:.0f}ë¶„, {gap_minutes/iv:.1f}ë´‰)")

                _log("ERROR",
                    f"âŒ [ì—°ì†ì„± ì˜¤ë¥˜] DataFrameì— {len(gaps_in_df)}ê°œ ê°­ ë°œê²¬!\n" +
                    "\n".join(gap_details)
                )

                # ğŸ”¥ ì„ íƒ 1) ì—ëŸ¬ ë°œìƒ (ì—„ê²© ëª¨ë“œ) - ìš´ì˜ í™˜ê²½ì—ì„œëŠ” ì£¼ì„ ì²˜ë¦¬
                # raise ValueError("DataFrame ì—°ì†ì„± ê²€ì¦ ì‹¤íŒ¨ - ë°ì´í„° ëˆ„ë½ ê°ì§€")

                # ğŸ”¥ ì„ íƒ 2) ê²½ê³ ë§Œ ë‚¨ê¸°ê³  ì§„í–‰ (ê´€ëŒ€ ëª¨ë“œ)
                _log("WARN", "âš ï¸ ì—°ì†ì„± ì˜¤ë¥˜ ê°ì§€ë˜ì—ˆìœ¼ë‚˜ ì§„í–‰ (ê´€ëŒ€ ëª¨ë“œ)")

        # 2) ì˜ˆìƒ ì‹œê°ê³¼ ì‹¤ì œ last_open ë¹„êµ
        expected_last = boundary_open
        actual_last = df.index[-1]
        time_diff_seconds = abs((actual_last - expected_last).total_seconds())

        if time_diff_seconds > iv * 60 * 0.5:  # 0.5ë´‰ ì´ìƒ ì°¨ì´
            time_diff_minutes = time_diff_seconds / 60
            _log("WARN",
                f"âš ï¸ [ì‹œê°„ ë¶ˆì¼ì¹˜] ê¸°ëŒ€ ë§ˆì§€ë§‰ ë´‰: {expected_last} | "
                f"ì‹¤ì œ ë§ˆì§€ë§‰ ë´‰: {actual_last} | "
                f"ì°¨ì´: {time_diff_minutes:.1f}ë¶„ ({time_diff_minutes/iv:.2f}ë´‰)"
            )

        last_open = df.index[-1]
        # ì‚¬ìš©ì í˜¼ë€ ë°©ì§€ìš© ë™ê¸°í™” ë¡œê·¸ (bar_open / bar_close ëª…ì‹œ)
        if q:
            last_close = last_open + timedelta(minutes=iv)
            # run_at = datetime.now()
            run_at = _now_kst_naive()  # âœ… KST-naiveë¡œ ê¸°ë¡ í†µì¼
            q.put((
                time.time(),
                "LOG",
                f"â± run_at={run_at:%Y-%m-%d %H:%M:%S} | bar_open={last_open} | bar_close={last_close} "
            ))

        # ì£¼ê¸°ì  GC
        if hasattr(_optimize_dataframe_memory, "last_gc_time"):
            if time.time() - _optimize_dataframe_memory.last_gc_time > 300:
                _force_memory_cleanup()
                _optimize_dataframe_memory.last_gc_time = time.time()
        else:
            _optimize_dataframe_memory.last_gc_time = time.time()

        yield df


_INTERVAL_MAP = {
    "minute1": "minute1",
    "minute3": "minute3",
    "minute5": "minute5",
    "minute10": "minute10",
    "minute15": "minute15",
    "minute30": "minute30",
    "minute60": "minute60",
    "minute240": "minute240",
    "day": "day",
    "week": "week",
}

# get_ohlcv_once() ì£¼ì„ ë° ì¸ë±ìŠ¤ ì •ê·œí™” ìˆ˜ì •
def get_ohlcv_once(ticker: str, interval_code: str, count: int = 500) -> pd.DataFrame:
    """
    ëŒ€ì‹œë³´ë“œìš© ì›ìƒ· OHLCV.
    âœ… ë°˜í™˜: columns = [Open, High, Low, Close, Volume], DatetimeIndex = 'KST-naive' (streamê³¼ ë™ì¼ ê¸°ì¤€)
    """
    interval = _INTERVAL_MAP.get(interval_code, "minute1")
    df = pyupbit.get_ohlcv(ticker=ticker, interval=interval, count=count)
    if df is None or df.empty:
        return pd.DataFrame(columns=["Open","High","Low","Close","Volume"])

    # âš ï¸ ì¤‘ìš”: pyupbit ì¸ë±ìŠ¤ëŠ” ì´ë¯¸ KST tz-naiveë¡œ ë°˜í™˜ë¨
    if isinstance(df.index, pd.DatetimeIndex):
        idx = pd.to_datetime(df.index)
        if getattr(idx, "tz", None) is None:
            # âœ… pyupbitì€ ì´ë¯¸ KST naiveë¡œ ë°˜í™˜í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            pass
        else:
            # tz-awareì¸ ê²½ìš°ì—ë§Œ KSTë¡œ ë³€í™˜ í›„ tz ì œê±°
            idx = idx.tz_convert("Asia/Seoul").tz_localize(None)
            df.index = idx

    out = df[["open","high","low","close","volume"]].rename(
        columns={"open":"Open","high":"High","low":"Low","close":"Close","volume":"Volume"}
    )

    try:
        log_det(out, "ONCE_BEFORE_RETURN")
    except Exception:
        pass

    return out
