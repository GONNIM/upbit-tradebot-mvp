from __future__ import annotations
import pyupbit
import pandas as pd
import time
import logging
import random
import gc
import psutil
import os
from datetime import datetime, timedelta

from zoneinfo import ZoneInfo


logger = logging.getLogger(__name__)


# --------- ì‹œê°„/ê²½ê³„ ìœ í‹¸ (KST naiveë¡œ ì¼ê´€) ---------
_IV_MIN = {
    "minute1": 1,
    "minute3": 3,
    "minute5": 5,
    "minute10": 10,
    "minute15": 15,
    "minute30": 30,
    "minute60": 60,
    "day": 1440,
}

# --------- JITTER ê°’ (intervalë³„ ì°¨ë“± ì ìš©) ---------
# ë´‰ ì¢…ê°€ í™•ì • í›„ ì¶”ê°€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
# âš ï¸ ì¤‘ìš”: Upbit APIëŠ” ë´‰ ì¢…ê°€ í™•ì •ê¹Œì§€ ì‹œê°„ì´ ê±¸ë¦¼
# - 1ë¶„ë´‰: ìµœì†Œ 1.0ì´ˆ ëŒ€ê¸° í•„ìš” (0.3ì´ˆëŠ” ë„ˆë¬´ ì§§ì•„ì„œ ë°ì´í„° ì¤€ë¹„ ì•ˆ ë¨)
# - ì§§ê²Œ ì„¤ì • ì‹œ: 1ë¶„ë§ˆë‹¤ ìš”ì²­í•˜ì§€ë§Œ 2ë¶„ì— í•œ ë²ˆë§Œ ë°ì´í„° ìˆ˜ì‹  â†’ 2ë¶„ ê°„ê²© yield
# - ê¶Œì¥: 1ë¶„ë´‰ 1.0~1.5ì´ˆ, ì¥ê¸°ë´‰ 1.5~2.0ì´ˆ
JITTER_BY_INTERVAL = {
    "minute1": 5.0,   # 1ë¶„ë´‰: ë°ì´í„° í™•ì • ëŒ€ê¸° (Upbit API ì§€ì—° ê³ ë ¤)
    "minute3": 3.0,
    "minute5": 3.0,
    "minute10": 3.0,
    "minute15": 3.0,
    "minute30": 3.0,
    "minute60": 3.0,
    "day": 5.0,
}

# --------- í•„ìˆ˜ ë°ì´í„° ê°œìˆ˜ ì •ì˜ (ëª©í‘œì¹˜) ---------
# âš ï¸ ì£¼ì˜: Upbit APIëŠ” ê³¼ê±° ë°ì´í„° ì œì•½ìœ¼ë¡œ ëª©í‘œì¹˜ë¥¼ ëª» ì±„ìš¸ ìˆ˜ ìˆìŒ
# â†’ ì ˆëŒ€ ìµœì†ŒëŸ‰(ABSOLUTE_MIN_CANDLES)ë§Œ ì¶©ì¡±í•˜ë©´ ì „ëµ ì‹¤í–‰ í—ˆìš©
REQUIRED_CANDLES = {
    "minute1": 2000,   # 1ë¶„ë´‰: 2000ê°œ (ëª©í‘œ, Upbit ì‹¤ì œ ì œì•½: ~800ê°œ)
    "minute3": 1500,   # 3ë¶„ë´‰: 1500ê°œ (ëª©í‘œ)
    "minute5": 1200,   # 5ë¶„ë´‰: 1200ê°œ (ëª©í‘œ)
    "minute10": 1000,  # 10ë¶„ë´‰: 1000ê°œ (ëª©í‘œ)
    "minute15": 800,   # 15ë¶„ë´‰: 800ê°œ (ëª©í‘œ)
    "minute30": 600,   # 30ë¶„ë´‰: 600ê°œ (ëª©í‘œ)
    "minute60": 500,   # 60ë¶„ë´‰: 500ê°œ (ëª©í‘œ)
    "day": 400,        # ì¼ë´‰: 400ê°œ (ëª©í‘œ)
}

# ì ˆëŒ€ ìµœì†Œ ìº”ë“¤ ê°œìˆ˜ (ì´ ê°’ ë¯¸ë§Œì´ë©´ ì „ëµ ì‹œì‘ ë¶ˆê°€)
# - ì „ëµë³„ë¡œ ë‹¤ë¥¸ ìµœì†Œê°’ ì ìš©
ABSOLUTE_MIN_CANDLES = {
    "MACD": 600,  # MACD: ìµœëŒ€ íŒŒë¼ë¯¸í„° Ã— 3
    "EMA": 200,   # EMA: ìµœëŒ€ íŒŒë¼ë¯¸í„° (slow_period=200 ê¸°ì¤€)
}
ABSOLUTE_MIN_CANDLES_DEFAULT = 600  # ì „ëµ ë¯¸ì§€ì • ì‹œ ê¸°ë³¸ê°’

# ëª©í‘œ ëŒ€ë¹„ ê²½ê³  ë¹„ìœ¨ (ì´ ë¹„ìœ¨ ë¯¸ë§Œì´ë©´ ê²½ê³ ë§Œ í‘œì‹œ)
WARNING_RATIO = 0.5  # 50%


# ë””í„°ë¯¸ë‹ˆì¦˜ ì²´í¬ ë¡œê·¸ í—¬í¼
def log_det(df: pd.DataFrame, tag: str):
    """
    dfê°€ í˜„ì¬ ë™ì¼í•œ ë´‰ ì§‘í•©ì¸ì§€ ë¹ ë¥´ê²Œ ê²€ì¦í•˜ê¸° ìœ„í•œ ë¡œê·¸.
    - rows/first/last + OHLCV ì²´í¬ì„¬ì„ ë‚¨ê¸´ë‹¤.
    - tag: í˜¸ì¶œ ì§€ì  êµ¬ë¶„ìš©(ex: PRE_INIT, LOOP_MERGED, ONCE_BEFORE_RETURN)
    """
    if df is None or df.empty:
        logger.info(f"[DET] {tag} | rows=0 (empty)")
        return
    try:
        rows = len(df)
        first_i, last_i = df.index[0], df.index[-1]
        # OHLCVë§Œ ì‚¬ìš©, ì†Œìˆ˜ 8ìë¦¬ ë°˜ì˜¬ë¦¼ í›„ ë¬¸ìì—´ â†’ í•´ì‹œ
        payload = df[["Open","High","Low","Close","Volume"]].round(8).to_csv(index=True, header=False)
        checksum = hash(payload)  # íŒŒì´ì¬ ë‚´ì¥ í•´ì‹œ(ì„¸ì…˜ë§ˆë‹¤ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ, ê°™ì€ í”„ë¡œì„¸ìŠ¤ ë¹„êµìš©)
        logger.info(f"[DET] {tag} | rows={rows} | first={first_i} | last={last_i} | checksum={checksum}")
    except Exception as e:
        logger.warning(f"[DET] {tag} | logging failed: {e}")


def _iv_min(interval: str) -> int:
    return _IV_MIN.get(interval, 10)

# v1.2025.10.18.2031
def _now_kst_naive() -> datetime:
    """
    âœ… ì‹œìŠ¤í…œ ë¡œì»¬íƒ€ì„(UTC ë“±)ì— ì˜ì¡´í•˜ì§€ ì•Šê³  KST ì‹œê°ì„ tz-awareë¡œ ë§Œë“  ë’¤ tz ì œê±°.
    - ëª¨ë“  ë°” ê²½ê³„ ê³„ì‚°ì„ 'KST-naive'ë¡œ í†µì¼í•˜ê¸° ìœ„í•¨.
    """
    kst_now = datetime.now(tz=ZoneInfo("Asia/Seoul"))
    return kst_now.replace(second=0, microsecond=0).replace(tzinfo=None)

def _floor_boundary(dt: datetime, interval: str) -> datetime:
    if interval == "day":
        return dt.replace(hour=0, minute=0, second=0, microsecond=0)
    iv = _iv_min(interval)
    m = (dt.minute // iv) * iv
    return dt.replace(minute=m, second=0, microsecond=0)

def _next_boundary(dt: datetime, interval: str) -> datetime:
    if interval == "day":
        nxt = dt.replace(hour=0, minute=0, second=0, microsecond=0)
        if dt >= nxt:
            nxt += timedelta(days=1)
        return nxt
    iv = _iv_min(interval)
    m = (dt.minute // iv + 1) * iv
    add_h = m // 60
    m = m % 60
    h = (dt.hour + add_h) % 24
    nxt = dt.replace(hour=h, minute=m, second=0, microsecond=0)
    if dt.hour + add_h >= 24:
        nxt += timedelta(days=1)
    return nxt

def _fmt_to_param(dt: datetime) -> str:
    return dt.strftime("%Y-%m-%d %H:%M:%S")


# --------- ë©”ëª¨ë¦¬ ìœ í‹¸ ---------
def _optimize_dataframe_memory(old_df, new_data, max_length):
    try:
        if len(old_df) >= max_length:
            old_df = old_df.iloc[-(max_length - 10):].copy()
        combined = pd.concat([old_df, new_data], ignore_index=False)
        result = combined.drop_duplicates().sort_index().iloc[-max_length:]
        memory_usage_mb = result.memory_usage(deep=True).sum() / 1024 / 1024
        if memory_usage_mb > 10:
            logger.warning(f"âš ï¸ DataFrame ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³¼ë‹¤: {memory_usage_mb:.2f}MB")
        return result
    except Exception as e:
        logger.error(f"âŒ DataFrame ìµœì í™” ì‹¤íŒ¨: {e}")
        return pd.concat([old_df, new_data]).drop_duplicates().sort_index().iloc[-max_length:]

def _force_memory_cleanup():
    try:
        collected = gc.collect()
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024
        logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: ê°ì²´ {collected}ê°œ ìˆ˜ì§‘, í˜„ì¬ ë©”ëª¨ë¦¬: {memory_mb:.1f}MB")
        if memory_mb > 500:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_mb:.1f}MB - ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ í•„ìš”")
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")


# --------- ë©”ì¸ ìŠ¤íŠ¸ë¦¼ ---------
def stream_candles(
    ticker: str,
    interval: str,
    q=None,
    max_retry: int = 5,
    retry_wait: int = 3,
    stop_event=None,
    max_length: int = 500,
    user_id: str = None,  # Phase 2: ìºì‹œ ì‚¬ìš©ì„ ìœ„í•œ user_id
    strategy_type: str = None,  # ì „ëµ íƒ€ì… (MACD/EMA)
):
    # âœ… ë°ì´í„° ìˆ˜ì§‘ ìƒíƒœ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ import
    if user_id:
        try:
            from services.db import update_data_collection_status, clear_data_collection_status
        except ImportError:
            update_data_collection_status = None
            clear_data_collection_status = None
    else:
        update_data_collection_status = None
        clear_data_collection_status = None
    def _log(level: str, msg: str):
        (logger.warning if level == "WARN" else logger.error if level == "ERROR" else logger.info)(msg)
        if q:
            # í•­ìƒ 3-íŠœí”Œ ìœ ì§€
            prefix = "âš ï¸" if level == "WARN" else "âŒ" if level == "ERROR" else "â„¹ï¸"
            q.put((time.time(), "LOG", f"{prefix} {msg}"))

    def standardize_ohlcv(df):
        if df is None or df.empty:
            raise ValueError(f"OHLCV ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {ticker}, {interval}")

        before_count = len(df)
        _log("INFO", f"[standardize] ì…ë ¥ ë°ì´í„°: {before_count}ê°œ, index type={type(df.index)}, tz={getattr(df.index, 'tz', 'N/A')}")

        df = df.rename(columns={"open":"Open","high":"High","low":"Low","close":"Close","volume":"Volume"})
        if "value" in df.columns:
            df = df.drop(columns=["value"])

        # ì¸ë±ìŠ¤ tz ì •ê·œí™”: KST naiveë¡œ í†µì¼
        # âš ï¸ ì¤‘ìš”: pyupbitì€ ì´ë¯¸ KST ì‹œê°„ëŒ€ë¡œ tz-naive ë°ì´í„°ë¥¼ ë°˜í™˜í•¨
        idx = pd.to_datetime(df.index)
        try:
            if getattr(idx, "tz", None) is None:
                # âœ… pyupbitì€ ì´ë¯¸ KST naiveë¡œ ë°˜í™˜í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                _log("INFO", f"[standardize] tz-naive ê°ì§€ â†’ pyupbitì€ ì´ë¯¸ KSTì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©")
            else:
                # tz-awareì¸ ê²½ìš°ì—ë§Œ KSTë¡œ ë³€í™˜ í›„ tz ì œê±°
                _log("INFO", f"[standardize] tz-aware ê°ì§€ (tz={idx.tz}) â†’ KSTë¡œ ë³€í™˜")
                idx = idx.tz_convert("Asia/Seoul").tz_localize(None)
                _log("INFO", f"[standardize] KST naiveë¡œ ë³€í™˜ ì™„ë£Œ")
        except Exception as e:
            # ì˜ˆì™¸ ë°œìƒ ì‹œ ìƒì„¸ ë¡œê·¸
            _log("ERROR", f"[standardize] íƒ€ì„ì¡´ ë³€í™˜ ì‹¤íŒ¨: {e}")
            # ë³€í™˜ ì‹¤íŒ¨ ì‹œì—ë„ ìµœì†Œí•œ ì •ë ¬ì€ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ idx ê·¸ëŒ€ë¡œ ì‚¬ìš©

        df.index = idx

        # dropna ì „ NaN ê°œìˆ˜ í™•ì¸
        na_counts = df.isna().sum()
        if na_counts.any():
            _log("WARN", f"[standardize] NaN ë°œê²¬: {na_counts[na_counts > 0].to_dict()}")

        # ì •ë ¬ í›„ ì¤‘ë³µ ì œê±° (dropnaëŠ” ë‚˜ì¤‘ì—)
        df = df.sort_index()
        before_dedup = len(df)
        df = df.loc[~df.index.duplicated(keep="last")]
        after_dedup = len(df)

        if before_dedup > after_dedup:
            _log("WARN", f"[standardize] ì¤‘ë³µ ì œê±°: {before_dedup - after_dedup}ê°œ ì‚­ì œ ({before_dedup} â†’ {after_dedup})")

        # NaN ì œê±°
        df = df.dropna()
        after_dropna = len(df)

        if after_dedup > after_dropna:
            _log("WARN", f"[standardize] NaN ì œê±°: {after_dedup - after_dropna}ê°œ ì‚­ì œ ({after_dedup} â†’ {after_dropna})")

        _log("INFO", f"[standardize] ìµœì¢… ì¶œë ¥: {after_dropna}ê°œ (ì†ì‹¤: {before_count - after_dropna}ê°œ, {100*(before_count-after_dropna)/before_count:.1f}%)")

        return df

    # â˜… ì´ˆê¸° íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ìš© í—¬í¼
    def _fetch_initial_history(to_param: str, retry_full: int = 3) -> pd.DataFrame:
        """
        UpbitëŠ” ë¶„ë´‰ ê¸°ì¤€ í•œ ë²ˆì— ìµœëŒ€ 200ê°œë§Œ ë°˜í™˜í•˜ë¯€ë¡œ,
        max_lengthê°€ 200ì„ ë„˜ëŠ” ê²½ìš° ì—¬ëŸ¬ ë²ˆ ë‚˜ëˆ ì„œ ê³¼ê±° íˆìŠ¤í† ë¦¬ë¥¼ ëª¨ì€ë‹¤.
        - MACD/EMAë¥¼ HTS ìˆ˜ì¤€ìœ¼ë¡œ ë§ì¶”ê¸° ìœ„í•œ ê¸´ íˆìŠ¤í† ë¦¬(ì˜ˆ: 3ë¶„ë´‰ 1500~2000ê°œ) í™•ë³´ìš©.
        - retry_full: ì „ì²´ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íšŸìˆ˜
        """
        iv_min = _iv_min(interval)
        remaining = max_length
        current_to = to_param
        chunks: list[pd.DataFrame] = []
        base_delay_local = retry_wait
        total_requested = max_length
        api_calls = 0
        start_time = time.time()

        expected_calls = (max_length + 199) // 200  # ì˜¬ë¦¼ ê³„ì‚°
        expected_time = expected_calls * 0.15  # API í˜¸ì¶œë‹¹ ì•½ 0.15ì´ˆ (0.1ì´ˆ ë”œë ˆì´ + ë„¤íŠ¸ì›Œí¬)
        _log("INFO", f"[ì´ˆê¸°-multi] íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì‹œì‘: max_length={max_length}, interval={interval}")
        _log("INFO", f"[ì´ˆê¸°-multi] ì˜ˆìƒ: API í˜¸ì¶œ {expected_calls}íšŒ, ì†Œìš” ì‹œê°„ ì•½ {expected_time:.1f}ì´ˆ")

        # âœ… ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ìƒíƒœ ì €ì¥
        if update_data_collection_status:
            update_data_collection_status(
                user_id=user_id,
                is_collecting=True,
                collected=0,
                target=max_length,
                progress=0.0,
                estimated_time=expected_time,
                message=f"ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ({interval}ë´‰, ëª©í‘œ: {max_length}ê°œ)"
            )

        while remaining > 0:
            if stop_event and stop_event.is_set():
                collected = sum(len(c) for c in chunks)
                _log("WARN", f"[ì´ˆê¸°-multi] stop_event ê°ì§€ â†’ ìˆ˜ì§‘ ì¤‘ë‹¨ (collected={collected}/{total_requested})")
                break

            per_call = min(200, remaining)  # Upbit ë¶„ë´‰ ìµœëŒ€ 200ê°œ
            df_part = None
            api_calls += 1

            for attempt in range(1, max_retry + 1):
                try:
                    _log("INFO", f"[ì´ˆê¸°-multi] API í˜¸ì¶œ #{api_calls}: count={per_call}, to={current_to}")
                    df_part = pyupbit.get_ohlcv(
                        ticker,
                        interval=interval,
                        count=per_call,
                        to=current_to,
                    )
                    if df_part is not None and not df_part.empty:
                        _log("INFO", f"[ì´ˆê¸°-multi] API ì‘ë‹µ ì„±ê³µ: {len(df_part)}ê°œ ìˆ˜ì‹ ")
                        break
                    else:
                        _log("WARN", f"[ì´ˆê¸°-multi] API ì‘ë‹µì´ ë¹„ì–´ìˆìŒ (attempt {attempt}/{max_retry})")
                except Exception as e:
                    _log("ERROR", f"[ì´ˆê¸°-multi] API ì˜ˆì™¸ ë°œìƒ: {e} (attempt {attempt}/{max_retry})")

                # Upbit API rate limit ëŒ€ì‘: í˜¸ì¶œ ê°„ ìµœì†Œ 0.1ì´ˆ ë”œë ˆì´
                delay = min(base_delay_local * (2 ** (attempt - 1)), 60) + random.uniform(0.1, 1.0)
                _log("WARN", f"[ì´ˆê¸°-multi] API ì¬ì‹œë„ ëŒ€ê¸°: {delay:.1f}ì´ˆ")
                time.sleep(delay)
            else:
                # max_retry ì‹¤íŒ¨ ì‹œ - ë¶€ë¶„ ìˆ˜ì§‘ ë°ì´í„°ë¼ë„ ë°˜í™˜í•˜ë„ë¡ ê°œì„ 
                collected = sum(len(c) for c in chunks)
                _log("ERROR", f"[ì´ˆê¸°-multi] API ì—°ì† ì‹¤íŒ¨ (collected={collected}/{total_requested})")
                # break ëŒ€ì‹  ê²½ê³ ë§Œ ë‚¨ê¸°ê³  ìˆ˜ì§‘ëœ ë°ì´í„° ë°˜í™˜
                break

            if df_part is None or df_part.empty:
                collected = sum(len(c) for c in chunks)
                _log("WARN", f"[ì´ˆê¸°-multi] ë¹ˆ ì‘ë‹µìœ¼ë¡œ ìˆ˜ì§‘ ì¢…ë£Œ (collected={collected}/{total_requested})")
                break

            chunks.append(df_part)
            got = len(df_part)
            remaining -= got

            collected_so_far = sum(len(c) for c in chunks)
            progress = collected_so_far / total_requested
            remaining_time = remaining * 0.15
            _log("INFO", f"[ì´ˆê¸°-multi] ì§„í–‰: {collected_so_far}/{total_requested} ({100*progress:.1f}%)")

            # âœ… ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
            if update_data_collection_status:
                update_data_collection_status(
                    user_id=user_id,
                    is_collecting=True,
                    collected=collected_so_far,
                    target=total_requested,
                    progress=progress,
                    estimated_time=remaining_time,
                    message=f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ({collected_so_far}/{total_requested})"
                )

            if got < per_call:
                # Upbit APIê°€ ìš”ì²­ëŸ‰ë³´ë‹¤ ì ê²Œ ë°˜í™˜ = ë” ì´ìƒ ê³¼ê±° ë°ì´í„° ì—†ìŒ
                _log("WARN", f"[ì´ˆê¸°-multi] APIê°€ ìš”ì²­ëŸ‰ë³´ë‹¤ ì ê²Œ ë°˜í™˜ (got={got}, requested={per_call}) â†’ ê³¼ê±° ë°ì´í„° ì†Œì§„")
                break

            # ë‹¤ìŒ ìš”ì²­ìš© 'to'ëŠ” ì´ë²ˆ ê¸°ì¤€ì‹œê°„ì—ì„œ got*interval ë§Œí¼ ê³¼ê±°ë¡œ ì´ë™
            try:
                dt_to = datetime.strptime(current_to, "%Y-%m-%d %H:%M:%S")
                dt_to -= timedelta(minutes=iv_min * got)
                current_to = _fmt_to_param(dt_to)
            except Exception as e:
                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¶”ê°€ í˜ì´ì§•ì€ í•˜ì§€ ì•Šê³  ì¢…ë£Œ
                collected = sum(len(c) for c in chunks)
                _log("ERROR", f"[ì´ˆê¸°-multi] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {e} (collected={collected}/{total_requested})")
                break

            # API rate limit ì¤€ìˆ˜: í˜¸ì¶œ ê°„ 0.1ì´ˆ ë”œë ˆì´
            time.sleep(0.1)

        if not chunks:
            _log("ERROR", f"[ì´ˆê¸°-multi] ìˆ˜ì§‘ ì‹¤íŒ¨: ë°ì´í„° ì—†ìŒ")
            return pd.DataFrame(columns=["Open","High","Low","Close","Volume"])

        raw = pd.concat(chunks)
        final_count = len(raw)
        success_rate = 100 * final_count / total_requested if total_requested > 0 else 0
        elapsed_time = time.time() - start_time
        _log("INFO", f"[ì´ˆê¸°-multi] ìˆ˜ì§‘ ì™„ë£Œ: {final_count}/{total_requested} ({success_rate:.1f}%), API í˜¸ì¶œ {api_calls}íšŒ, ì†Œìš”ì‹œê°„ {elapsed_time:.2f}ì´ˆ")

        return raw
    
    # ---- ì´ˆê¸° ë¡œë“œ: ë§‰ ë‹«íŒ ê²½ê³„ê¹Œì§€ ----
    base_delay = retry_wait
    df = None
    now = _now_kst_naive()
    bar_close = _floor_boundary(now, interval)
    to_param = _fmt_to_param(bar_close)

    # â˜… Phase 2: DB ìºì‹œ ìš°ì„  í™•ì¸
    # âš ï¸ TEMPORARY: íƒ€ì„ì¡´ ìˆ˜ì • í›„ ìºì‹œ ë¬´íš¨í™” (ì˜ëª»ëœ íƒ€ì„ìŠ¤íƒ¬í”„ ë°©ì§€)
    if False and user_id:  # ìºì‹œ ë¡œì§ ì„ì‹œ ë¹„í™œì„±í™”
        try:
            from services.db import load_candle_cache
            cached_df = load_candle_cache(user_id, ticker, interval, max_length)

            if cached_df is not None and len(cached_df) >= max_length * 0.9:  # 90% ì´ìƒì´ë©´ ì‚¬ìš©
                df = cached_df
                _log("INFO", f"[CACHE-HIT] {len(df)} candles loaded from DB cache (skip API)")
            elif cached_df is not None:
                _log("INFO", f"[CACHE-PARTIAL] {len(cached_df)} candles in cache (insufficient, will fetch from API)")
        except Exception as e:
            _log("WARN", f"[CACHE] Load failed, will use API: {e}")

    _log("INFO", "[CACHE] íƒ€ì„ì¡´ ìˆ˜ì • í›„ ìºì‹œ ì„ì‹œ ë¹„í™œì„±í™” - APIì—ì„œ ì§ì ‘ ìˆ˜ì§‘")

    # âœ… ì „ëµë³„ ìµœì†Œ ìº”ë“¤ ê°œìˆ˜ ê²°ì •
    strategy_tag = (strategy_type or "MACD").upper().strip()
    absolute_min = ABSOLUTE_MIN_CANDLES.get(strategy_tag, ABSOLUTE_MIN_CANDLES_DEFAULT)
    _log("INFO", f"[ì´ˆê¸°] strategy={strategy_tag}, absolute_min_candles={absolute_min}")

    # â˜… ìºì‹œ ë¯¸ìŠ¤ ë˜ëŠ” ë¶€ì¡±: API í˜¸ì¶œ
    if df is None:
        _log("INFO", f"[ì´ˆê¸°] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: ticker={ticker}, interval={interval}, max_length={max_length}")

        if max_length <= 200:
            for attempt in range(1, max_retry + 1):
                if stop_event and stop_event.is_set():
                    _log("WARN", "stream_candles ì¤‘ë‹¨ë¨: ì´ˆê¸° ìˆ˜ì§‘ ì¤‘ stop_event ê°ì§€")
                    return
                try:
                    _log("INFO", f"[ì´ˆê¸°] API ë‹¨ì¼ í˜¸ì¶œ: count={max_length}, to={to_param}")
                    df = pyupbit.get_ohlcv(ticker, interval=interval, count=max_length, to=to_param)
                    if df is not None and not df.empty:
                        _log("INFO", f"[ì´ˆê¸°] API ì‘ë‹µ ì„±ê³µ: {len(df)}ê°œ ìˆ˜ì‹ ")
                        break
                except Exception as e:
                    _log("ERROR", f"[ì´ˆê¸°] API ì˜ˆì™¸ ë°œìƒ: {e}")

                delay = min(base_delay * (2 ** (attempt - 1)), 60) + random.uniform(0, 5)
                _log("WARN", f"[ì´ˆê¸°] API ì‹¤íŒ¨ ({attempt}/{max_retry}), {delay:.1f}ì´ˆ í›„ ì¬ì‹œë„")
                time.sleep(delay)
        else:
            # â˜… MACD/EMA ì•ˆì •í™”ë¥¼ ìœ„í•´ ê¸´ íˆìŠ¤í† ë¦¬(max_length) í™•ë³´ + ì¬ì‹œë„
            _log("INFO", f"[ì´ˆê¸°] max_length > 200 â†’ multi-fetch ëª¨ë“œ ì‚¬ìš© (ìµœëŒ€ 3íšŒ ì¬ì‹œë„)")

            retry_count = 0
            max_full_retry = 3

            while retry_count < max_full_retry:
                df = _fetch_initial_history(to_param, retry_full=max_full_retry)

                if df is not None and not df.empty:
                    temp_len = len(df)
                    success_rate = 100 * temp_len / max_length if max_length > 0 else 0

                    # ì ˆëŒ€ ìµœì†ŒëŸ‰ ì´ìƒì´ë©´ ì„±ê³µ (Upbit API ì œì•½ ê³ ë ¤)
                    if temp_len >= absolute_min:
                        _log("INFO", f"[ì´ˆê¸°-ì¬ì‹œë„] ìˆ˜ì§‘ ì„±ê³µ: {temp_len}/{max_length} ({success_rate:.1f}%) - ì ˆëŒ€ ìµœì†ŒëŸ‰({absolute_min}) ì¶©ì¡±")
                        break
                    else:
                        retry_count += 1
                        if retry_count < max_full_retry:
                            retry_delay = 5 + random.uniform(0, 3)
                            _log("WARN", f"[ì´ˆê¸°-ì¬ì‹œë„] ì ˆëŒ€ ë¶€ì¡± ({temp_len}/{absolute_min}) - {retry_delay:.1f}ì´ˆ í›„ ì „ì²´ ì¬ì‹œë„ ({retry_count}/{max_full_retry})")
                            time.sleep(retry_delay)
                        else:
                            _log("ERROR", f"[ì´ˆê¸°-ì¬ì‹œë„] ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬: {temp_len}/{absolute_min} (ì ˆëŒ€ ìµœì†ŒëŸ‰ ë¯¸ë‹¬)")
                else:
                    retry_count += 1
                    if retry_count < max_full_retry:
                        retry_delay = 5 + random.uniform(0, 3)
                        _log("ERROR", f"[ì´ˆê¸°-ì¬ì‹œë„] ìˆ˜ì§‘ ì‹¤íŒ¨ - {retry_delay:.1f}ì´ˆ í›„ ì „ì²´ ì¬ì‹œë„ ({retry_count}/{max_full_retry})")
                        time.sleep(retry_delay)

        # â˜… Phase 2: API í˜¸ì¶œ í›„ DBì— ì €ì¥
        if user_id and df is not None and not df.empty:
            try:
                from services.db import save_candle_cache
                save_candle_cache(user_id, ticker, interval, df)
            except Exception as e:
                _log("WARN", f"[CACHE] Save failed (ignored): {e}")

    if df is None or df.empty:
        raise ValueError(f"[ì´ˆê¸°] ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: ticker={ticker}, interval={interval}")

    _log("INFO", f"[ì´ˆê¸°] ìˆ˜ì§‘ëœ ì›ë³¸ ë°ì´í„°: {len(df)}ê°œ")

    df = standardize_ohlcv(df).drop_duplicates()
    final_len = len(df)
    success_rate = 100 * final_len / max_length if max_length > 0 else 0

    _log("INFO", f"[ì´ˆê¸°] standardize í›„ ìµœì¢… ë°ì´í„°: {final_len}ê°œ (ëª©í‘œ: {max_length}ê°œ, ë‹¬ì„±ë¥ : {success_rate:.1f}%)")

    # â˜… ì ˆëŒ€ ìµœì†ŒëŸ‰ ê²€ì¦ (Upbit API ì œì•½ ê³ ë ¤)
    if final_len < absolute_min:
        raise ValueError(
            f"âŒ ë°ì´í„° ì ˆëŒ€ ë¶€ì¡±ìœ¼ë¡œ ì „ëµ ì‹œì‘ ì°¨ë‹¨: {final_len}/{absolute_min} (ì ˆëŒ€ ìµœì†ŒëŸ‰) "
            f"- MA ê³„ì‚°ì— ìµœì†Œ {absolute_min}ê°œ í•„ìš” (í˜„ì¬ {success_rate:.1f}%)"
        )

    # ëª©í‘œ ëŒ€ë¹„ 50% ë¯¸ë§Œì´ë©´ ê²½ê³  (ì „ëµì€ ì‹¤í–‰)
    if final_len < max_length * WARNING_RATIO:
        _log("WARN",
            f"âš ï¸ ëª©í‘œ ëŒ€ë¹„ {success_rate:.1f}% ë‹¬ì„± ({final_len}/{max_length}) - "
            f"Upbit API ì œì•½ìœ¼ë¡œ ì¶”ì •. ì ˆëŒ€ ìµœì†ŒëŸ‰({absolute_min})ì€ ì¶©ì¡±í•˜ì—¬ ì „ëµ ì‹¤í–‰"
        )

    # âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ - ìƒíƒœ ì´ˆê¸°í™”
    if clear_data_collection_status:
        clear_data_collection_status(user_id)
        _log("INFO", f"[ì´ˆê¸°] ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ! ì—”ì§„ ì‹œì‘í•©ë‹ˆë‹¤.")

    yield df

    last_open = df.index[-1]  # ìš°ë¦¬ê°€ ê°€ì§„ ë§ˆì§€ë§‰ bar_open (tz-naive)

    # ---- ì‹¤ì‹œê°„ ë£¨í”„: ê²½ê³„ ë™ê¸°í™” â†’ ë‹«íŒ ë´‰ ì¡°íšŒ â†’ ê°­ ë°±í•„ ----
    # âœ… intervalë³„ JITTER ê°’ ì„ íƒ
    jitter = JITTER_BY_INTERVAL.get(interval, 0.7)
    _log("INFO", f"[ì‹¤ì‹œê°„ ë£¨í”„] interval={interval}, jitter={jitter}ì´ˆ")

    # âœ… API ì‘ë‹µ ì§€ì—° ì¬ì‹œë„ ì¹´ìš´í„°
    api_retry_count = 0

    while not (stop_event and stop_event.is_set()):
        now = _now_kst_naive()
        next_close = _next_boundary(now, interval)
        sleep_sec = max(0.0, (next_close - now).total_seconds() + jitter)

        # ğŸ” DEBUG: ë£¨í”„ ì§„ì… í™•ì¸
        _log("INFO", f"[ì‹¤ì‹œê°„ ë£¨í”„] sleep={sleep_sec:.1f}ì´ˆ | now={now} | next_close={next_close} | last_open={last_open}")
        time.sleep(sleep_sec)

        # ë§‰ ë‹«íŒ ë´‰ì˜ open
        iv = _iv_min(interval)
        boundary_open = next_close - timedelta(minutes=iv)  # ë‘˜ ë‹¤ tz-naive

        # ì¤‘ê°„ ëˆ„ë½ë¶„ ê³„ì‚°(ë¶„ ë‹¨ìœ„)
        gap = int((boundary_open - last_open).total_seconds() // (iv * 60))
        need = max(1, min(gap, 200))

        # ğŸ” DEBUG: API í˜¸ì¶œ ì „ íŒŒë¼ë¯¸í„°
        _log("INFO", f"[ì‹¤ì‹œê°„ API] boundary_open={boundary_open} | gap={gap} | need={need} | to={_fmt_to_param(next_close)}")

        # ì¬ì‹œë„ ë£¨í”„
        new = None
        for attempt in range(1, max_retry + 1):
            if stop_event and stop_event.is_set():
                _log("WARN", "stream_candles ì¤‘ë‹¨ë¨: ì‹¤ì‹œê°„ ë£¨í”„ ì¤‘ stop_event ê°ì§€")
                return
            try:
                new = pyupbit.get_ohlcv(ticker, interval=interval, count=need, to=_fmt_to_param(next_close))
                if new is not None and not new.empty:
                    break
            except Exception as e:
                _log("ERROR", f"[ì‹¤ì‹œê°„] API ì˜ˆì™¸: {e}")
            delay = min(base_delay * (2 ** (attempt - 1)), 30) + random.uniform(0, 2)
            _log("WARN", f"[ì‹¤ì‹œê°„] API ì‹¤íŒ¨ ({attempt}/{max_retry}), {delay:.1f}ì´ˆ í›„ ì¬ì‹œë„")
            time.sleep(delay)
        else:
            backoff = min(30 + random.uniform(0, 10), 300)
            _log("ERROR", f"[ì‹¤ì‹œê°„] API ì—°ê²° ì‹¤íŒ¨, {backoff:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
            time.sleep(backoff)
            continue

        # ğŸ” DEBUG: API ì‘ë‹µ ë°ì´í„° í™•ì¸
        if new is not None and not new.empty:
            _log("INFO", f"[ì‹¤ì‹œê°„ API ì‘ë‹µ] rows={len(new)} | first={new.index[0]} | last={new.index[-1]}")

            # âœ… API ì‘ë‹µ ê²€ì¦: ê¸°ëŒ€í•œ ì‹œê°„ëŒ€ì™€ ì‹¤ì œ ì‘ë‹µ ë¹„êµ
            expected_last = boundary_open  # ìš°ë¦¬ê°€ ê¸°ëŒ€í•˜ëŠ” ë§ˆì§€ë§‰ ë´‰
            actual_last = new.index[-1]
            time_gap = (expected_last - actual_last).total_seconds() / 60  # ë¶„ ë‹¨ìœ„

            if time_gap > iv * 2:  # 2ë´‰ ì´ìƒ ì°¨ì´ ë‚˜ë©´
                _log("WARN",
                    f"[ì‹¤ì‹œê°„ API] ì‘ë‹µ ì§€ì—° ê°ì§€! "
                    f"ê¸°ëŒ€: {expected_last} | ì‹¤ì œ: {actual_last} | ê°­: {time_gap:.0f}ë¶„"
                )
                # ì§§ê²Œ ëŒ€ê¸° í›„ ì¬ì‹œë„ (ìµœëŒ€ 2íšŒ)
                if api_retry_count < 2:
                    api_retry_count += 1
                    retry_delay = 3 + random.uniform(0, 2)
                    _log("WARN", f"[ì‹¤ì‹œê°„ API] {retry_delay:.1f}ì´ˆ í›„ ì¬ì‹œë„ ({api_retry_count}/2)")
                    time.sleep(retry_delay)
                    continue
                else:
                    _log("WARN", "[ì‹¤ì‹œê°„ API] ìµœëŒ€ ì¬ì‹œë„ ë„ë‹¬, ê¸°ì¡´ ë°ì´í„°ë¡œ ì§„í–‰")
                    api_retry_count = 0
            else:
                # ì •ìƒ ì‘ë‹µì´ë©´ ì¬ì‹œë„ ì¹´ìš´í„° ë¦¬ì…‹
                api_retry_count = 0
        else:
            _log("WARN", f"[ì‹¤ì‹œê°„ API ì‘ë‹µ] new is None or empty!")
            continue

        new = standardize_ohlcv(new).drop_duplicates()

        # ğŸ” DEBUG: standardize í›„ ë°ì´í„°
        _log("INFO", f"[ì‹¤ì‹œê°„ í‘œì¤€í™” í›„] rows={len(new)} | first={new.index[0]} | last={new.index[-1]}")

        # âœ… ìš°ë¦¬ê°€ ê°€ì§„ ë§ˆì§€ë§‰ ì´í›„ ê²ƒë§Œ (>= ì‚¬ìš©ìœ¼ë¡œ ê°™ì€ ë´‰ë„ ì—…ë°ì´íŠ¸ í—ˆìš©)
        before_filter_count = len(new)
        new = new[new.index >= last_open]  # '>' â†’ '>=' ë³€ê²½

        # âœ… ì¤‘ë³µ ì œê±° (ê°™ì€ ì¸ë±ìŠ¤ëŠ” ìµœì‹  ê°’ ìœ ì§€)
        new = new.loc[~new.index.duplicated(keep='last')]

        # ğŸ” DEBUG: í•„í„°ë§ ê²°ê³¼
        _log("INFO", f"[ì‹¤ì‹œê°„ í•„í„°ë§] before={before_filter_count} | after={len(new)} | filter_condition: index >= {last_open}")

        if new.empty:
            _log("WARN", f"[ì‹¤ì‹œê°„ í•„í„°ë§] âš ï¸ ìƒˆ ë°ì´í„° ì—†ìŒ! ëª¨ë“  ë°ì´í„°ê°€ last_open({last_open}) ì´ì „ì„. continueí•˜ì—¬ ë‹¤ìŒ ë£¨í”„ ëŒ€ê¸°...")
            continue

        # ì¤‘ë³µ/ì •ë ¬ì€ _optimize_dataframe_memory ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë˜ì§€ë§Œ
        # í˜¹ì‹œ ë‚¨ì€ ì¤‘ë³µì— ëŒ€í•´ ìµœì‹  ê°’ ìš°ì„ ìœ¼ë¡œ í•œ ë²ˆ ë” ë³´ì •
        # df = _optimize_dataframe_memory(df, new, max_length).loc[~_optimize_dataframe_memory(df, new, max_length).index.duplicated(keep="last")].sort_index()
        # âœ… í•œ ë²ˆë§Œ ê³„ì‚°í•œ ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ í˜¸ì¶œ/ë ˆì´ìŠ¤ ìœ„í—˜ ì œê±°
        tmp = _optimize_dataframe_memory(df, new, max_length)
        df = tmp.loc[~tmp.index.duplicated(keep="last")].sort_index()
        del tmp

        # ì‹¤ì‹œê°„ ë³‘í•© í›„ DET ë¡œê¹… (ë¡œì»¬/ì„œë²„ ë¹„êµ í•µì‹¬ ì§€ì )
        log_det(df, "LOOP_MERGED")

        # â˜… Phase 2: ì‹¤ì‹œê°„ ë°ì´í„°ë„ DBì— ì €ì¥ (ì ì§„ì  íˆìŠ¤í† ë¦¬ ëˆ„ì )
        if user_id and not new.empty:
            try:
                from services.db import save_candle_cache
                save_candle_cache(user_id, ticker, interval, new)
            except Exception as e:
                # ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ë©”ì¸ ë£¨í”„ëŠ” ê³„ì† ì§„í–‰
                pass

        last_open = df.index[-1]
        # ì‚¬ìš©ì í˜¼ë€ ë°©ì§€ìš© ë™ê¸°í™” ë¡œê·¸ (bar_open / bar_close ëª…ì‹œ)
        if q:
            last_close = last_open + timedelta(minutes=iv)
            # run_at = datetime.now()
            run_at = _now_kst_naive()  # âœ… KST-naiveë¡œ ê¸°ë¡ í†µì¼
            q.put((
                time.time(),
                "LOG",
                f"â± run_at={run_at:%Y-%m-%d %H:%M:%S} | bar_open={last_open} | bar_close={last_close} "
            ))

        # ì£¼ê¸°ì  GC
        if hasattr(_optimize_dataframe_memory, "last_gc_time"):
            if time.time() - _optimize_dataframe_memory.last_gc_time > 300:
                _force_memory_cleanup()
                _optimize_dataframe_memory.last_gc_time = time.time()
        else:
            _optimize_dataframe_memory.last_gc_time = time.time()

        yield df


_INTERVAL_MAP = {
    "minute1": "minute1",
    "minute3": "minute3",
    "minute5": "minute5",
    "minute10": "minute10",
    "minute15": "minute15",
    "minute30": "minute30",
    "minute60": "minute60",
    "minute240": "minute240",
    "day": "day",
    "week": "week",
}

# get_ohlcv_once() ì£¼ì„ ë° ì¸ë±ìŠ¤ ì •ê·œí™” ìˆ˜ì •
def get_ohlcv_once(ticker: str, interval_code: str, count: int = 500) -> pd.DataFrame:
    """
    ëŒ€ì‹œë³´ë“œìš© ì›ìƒ· OHLCV.
    âœ… ë°˜í™˜: columns = [Open, High, Low, Close, Volume], DatetimeIndex = 'KST-naive' (streamê³¼ ë™ì¼ ê¸°ì¤€)
    """
    interval = _INTERVAL_MAP.get(interval_code, "minute1")
    df = pyupbit.get_ohlcv(ticker=ticker, interval=interval, count=count)
    if df is None or df.empty:
        return pd.DataFrame(columns=["Open","High","Low","Close","Volume"])

    # âš ï¸ ì¤‘ìš”: pyupbit ì¸ë±ìŠ¤ëŠ” ì´ë¯¸ KST tz-naiveë¡œ ë°˜í™˜ë¨
    if isinstance(df.index, pd.DatetimeIndex):
        idx = pd.to_datetime(df.index)
        if getattr(idx, "tz", None) is None:
            # âœ… pyupbitì€ ì´ë¯¸ KST naiveë¡œ ë°˜í™˜í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            pass
        else:
            # tz-awareì¸ ê²½ìš°ì—ë§Œ KSTë¡œ ë³€í™˜ í›„ tz ì œê±°
            idx = idx.tz_convert("Asia/Seoul").tz_localize(None)
            df.index = idx

    out = df[["open","high","low","close","volume"]].rename(
        columns={"open":"Open","high":"High","low":"Low","close":"Close","volume":"Volume"}
    )

    try:
        log_det(out, "ONCE_BEFORE_RETURN")
    except Exception:
        pass

    return out
